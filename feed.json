{
    "version": "https://jsonfeed.org/version/1",
    "title": "枯萎的花将在另一彼岸悄然绽放",
    "subtitle": null,
    "icon": "https://yunhdan.github.io/images/favicon.ico",
    "description": "计算机视觉 & 图像恢复",
    "home_page_url": "https://yunhdan.github.io",
    "items": [
        {
            "id": "https://yunhdan.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E7%90%86%E8%AE%BA/",
            "url": "https://yunhdan.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E7%90%86%E8%AE%BA/",
            "title": "操作系统理论",
            "date_published": "2025-09-08T06:55:47.000Z",
            "content_html": "<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script>",
            "tags": []
        },
        {
            "id": "https://yunhdan.github.io/cs/Web/",
            "url": "https://yunhdan.github.io/cs/Web/",
            "title": "Web",
            "date_published": "2025-08-20T07:14:54.000Z",
            "content_html": "<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h1 id=\"网页部署\"><a class=\"anchor\" href=\"#网页部署\">#</a> 网页部署</h1>\n<ul>\n<li>布置静态网页，启蒙介绍：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FianR4Zi9hcnRpY2xlL2RldGFpbHMvMTQ1NDMxMjI2\">https://blog.csdn.net/abjtxf/article/details/145431226</span></li>\n<li>同上，相较更详细一些的启蒙介绍：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9seG5jaGFuLmNuL2Nmd29ya2Vycy13ZWJwYWdlcy5odG1s\">https://lxnchan.cn/cfworkers-webpages.html</span></li>\n</ul>\n",
            "tags": [
                "计算机"
            ]
        },
        {
            "id": "https://yunhdan.github.io/cs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E7%BB%84%E6%88%90/",
            "url": "https://yunhdan.github.io/cs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E7%BB%84%E6%88%90/",
            "title": "计算机系统组成",
            "date_published": "2025-08-13T08:37:55.000Z",
            "content_html": "<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><div class=\"note info\">\n<p>有用的计算机系统知识，欢迎来闯关！</p>\n</div>\n<h1 id=\"概论\"><a class=\"anchor\" href=\"#概论\">#</a> 概论</h1>\n<ol>\n<li class=\"quiz multi\">冯诺依曼体系结构三大思想为？\n<ul class=\"options\">\n<li class=\"correct\">计算机硬件系统由运算器、存储器、控制器、输入设备、输出设备 5 大部件组成。</li>\n<li class=\"correct\">“存储程序”：指令和数据以同等地位存储在存储器中。</li>\n<li class=\"correct\">指令和数据均用二进制表示。</li>\n<li>以输入设备和输出设备为中心。</li>\n</ul>\n</li>\n</ol>\n<div class=\"note info\">\n<p>“存储程序” 的概念是指将指令事先输入计算机的主存，然后取程序在主存的首地址来执行程序的第一条指令，按程序规定的顺序执行其他指令，直到程序结束。</p>\n<p>这揭示了计算机的工作过程：1）把程序和数据存入主存。2）把主存的源程序转换为可执行文件。3）从可执行文件的首地址开始逐条执行指令。</p>\n</div>\n<div class=\"note info\">\n<p>计算机系统结构层次从高到低分为：应用软件级、高级语言级、汇编语言级、操作系统级、机器语言级、硬件逻辑级。</p>\n</div>\n<ol start=\"2\">\n<li class=\"quiz false\">\n<p>编译是将源程序翻译为可执行的目标程序，也就是汇编语言文件。</p>\n<blockquote>\n<p>目标程序文件是二进制的机器语言文件。<br />\n编译包含了汇编过程。</p>\n</blockquote>\n</li>\n<li>\n<p>回顾一下，java 虚拟机 jvm 的作用是什么？</p>\n</li>\n</ol>\n<details class=\"success\"><summary>答案</summary><div>\n<p>java 程序并不一步到位编译为目标程序，而是先编译成中间文件字节码，即.class 文件，然后再用解释的方式一条一条执行。而 JVM 的作用就是作为一个中介将字节码逐条解释为目标代码，然后执行。</p>\n</div></details>\n<ol start=\"4\">\n<li>描述一下指令的执行过程和程序的执行过程。</li>\n</ol>\n<details class=\"success\"><summary>答案</summary><div>\n<p>指令的执行过程：<br />\n取指：到 PC 取指令的地址，然后在主存中取出指令内容，最终送入指令寄存器 IR。<br />\n分析指令：控制器根据 IR 译码后送出控制信号。<br />\n执行指令：控制器根据控制信号在主存取出操作数，在运算器完成指令。</p>\n<p>程序的执行过程：<br />\n程序的第一条指令在 PC 中被取出，经过分析和执行后，通过第一条指令的地址计算出下一条指令的地址，用新得到的指令地址继续读出第二条指令并执行，直到程序结束。</p>\n</div></details>\n<div class=\"note info\">\n<p>计算机主要四大性能指标：机器字长、数据通路带宽、主存容量、运算速度。运算速度有吞吐量、响应时间、时钟周期、主频、CPI 等指标。</p>\n</div>\n<ol start=\"5\">\n<li class=\"quiz false\">\n<p>数据通路带宽是指数据总线一次所能串行传送信息的位数。</p>\n<blockquote>\n<p>带宽是衡量并行传送信息的能力指标。</p>\n</blockquote>\n</li>\n<li class=\"quiz true\">\n<p>主存容量可以用字数<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo>×</mo></mrow><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.66666em;vertical-align:-0.08333em;\"></span><span class=\"mord\">×</span></span></span></span> 字长（如 512K <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo>×</mo></mrow><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.66666em;vertical-align:-0.08333em;\"></span><span class=\"mord\">×</span></span></span></span> 16 位）来表示。</p>\n</li>\n</ol>\n<div class=\"note info\">\n<p>吞吐量是指系统单位时间内处理请求或指令的数量。<br />\n相应时间是指用户向系统发送一个请求，到计算机对该请求做出相应并获得结果的时间。</p>\n</div>\n<ol start=\"7\">\n<li class=\"quiz true\">\n<p>CPU 最小的时间单位是时钟周期，又称为节拍脉冲。</p>\n</li>\n<li>\n<p>什么是 CPI？</p>\n</li>\n</ol>\n<details class=\"success\"><summary>答案</summary><div>\n<p>英文全名叫 Clock cycle Per Instruction，即执行一条指令所需的时钟周期数。</p>\n</div></details>\n",
            "tags": [
                "计算机"
            ]
        },
        {
            "id": "https://yunhdan.github.io/book/%E3%80%8A%E8%AE%A4%E7%9F%A5%E8%A7%89%E9%86%92%E3%80%8B/",
            "url": "https://yunhdan.github.io/book/%E3%80%8A%E8%AE%A4%E7%9F%A5%E8%A7%89%E9%86%92%E3%80%8B/",
            "title": "《认知觉醒》",
            "date_published": "2025-08-07T06:55:47.000Z",
            "content_html": "<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h3 id=\"认知觉醒开启自我改变的原动力\"><a class=\"anchor\" href=\"#认知觉醒开启自我改变的原动力\">#</a> <strong>《认知觉醒：开启自我改变的原动力》</strong></h3>\n<p>周岭</p>\n<h3 id=\"第一章-大脑一切问题的起源\"><a class=\"anchor\" href=\"#第一章-大脑一切问题的起源\">#</a> <strong>第一章 大脑 —— 一切问题的起源</strong></h3>\n<ul>\n<li>为了适应陆地生活，爬行动物演化出了最初的 “本能脑”。本能脑的结构很简单，只有一个原始的反射模块，可以让爬行动物对环境快速做出本能反应，比如遇到危险就战斗或逃跑，遇到猎物就捕食，遇到心仪的异性就追求等。</li>\n<li>哺乳动物的大脑里也发展出一个独特的情感区域（边缘系统），脑科学家称之为 “情绪脑”</li>\n<li>人类沉迷于自己独有的理智，所以把这个新的脑区称为 “理智脑”</li>\n<li>本能脑已经有近 3.6 亿年的历史，情绪脑有近 2 亿年的历史，而理智脑出现的时间只有 250 万年不到。</li>\n<li>本能脑早在婴儿时期就比较完善了，情绪脑则要等到青春期早期才趋于完善，而理智脑最晚，要等到成年早期才基本发育成熟。</li>\n<li>我们的大脑里大约有 860 亿个神经元细胞，而本能脑和情绪脑拥有近八成，所以它们对大脑的掌控力更强。</li>\n<li>种种迹象表明，理智脑对大脑的控制能力很弱，所以我们在生活中做的大部分决策往往源于本能和情绪，而非理智。当然，不管是何种因素影响我们做出决策，初衷都是让我们好，只不过本能脑和情绪脑的决策往往与现代社会脱节，因为它们以为自己还处于原始社会。</li>\n<li>进化之手还未来得及完全改造我们，这些在远古社会具有生存优势的天性，在现代社会反而成了阻碍，甚至可以说，我们当前遇到的几乎所有的成长问题都可以归结到目光短浅、即时满足的天性上，不过在现代社会，用避难趋易和急于求成来代指它们显然更加贴切。</li>\n<li>避难趋易 —— 只做简单和舒适的事，喜欢在核心区域周边打转，待在舒适区内逃避真正的困难；</li>\n<li>急于求成 —— 凡事希望立即看到结果，对不能马上看到结果的事往往缺乏耐心，非常容易放弃。</li>\n<li>事实上理智脑很少有主见，大多数时候我们以为自己在思考，其实都是在对自身的行为和欲望进行合理化，这正是人类被称作 “自我解释的动物” 的原因。</li>\n<li>理智脑发达的人更能：・立足长远，主动走出舒适区；・为潜在的风险克制自己，为可能的收益延时满足；・保持耐心，坚持做那些短期内看不到效果的 “无用之事”；・抵制诱惑，面对舒适和娱乐时，做出与其他人不同的选择……</li>\n<li>习惯之所以难以改变，就是因为它是自我巩固的 —— 越用越强，越强越用。要想从既有的习惯中跳出来，最好的方法不是依靠自制力，而是依靠知识</li>\n<li>很多人在成长的过程中感到极度痛苦，就是因为他们总是用意志力去对抗本能和情绪，最后把自己搞得精疲力竭，却收效甚微</li>\n<li>无论个体还是群体，人类的安全感都源于自己在某一方面拥有的独特优势：或能力，或财富，或权力，或影响力。</li>\n<li>归结起来，焦虑的原因就两条：想同时做很多事，又想立即看到效果。</li>\n<li>焦虑就是因为欲望与能力之间差距过大。</li>\n<li>焦虑是天性，是人类的默认设置。</li>\n<li>克制欲望，不要让自己同时做很多事；・面对现实，看清自己真实的能力水平；・要事优先，想办法只做最重要的事情；・接受环境，在局限中做力所能及的事；・直面核心，狠狠逼自己一把去突破它。</li>\n<li>很多时候，我们对困难的事物缺乏耐心是因为看不到全局、不知道自己身在何处，所以总是拿着天性这把短视之尺到处衡量，以为做成一件事很简单。事实上，如果我们能了解一些事物发展的基本规律，改用理性这把客观之尺，则会极大地提升耐心。</li>\n<li>舒适区边缘另一个重要的规律是它揭示了能力成长的普遍法则：无论个体还是群体，其能力都以 “舒适区 — 拉伸区￼— 困难区” 的形式分布，要想让自己高效成长，必须让自己始终处于舒适区的边缘，贸然跨到困难区会让自己受挫，而始终停留在舒适区会让自己停滞（见图 1-4）。 ￼</li>\n<li>对于学习而言，学习之后的思考、思考之后的行动、行动之后的改变更重要，如果不盯住内层的改变量，那么在表层投入再多的学习量也会事倍功半；因此，从权重上看，改变量﹥行动量﹥思考量﹥学习量（见图 1-5）。</li>\n<li>另一个值得关注的微观规律是学习的平台期。这个规律表明，学习进展和时间的关系并不是我们想象中的那种线性关系（学多少是多少），而是呈现一种波浪式上升曲线（见图 1-6）。 ￼</li>\n<li>在平台期，我们可能付出了大量的努力，但看起来毫无进步，甚至可能退步，不过这仅仅是一个假象，因为大脑中的神经元细胞依旧在发生连接并被不停地巩固，到了某一节点后，就会进入下一个快速上升阶段。</li>\n<li>当我们清楚了上述规律之后，就能在面对长期的冷寂或挫折失败时做出与他人不同的选择：有人选择放弃，而我们继续坚持。同时，我们不会因自己进步缓慢而沮丧，也不会因别人成长迅速而焦虑。就像写公众号，有耐心的人会牢牢盯住长远价值，他们的目光在 5 年、10 年之后，所以不会因眼下文章的阅读量低而缺失动力，也不会因别人写出了 10W + 的文章（一篇文章的阅读量达到 10 万以上）而焦虑不安，毕竟各自所处的阶段不同，只要持续创造价值，别人的今天就是自己的明天。</li>\n<li>耐心不是毅力带来的结果，而是具有长远目光的结果。</li>\n<li>舒适和诱惑是本能脑与情绪脑的最爱，完全放弃舒适和诱惑就相当于和本能脑、情绪脑直接对抗，很显然，理智脑不是它们的对手，失败是迟早的事。明智的做法是和它们沟通，这也是理智脑最擅长的。就像上面自己和自己对话一样，温和地告诉它们：“该有的享受一点都不会少，只是不是现在享受，而是在完成重要的事情之后。” 这是一个有效的策略，因为放弃享受，它们是不会同意的，但延迟享受，它们是能接受的。</li>\n<li>这种 “后娱乐” 的好处是，将享乐的快感建立在完成重要任务后的成就感之上，很放松、踏实，就像一种奖赏；而 “先娱乐” 虽然刚开始很快活，但精力会无限发散，拖延重要的工作，随着时间的流逝，人会空虚、焦虑。</li>\n<li>耐心就是这样，不能急于求成，允许自己缓慢地改变，甚至经常失败。无论结果如何，和自己对话都会产生效果。</li>\n<li>要想办法看清那些想做之事的意义和好处，你看到的维度越多，耐心就会越强。</li>\n</ul>\n<h3 id=\"第二章-潜意识生命留给我们的彩蛋\"><a class=\"anchor\" href=\"#第二章-潜意识生命留给我们的彩蛋\">#</a> <strong>第二章 潜意识 —— 生命留给我们的彩蛋</strong></h3>\n<ul>\n<li>人生也像是一场消除模糊的比赛，谁的模糊越严重，谁就越混沌；谁的模糊越轻微，谁就越清醒。</li>\n<li>而具体事件一旦变模糊，其边界就会无限扩大，原本并不困难的小事，也会在模糊的潜意识里变得难以解决。</li>\n<li>真正的困难总比想象的要小很多。人们拖延、纠结、畏惧、害怕的根本原因往往不是事情本身有多难，而是内心的想法变得模糊。</li>\n<li>记住，任何痛苦事件都不会自动消失，哪怕再小的事情也是如此。要想不受其困扰，唯一的办法就是正视它、看清它、拆解它、化解它，不给它进入潜意识的机会，不给它变模糊的机会；即使已经进入潜意识，也要想办法将它挖出来。所以，当你感到心里有说不清、道不明的难受的感觉时，赶紧坐下来，向自己提问。・到底是什么让自己烦躁不安？是上台演讲、会见某人，还是思绪纷乱？・具体是什么让自己恐惧担忧？是能力不足、准备不够，还是害怕某事发生？・面对困境，我能做什么？不能做什么？如果做不到或搞砸了，最坏的结果是什么？ 一层层挖下去，直到挖不动为止。坦然地承认、接纳那些难以启齿的想法，让情绪极度透明。虽然直面情绪不会让痛苦马上消失，甚至短时间内还会加剧痛苦，但这会让你主导形势，至少不会被情绪无端恐吓。 恐惧就是一个欺软怕硬的货色，你躲避它，它就张牙舞爪，你正视它，它就原形毕露。一旦把它看得清清楚楚，情绪就会慢慢从潜意识中消散，你的生活将会舒畅无比。</li>\n<li>行动力不足的真正原因是选择模糊。 所谓选择模糊，就是我们在面对众多可能性时无法做出清晰、明确的选择。</li>\n<li>当我们的头脑中有很多模糊的选项时，我们就会不自觉地选择那个最清晰、简单和确定的选项。也就是说，当我们没有足够清晰的指令或者目标时，就很容易选择享乐，放弃那些本该坚持但比较烧脑的选项。 因此，在现代生活中，要想让自己更胜一筹，就必须学会花费更多的脑力和心力去思考如何拥有足够清晰的目标。我们要把目标和过程细化、具体化，在诸多可能性中建立一条单行通道，让自己始终处于 “没得选” 的状态。￼</li>\n<li>“凭感觉” 之所以被称为顶级的方法，是因为它能帮我们感知真正适合自己并需要的东西，让自己处于学习的 “拉伸区”。如果单纯运用理性，我们通常会向优等生看齐，把眼光放在那些最难的题目上，想着如何追赶他们；如果顺从天性，我们就会在最简单的题目上打转（见图 2-1）。 ￼ 图 2-1　理性、感性、天性的选择倾向</li>\n<li>读书方法 —— 只取一个全书最触动自己的点，然后尽可能去实践、改变。这样读书不仅收获更大，而且也不会焦虑。</li>\n<li>学习虽然不是一件轻松的事，但在合适的区域内，我们依旧可以体验到轻松和有趣，如果你感受到的总是痛苦和无趣，那十有八九是感觉不对 —— 要么在困难区煎熬，要么在舒适区打转。</li>\n<li>理智的分析和计算无法解出内心的真正需求，唯有感性的觉知和洞察才能让答案浮出水面。</li>\n<li>直视死亡可以简化一切事物，让自己把注意力重新集中在真正重要的事情上；对于你喜欢的人物，不管是虚构的还是真实的，只要让你深深地着迷，就可以从这些人物身上反射出内心理想的自己</li>\n<li>小事听从你的脑，大事听从你的心。</li>\n</ul>\n<h3 id=\"第三章-元认知人类的终极能力\"><a class=\"anchor\" href=\"#第三章-元认知人类的终极能力\">#</a> <strong>第三章 元认知 —— 人类的终极能力</strong></h3>\n<ul>\n<li>高级的元认知 —— 时刻帮你从高处、深处、远处看待现在的自己，让自己保持清醒、不迷失，保持动力、不懈怠，保持平和、不冲动。</li>\n<li>冥想就是那种只要静坐在某处，然后放松身体，把注意力完全集中到呼吸和感受上的活动。 冥想带来的极度专注可以帮大脑做健身操。通过持续锻炼，大脑可以直接从物理上提升人的元认知能力，如果过程中觉察到自己走神了，我们只需柔和地将注意力拉回来。</li>\n<li>反馈是这个世界的进化机制。有反馈，并形成回路，就可能使任何系统开始自我进化，无论机械设计还是软件系统都是如此。</li>\n<li>元认知能力就是觉察力和自控力的组合。所以从实用角度讲，元认知能力可以被重新定义为：自我审视、主动控制，防止被潜意识左右的能力。</li>\n</ul>\n<h3 id=\"第四章-专注力情绪和智慧的交叉地带\"><a class=\"anchor\" href=\"#第四章-专注力情绪和智慧的交叉地带\">#</a> <strong>第四章 专注力 —— 情绪和智慧的交叉地带</strong></h3>\n<ul>\n<li>目标定义越明确，注意力的感知精度就会越高，精力越集中，技能越精进。如果目标太大，那就将它分解成小目标，这样做也是为了使目标更具体、精细。</li>\n<li>在短时间内投入 100% 的精力比长时间投入 70% 的精力好，因为专注的真正动力并不是毅力和耐心，而是不断发现技巧上的微妙差异和持续存在的关注点，精力越集中则感知越细微。</li>\n<li>先保持极度专注，想不出答案时再将注意力转换到另一件与此毫不相干的事情上。即事前聚精会神，让意识极度投入；事后完全忘记，让意识彻底撒手。这样，灵感和答案就会大概率地出现。</li>\n<li>当人们对当前的活动感到厌倦时，说明应该提高难度；当人们对当前的活动感到焦虑时，说明应该保持这个水平专注练习，如此反复交替就可以让自己进入心流通道，沉浸其中。</li>\n</ul>\n<h3 id=\"第五章-学习力学习不是一味地努力\"><a class=\"anchor\" href=\"#第五章-学习力学习不是一味地努力\">#</a> <strong>第五章 学习力 —— 学习不是一味地努力</strong></h3>\n<ul>\n<li>“教” 是最好的 “学”，如果一件事情你不能讲清楚，十有八九你还没有完全理解。当然，教的最高境界是用最简洁的话让一个外行人明白你讲的东西。</li>\n<li>每读完一本有价值的好书，就用写作的方式把作者的思想用自己的语言重构出来，尽力结合自身经历、学识、立场，去解释、去延伸，而不是简单地把书本的要点进行罗列。</li>\n<li>在重构时，我们可以只取最触动自己的观点，其他观点可以放弃，即使它们很有道理。</li>\n<li>选择一些值得关注的人，和他们保持联结。他们释放的一些有价值的信息会引领我们走向更广阔的世界，但无论如何，最终要自己去读、自己去想、自己去做。</li>\n<li>只有真正和自己有关的内容才对自己有用，在这个注意力非常匮乏的时代，没有必要把所有的书或是书中所有的内容都读完。</li>\n<li>知识的获取不在于多少，而在于是否与自己有关联，以及这种关联有多充分。</li>\n<li>但凡收获一个感悟、了解一个观点或是学到一个知识，只要触动了自己，就要想办法让它效率最大化，而效率最大化的办法就是主动关联到别处，并让自己的行动发生改变。所以你不妨也把这句话当作口头禅，时常问自己：这个道理还能用在什么地方？</li>\n<li>只有当知识能够帮助你做实际决策的时候，它才是你的知识。</li>\n<li>在个人成长领域，没有最优、最确定、最权威的认知体系，只有最适合我们当前状态的认知体系。</li>\n<li>处于认知圈边缘的知识与我们的实际需求贴合得最紧密，因此也更容易让我们产生触动，进而与现有的知识进行关联。</li>\n<li>搭建个人认知体系的真相：打碎各家的认知体系，只取其中最触动自己的点或块，然后将其拼接成自己的认知网络。</li>\n<li>和一般的日志不同，每日反思不是记流水账，而是留意每天最触动自己的那件事，不管是好的启发还是坏的体验，都写下来复盘，写得越细越好。</li>\n<li>用自己的语言重新解释新知识，这会促使自己原有的知识体系对新知识做出反应。</li>\n<li>这就是打卡心态的特性：学不到，假装一下；学到了，立即停止。所以单纯抱着打卡这一任务心态去学习，很少会有强烈的主动性，毕竟在任务心态的驱使下，人们关注的是完成情况，对任务本身没有更大的热情。</li>\n<li>任务心态破坏了身心合一的状态，这种不良体验会加剧人们对学习活动的厌恶感，形成恶性循环。</li>\n<li>我们在任务设置时要使用新策略：设下限，不设上限。</li>\n<li>科学的学习策略是产出作品、获取反馈，驱动本能脑和情绪脑去 “玩玩玩”，而不是一味地努力坚持，让理智脑苦苦地去 “学学学”</li>\n<li>想创造全新的学习动机，就得放弃一味打卡输入的做法，想办法直接运用或产出作品，获取反馈。</li>\n<li>不能一味地学学学而毫无产出，因为没有反馈的学习不仅是痛苦的，而且十有八九会失败。</li>\n<li>・教是最好的学；・用是最好的学；・输出倒逼输入；・请用作品说话……</li>\n<li>分享不是随意分享半成品，而是尽最大力气将作品打磨成自己当前能力范围内可完成的最好的样子。</li>\n<li>古典的跃迁理论：打磨作品 — 到达一个小山的头部 — 受到更多关注 — 移动到一个更大山头的头部 — 借助系统推力，实现人生跃迁。</li>\n<li>有效学习的关键是保持极度专注，而非一味比拼毅力和耐心。</li>\n<li>轻松的学霸，他们学习时从不过度消耗自己，只要感到精力不足，就停下来主动休息，这反而使他们精力桶的水位得到快速回升。</li>\n<li>让自己先尽力保持短时间的极度专注，到有些累的时候就主动停下来，这是更加明智的生活和学习策略。</li>\n<li>太容易的内容会让人因无聊而走神，太困难的内容会让人因畏惧而逃避，所以选择做那些 “跳一跳就能够得着” 的学习或工作，是最容易进入专注状态</li>\n</ul>\n<h3 id=\"第六章-行动力没有行动世界只是个概念\"><a class=\"anchor\" href=\"#第六章-行动力没有行动世界只是个概念\">#</a> <strong>第六章 行动力 —— 没有行动世界只是个概念</strong></h3>\n<ul>\n<li>在初始阶段，强迫自己先做重要的事情，一旦进入正向的增强回路，你便能拥有强大的行动力 —— 这正是增强自制力、提升行动力的秘密</li>\n<li>清晰力，也就是把目标细化、具体化的能力 —— 行动力只有在清晰力的支撑下才能得到重构。</li>\n<li>从某种程度上说，有自己热爱的事，比行动力本身要重要得多，因为一旦有了热情，你就会自带 “要事第一” 和 “提高清晰力” 等各种属性。</li>\n<li>正如爱因斯坦所说：“如果给我 1 小时解答一道决定我生死的问题，我会花 55 分钟弄清楚这道题到底在问什么。一旦清楚它到底在问什么，剩下的 5 分钟足够回答这个问题。”</li>\n<li>想先看到结果再行动的人往往无法看到结果。耍小聪明的人会因为结果不明朗，担心付出没有回报，所以不愿行动，以致永远停留在原地</li>\n<li>不要垂涎别人二十几岁身家百万，不要羡慕别人一夜成名，他们的故事若无法真实地改变你，那对你而言都是幻想。还不如踏踏实实地用行动让自己一点一点变好，毕竟，现实结果才是最好的 “评判师”。</li>\n<li>比如我开始早起和跑步的时候，起初是有一些痛苦的，但扛过去之后，我就体会到了早起和跑步给自己身心带来的不可思议的体验。回头想，最难坚持的时候可能就是突破阈值的时候</li>\n<li>如果你觉得别人讲的道理有理有据，而自己暂时无法反驳，碰巧自己又非常想做这件事，那就相信他们说的是对的，然后笃定地行动</li>\n<li>在实践途中，你自然也要保持思考，用行动反复验证他们的理论，不适则改、适则用，直到自己真正做到为止。届时你不仅能做成那件事，还能探索出自己的理论，成为别人眼中的高手。</li>\n<li>更使人困扰的是，道理知道得越多，行动力反而越弱，因为似乎总有更好的道理等着我们去发现。</li>\n<li>不发生真正改变的学习都是无效的学习</li>\n<li>懂得百点不如改变一点。真正的成长不在于自己懂得了多少道理，而在于自己改变了多少。</li>\n</ul>\n<h3 id=\"第七章-情绪力情绪是多角度看问题的智慧\"><a class=\"anchor\" href=\"#第七章-情绪力情绪是多角度看问题的智慧\">#</a> <strong>第七章 情绪力 —— 情绪是多角度看问题的智慧</strong></h3>\n<ul>\n<li>当一个人同时面临很多任务的时候，他的心智带宽就会降低，反而没有了行动力和自控力。有生活经验的人都会尽量克制自己的欲望，在做重要之事的同时主动安排娱乐活动，尽量保持日程的闲余 —— 这种方法是科学的、智慧的。</li>\n<li>真正的行动力高手不是有能耐在同一时间做很多事的人，而是会想办法避免同时做很多事的人。这样的人自然不会把自己的日程安排得太满，无论做学习计划，还是做工作安排，他们都会给自己留足够的闲余，让自己从容地面对每一刻。</li>\n<li>适当的闲余是我们应对压力和意外的宝贵资源，但是过多的闲余可不是什么好事，如果有大量的金钱，就容易萌生无谓的欲望；有大量的时间，也容易陷入低效的状态。</li>\n<li>那些习惯从单一角度识人的人，往往比较单纯，也更容易受伤，本质上是因为他们缺乏多角度认知事物的意识。</li>\n<li>每个人因为生活环境不同、经历不同、学识不同，所以在看待同一个问题时，理解层次和还原程度也不尽相同。</li>\n<li>困难和压力总能把人的情绪和注意力抓得死死的，让你很难看到其他角度。</li>\n<li>这个世界比我们想象的要积极，我们以为自己没得选，其实还有很多角度可供选择，毕竟任何事物都是多维、立体的。看似悲观的事物背后肯定有乐观的一面，严肃事物的背后必然有好玩的一面，我们暂时看不见不代表它不存在。</li>\n<li>当你遇到那些 “不想做但必须做” 的事情时，只要在心里默念一句 “咒语”，就可以让自己跳出事情本身。这句 “咒语” 便是：我并不是在做这件事，我只是在做另外一件事。</li>\n<li>产生内部动机最好的方式莫过于立足于让自己变好。</li>\n<li>当人的注意力都在享受上时，他对跑步的心态就不一样了。相比起来，别人为了身材和身体苦苦坚持，而他只是享受愉快的跑步过程。</li>\n<li>比如阅读这件事。我从来不认为自己是在阅读，而是设想自己在和智者聊天。</li>\n<li>事实上，人是一种自我解释的动物，世界的意义是人类赋予的。</li>\n</ul>\n<p></p>\n",
            "tags": [
                "读书"
            ]
        },
        {
            "id": "https://yunhdan.github.io/cs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E7%90%86%E8%AE%BA/",
            "url": "https://yunhdan.github.io/cs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E7%90%86%E8%AE%BA/",
            "title": "计算机网络理论",
            "date_published": "2025-08-04T07:09:14.000Z",
            "content_html": "<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h1 id=\"计算机网络体系结构\"><a class=\"anchor\" href=\"#计算机网络体系结构\">#</a> 计算机网络体系结构</h1>\n<div class=\"note success\">\n<p>互联网相较互连网，范围更大。互连网是多个互连网互连而来的。<br />\n互联网由核心部分与边缘部分组成，边缘部分主要是大部分的主机构成，核心部分主要是大部分的路由器组成。<br />\nISP 又称互联网服务提供者，比如中国电信，中国移动，中国联通。</p>\n</div>\n<ol>\n<li class=\"quiz fill\">\n<p>互联网两个重要特点是<span class=\"gap\">连通性和共享</span> </p>\n</li>\n<li class=\"quiz fill\">\n<p>计算机之间的通信，本质上是<span class=\"gap\">进程和进程</span>之间的通信。</p>\n</li>\n<li class=\"quiz fill\">\n<p>主机与主机之间的通信方式有两种，分别是<span class=\"gap\">客户 - 服务器方式（C/S）</span>和<span class=\"gap\">对等方式（P2P）</span></p>\n</li>\n<li class=\"quiz fill\">\n<p>电路交换，表现在链路上的一个重要特点是<span class=\"gap\">通话全部时间内链路始终被两端用户占用</span></p>\n</li>\n</ol>\n<div class=\"note success\">\n<p>分组转发采用的是存储转发技术。<br />\n1GB = 1024MB = <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mn>2</mn><mn>10</mn></msup></mrow><annotation encoding=\"application/x-tex\">2^{10}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8141079999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\">2</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">1</span><span class=\"mord mtight\">0</span></span></span></span></span></span></span></span></span></span></span></span>MB = <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mn>2</mn><mn>20</mn></msup></mrow><annotation encoding=\"application/x-tex\">2^{20}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8141079999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\">2</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">2</span><span class=\"mord mtight\">0</span></span></span></span></span></span></span></span></span></span></span></span>KB = <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mn>2</mn><mn>30</mn></msup></mrow><annotation encoding=\"application/x-tex\">2^{30}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8141079999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\">2</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">3</span><span class=\"mord mtight\">0</span></span></span></span></span></span></span></span></span></span></span></span> B。（存储 - 字节视角）<br />\n1Gb = 10^3 Mb = 10^6 Kb = 10^9 b。（网络 - 比特视角）</p>\n</div>\n<ol start=\"5\">\n<li class=\"quiz multi\">\n<p>分组转发有哪四大优点？</p>\n<ul class=\"options\">\n<li>方便</li>\n<li class=\"correct\">高效</li>\n<li class=\"correct\">迅速</li>\n<li class=\"correct\">可靠</li>\n<li class=\"correct\">灵活</li>\n</ul>\n</li>\n<li class=\"quiz multi\">\n<p>计算机网络有哪些性能指标？</p>\n<ul class=\"options\">\n<li class=\"correct\">速率</li>\n<li class=\"correct\">带宽</li>\n<li class=\"correct\">吞吐量</li>\n<li class=\"correct\">时延</li>\n<li class=\"correct\">往返时间 RTT</li>\n</ul>\n<blockquote>\n<p>还有利用率、时延带宽积这两个指标。<br />\n时延又有分为发送时延、传播时延、排队时延、处理时延。<br />\n带宽是某个信道，单位时间内最大通过数据量，和速率一样单位是 bit/s。吞吐量是指单位时间内通过某个网络的实际数据量，更强调实际，单位是 bit。<br />\n时延带宽积正如其名，是传播时延与带宽的乘积，反映的是某条链路最大的比特容量。往返时间 RTT 是指发送方发送信息开始，到收到对方确认信息所花的时间。</p>\n</blockquote>\n</li>\n</ol>\n<div class=\"note success\">\n<p>OSI 和 TCP/IP 模型主要有五层：应用层、传输层、网络层、数据链路层、物理层。当然还有一些辅助的层：表示层、会话层。这两个暂时不是学习重点。</p>\n</div>\n<ol start=\"7\">\n<li class=\"quiz fill\">\n<p>应用层为<span class=\"gap\">应用程序</span>提供数据传输服务，数据单位是报文。</p>\n</li>\n<li class=\"quiz fill\">\n<p>传输层为<span class=\"gap\">进程</span>提供端到端的数据传输服务，数据单位是报文段。</p>\n</li>\n<li class=\"quiz fill\">\n<p>网络层主要为<span class=\"gap\">主机</span>提供数据传输服务，数据单位是数据包（packet—— 分组交换的数据单元）。</p>\n</li>\n<li class=\"quiz fill\">\n<p>数据链路层为同一链路的<span class=\"gap\">主机</span>提供数据传输服务，数据单位是帧。</p>\n</li>\n</ol>\n<div class=\"note success\">\n<p>物理层主要负责在物理介质上传输数据比特流。</p>\n</div>\n<ol start=\"11\">\n<li class=\"quiz fill\">\n<p>为什么计算机网络要分层？<span class=\"gap\">（点我显示答案）</span></p>\n<blockquote>\n<ol>\n<li>各层可以相互独立，上下层只需要知道对方的接口即可，无需知道内部实现。</li>\n<li>灵活性好，某层要修改内部实现时，无需更改接口。</li>\n<li>易于实现和维护，整个系统被划分为多个独立的子系统。</li>\n<li>方便标准化，各层可以通过协议说明各自的功能和服务。</li>\n<li>分层使得更具扩展性。</li>\n</ol>\n</blockquote>\n</li>\n<li class=\"quiz fill\">\n<p>如果数据帧长度为 1Kb，发送速率是 200bit/s，求发送时延。<span class=\"gap\">（点我显示答案）</span></p>\n<blockquote>\n<p>发送时延 = 数据帧长度（bit） / 发送速率（bit /s）。因此答案为 5s。</p>\n</blockquote>\n</li>\n<li class=\"quiz true\">\n<p>传播时延随着信道的长度增大而增大。</p>\n<blockquote>\n<p>传播时延 = 信道长度（m） / 光速（m/s）。</p>\n</blockquote>\n</li>\n<li class=\"quiz false\">\n<p>信道利用率增加，则该信道的时延降低。</p>\n<blockquote>\n<p>时延应该更高，公式反映为：当前时延 D = 空闲时延 D0 / （1 - 利用率 U）。空闲时延 D0 一般是不变的。</p>\n</blockquote>\n</li>\n</ol>\n<h1 id=\"应用层\"><a class=\"anchor\" href=\"#应用层\">#</a> 应用层</h1>\n<ol start=\"15\">\n<li></li>\n</ol>\n",
            "tags": [
                "计算机"
            ]
        },
        {
            "id": "https://yunhdan.github.io/cs/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/",
            "url": "https://yunhdan.github.io/cs/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/",
            "title": "图像处理",
            "date_published": "2025-07-26T09:19:37.000Z",
            "content_html": "<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h1 id=\"小波变换\"><a class=\"anchor\" href=\"#小波变换\">#</a> 小波变换</h1>\n<ul>\n<li>十分易懂的关于小波变换的介绍，可以形成初步的感性上的认知：<span class=\"exturl\" data-url=\"aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8yMjQ1MDgxOA==\">https://zhuanlan.zhihu.com/p/22450818</span></li>\n<li>这一篇在第一篇的基础上，深入了更多的内容：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxOTkwMjk0L2FydGljbGUvZGV0YWlscy8xMTQyMzg1MTU=\">https://blog.csdn.net/qq_41990294/article/details/114238515</span></li>\n</ul>\n<details class=\"info\"><summary>Summary</summary><div>\n<ul>\n<li>小波变换的完全展开由母小波（波函数 <code>wavelet function</code> ）和父小波（尺度函数 <code>scale function</code> ）定义，wavelet function 等同于对信号做高通滤波保留变化细节，而 scaling function 等同于对信号做低通滤波保留平滑的 shape。</li>\n</ul>\n</div></details>\n<ul>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L20wXzM3NjA1NjQyL2FydGljbGUvZGV0YWlscy8xMzU1OTgwNTc=\">https://blog.csdn.net/m0_37605642/article/details/135598057</span></li>\n</ul>\n",
            "tags": [
                "计算机"
            ]
        },
        {
            "id": "https://yunhdan.github.io/other/%E8%B0%88%E5%88%A4%E6%8A%80%E5%B7%A7/",
            "url": "https://yunhdan.github.io/other/%E8%B0%88%E5%88%A4%E6%8A%80%E5%B7%A7/",
            "title": "实用谈判技巧",
            "date_published": "2025-07-16T06:42:42.535Z",
            "content_html": "<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><div class=\"note info\">\n<p>学习自白斌老师的谈判实用技巧。</p>\n</div>\n<ul>\n<li>移花接木：若对方的要求十分难以接受，当面拒绝不合适时，可以首先进行同意，然后再提出一个让对方难以接受的条件进行拒绝。“送我宝马”“可以，但有个条件，我要让你送我一栋别墅”。</li>\n<li>降低对方期待值：如果对方想要让你接受一个比较坏的结果，你可以告知他接受该结果后的一个更坏的局面。反之亦然，让对方接受一个东西，若对方不接受，则告知将会面临一个更大的后果。</li>\n</ul>\n",
            "tags": [
                "琐碎"
            ]
        },
        {
            "id": "https://yunhdan.github.io/research/%E6%9C%9F%E5%88%8A%E3%80%81%E4%BC%9A%E8%AE%AE%E6%8A%95%E7%A8%BF%E7%BB%8F%E9%AA%8C/",
            "url": "https://yunhdan.github.io/research/%E6%9C%9F%E5%88%8A%E3%80%81%E4%BC%9A%E8%AE%AE%E6%8A%95%E7%A8%BF%E7%BB%8F%E9%AA%8C/",
            "title": "期刊、会议投稿经验",
            "date_published": "2025-07-07T07:44:16.000Z",
            "content_html": "<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h1 id=\"rebuttal经验\"><a class=\"anchor\" href=\"#rebuttal经验\">#</a> Rebuttal 经验</h1>\n<ul>\n<li>基础的关于 Rebuttal 的介绍，以及一些有用经验，可以当作入门学习，便于建立基本的认识：<span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cuemhpaHUuY29tL3RhcmRpcy96bS9hcnQvMTA0Mjk4OTIzP3NvdXJjZV9pZD0xMDA1\">https://www.zhihu.com/tardis/zm/art/104298923?source_id=1005</span></li>\n<li>《站着，还把论文中了》，浓缩 Rebuttal 时期中的精华：<span class=\"exturl\" data-url=\"aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC82MDIwMjQ0ODklRTMlODAlODIlRTQlQkIlQTUlRTUlOEYlOEElRTUlOEYlQTYlRTUlQTQlOTYlRTclOUElODQlRTQlQjglODAlRTQlQjglQUElRTUlODUlQjMlRTQlQkElOEVSZWJ1dHRhbCVFNyU5QSU4NCVFNSU5QiU5RSVFNyVBRCU5NCVFRiVCQyU4QyVFNCVCOSU5RiVFNiU5OCVBRiVFNiVCNSU5MyVFNyVCQyVBOSVFOCU4OCVBQyVFNyU5QSU4NCVFNyVCMiVCRSVFNSU4RCU4RSVFRiVCQyU5QWh0dHBzOi8vd3d3LnpoaWh1LmNvbS9xdWVzdGlvbi8zMjA1NTk5Ni9hbnN3ZXIvNDg2NzUzNDExMzY=\">https://zhuanlan.zhihu.com/p/602024489。以及另外的一个关于 Rebuttal 的回答，也是浓缩般的精华：https://www.zhihu.com/question/32055996/answer/48675341136</span></li>\n<li>和 Rebuttal 格式与模板有关的教程：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxODk1MDAzL2FydGljbGUvZGV0YWlscy8xMzUwNTA5NTc=\">https://blog.csdn.net/qq_41895003/article/details/135050957</span></li>\n</ul>\n<h1 id=\"一些rebuttal经历\"><a class=\"anchor\" href=\"#一些rebuttal经历\">#</a> 一些 Rebuttal 经历</h1>\n<ul>\n<li>不建议学，算是特殊情况，只有在审稿人十分脑残时才用：<span class=\"exturl\" data-url=\"aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8zNTM3NjE5MjA=\">https://zhuanlan.zhihu.com/p/353761920</span></li>\n</ul>\n",
            "tags": [
                "学术"
            ]
        },
        {
            "id": "https://yunhdan.github.io/other/%E6%80%9D%E6%83%B3%E5%BB%BA%E8%AE%BE/",
            "url": "https://yunhdan.github.io/other/%E6%80%9D%E6%83%B3%E5%BB%BA%E8%AE%BE/",
            "title": "思想建设",
            "date_published": "2025-07-07T07:43:52.000Z",
            "content_html": "<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><div class=\"note info\">\n<p>自有记载以来，所有宝贵的思想。</p>\n</div>\n<h1 id=\"2024-05\"><a class=\"anchor\" href=\"#2024-05\">#</a>  <code>2024-05</code></h1>\n<ul>\n<li>不要看外界认为对方很牛逼你就觉得牛逼，而是要亲身与之经历后，实实在在因为某个地方你觉得对方牛逼而牛逼。</li>\n</ul>\n<h1 id=\"2024-07\"><a class=\"anchor\" href=\"#2024-07\">#</a>  <code>2024-07</code></h1>\n<ul>\n<li>如果心中感觉到困惑、混乱、不知所从，可以尝试将心中的问题清晰、明确化，多问是什么、为什么、怎么办，在纸上写出来。</li>\n<li>人感到困惑是很正常的，本质上是新旧思想冲突的表现。</li>\n</ul>\n<h1 id=\"2024-08\"><a class=\"anchor\" href=\"#2024-08\">#</a>  <code>2024-08</code></h1>\n<ul>\n<li>要尊重、留心、观察自己一瞬而过的小想法，说不定就能萌发很多灵感。</li>\n</ul>\n<h1 id=\"2024-10\"><a class=\"anchor\" href=\"#2024-10\">#</a>  <code>2024-10</code></h1>\n<ul>\n<li>什么是稀缺人才，要么在某个领域做到顶尖，要么在交叉领域做到第一梯队。</li>\n</ul>\n<h1 id=\"2024-11\"><a class=\"anchor\" href=\"#2024-11\">#</a>  <code>2024-11</code></h1>\n<ul>\n<li>人际交往的本质是利益交换。建立合作关系的前提是信任，而信任的前提是对对方有充分的认识。</li>\n</ul>\n<h1 id=\"2025-04\"><a class=\"anchor\" href=\"#2025-04\">#</a>  <code>2025-04</code></h1>\n<ul>\n<li>尽可能要让一个环境内的资源向自己倾斜，尽管这很难，但这是相当重要的，而且这不仅是一种能力和本事，更是一种意识。</li>\n</ul>\n<h1 id=\"2025-07\"><a class=\"anchor\" href=\"#2025-07\">#</a>  <code>2025-07</code></h1>\n<ul>\n<li>在绝境中，与其认定自己已经注定败局，不如相信自己将来能赢。</li>\n<li>一切事物都有两面性，正如一条发布到网上的评论，无论有多少人喷，一定会有站在你的立场，理解你支持你的人。风险与机遇并存，巨大的挑战背景下，一定伴有着令人垂涎的宝物。</li>\n<li>想让一个人成长、变强很简单，就是让他主动承担，担任领头人，主负责人。</li>\n</ul>\n",
            "tags": [
                "琐碎"
            ]
        },
        {
            "id": "https://yunhdan.github.io/research/2025%20CVPR%20NTIRE%20Image%20Denoise%E6%8A%A5%E5%91%8A%E7%A0%94%E7%A9%B6%E6%80%9D%E8%80%83/",
            "url": "https://yunhdan.github.io/research/2025%20CVPR%20NTIRE%20Image%20Denoise%E6%8A%A5%E5%91%8A%E7%A0%94%E7%A9%B6%E6%80%9D%E8%80%83/",
            "title": "2025 CVPR NTIRE Image Denoise报告研究思考",
            "date_published": "2025-07-02T14:38:47.000Z",
            "content_html": "<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><div class=\"note info\">\n<p>这次的挑战不考虑计算效率和模型复杂度，所以用混合架构以最大化性能需求。</p>\n</div>\n<p>排名情况：</p>\n<p><img data-src=\"../../assets/2025NtireDenoise4.png\" alt=\"\" /></p>\n<h1 id=\"第一名自制200万张训练集src-b\"><a class=\"anchor\" href=\"#第一名自制200万张训练集src-b\">#</a> 第一名：自制 200 万张训练集（ <code>SRC-B</code> ）</h1>\n<p><img data-src=\"../../assets/2025NtireDenoise.png\" alt=\"\" /></p>\n<ul>\n<li>网络结构如上图所示，先使用基于 Transformer 的 <code>Restormer</code>  提取全局信息，然后再使用基于卷积的 <code>NAFNet</code>  进行局部细节的增强，最后使用特征融合模块整合上述两部分的特征，平衡去噪和细节保留，提升整体性能。</li>\n<li>用自制 200 万张训练集进行训练整个网络，再使用官方的训练集集微调细节增强网络，最后用官方验证集和自定义训练集中的 1000 张图像继续微调整个网络。</li>\n<li>对数据进行了一定的筛选，选择拉普拉斯 Var、 <code>BRISQUE</code>  和 <code>NIQE</code>  三个指标排名前 30% 的图像。另外，基于 Clip 特征进行了语义选择，保证数据集反映各种场景的多样性。</li>\n<li>使用渐进式学习， <code>patch</code>  从 256 到 448 和 768。</li>\n</ul>\n<h1 id=\"第二名混合架构与集成学习的典范snucv\"><a class=\"anchor\" href=\"#第二名混合架构与集成学习的典范snucv\">#</a> 第二名：混合架构与集成学习的典范（ <code>SNUCV</code> ）</h1>\n<p><img data-src=\"../../assets/2025NtireDenoise2.png\" alt=\"\" /></p>\n<ul>\n<li><code>MambaIRv2+Xformer+Restormer</code>  混合架构，最后再利用 <code>Xformer</code>  集成。这三个网络的最深层特征被拼接到一起，然后集成到集成模型的最深层特征中，这种做法有点类似于残差学习，确保前面网络的特征能够有效、无损失地保留到后面的集成模型中。</li>\n<li>依然是渐进式训练，patch 大小逐渐增加为 [128, 160, 192, 256, 320, 384]，对应的 batch 大小为 [8, 5, 4, 2, 1, 1]。训练是先训练前面三个网络，然后再单独训练后面的集成模型，如果前面三个网络没有训练好，后面的集成模型训练是没有意义的。</li>\n<li>前面的去噪模型使用 <code>L1</code>  损失训练，后面的集成模型使用 <code>L1</code> 、 <code>MSE</code>  和高频损失组合训练。</li>\n</ul>\n<h1 id=\"第三名边缘特征学习buptmm\"><a class=\"anchor\" href=\"#第三名边缘特征学习buptmm\">#</a> 第三名：边缘特征学习（ <code>BuptMM</code> ）</h1>\n<p><img data-src=\"../../assets/2025NtireDenoise3.png\" alt=\"\" /></p>\n<ul>\n<li>\n<p>这个模型在推理的时候用 TLC 技术增强 <code>Restormer</code>  的推理性能。这是一种通过去除纹理中的退化表征来提升图像恢复效果的一种技术。</p>\n</li>\n<li>\n<p>分别训练 <code>Restormer</code>  和 HAT，而且分别都用渐进式的训练方法。HAT 是一个通过激活像素的方式提升 Transformer 在图像恢复上性能的方法。</p>\n</li>\n<li>\n<p>对 <code>Restormer</code>  和 HAT 的输出结果用 Canny 算子处理为边缘纹理二值图，然后进行 OR 和 XOR 操作，边缘像素点统一使用 HAT 的输出，而其他非边缘位置像素点，取两个模型输出的均值。可能是考虑到 <code>Restormer</code>  的全局注意力可能会过度平滑细节，导致边缘模糊。具体的步骤可以参考 <code>Deepseek</code>  的分析：</p>\n<p><img data-src=\"../../assets/2025NtireDenoise7.png\" alt=\"\" /></p>\n</li>\n</ul>\n<h1 id=\"第四名直筒型依然有效hmidenoise\"><a class=\"anchor\" href=\"#第四名直筒型依然有效hmidenoise\">#</a> 第四名：直筒型依然有效（ <code>HMiDenoise</code> ）</h1>\n<p><img data-src=\"../../assets/2025NtireDenoise5.png\" alt=\"\" /></p>\n<ul>\n<li>依然有渐进式训练，随着 patch size 增大，batch size 减小。</li>\n<li><code>RHAG</code>  是基于 HAT 的改进版本，也是类似于 Transformer 的架构。可见直筒型在图像恢复的重要性依然很明显。</li>\n</ul>\n<h1 id=\"第五名纯restormer数据增强pixel-purifiers\"><a class=\"anchor\" href=\"#第五名纯restormer数据增强pixel-purifiers\">#</a> 第五名：纯 <code>Restormer</code>  + 数据增强（Pixel Purifiers）</h1>\n<p><img data-src=\"../../assets/2025NtireDenoise6.png\" alt=\"\" /></p>\n<ul>\n<li>模型架构是原封不动的 Restormer。</li>\n<li>使用硬数据挖掘技术提升 PSNR。目前我的理解是在训练集中挑出训练损失较高的 patch，统一放到最后训练完后的进一步微调，做迁移学习，提升泛化性。</li>\n<li>推理采用分块推理，将大尺寸图像分割为与训练相同尺寸的小块（patch），逐块输入模型。因为块和块之间的边界拼接容易带来伪影，所以相邻的块设置重叠区域，重叠宽度大，图像质量高，但速度慢。最终拼接时对重叠区域的像素进行加权平均。</li>\n<li>在推理时使用自集成技术提升推理性能。</li>\n</ul>\n<h1 id=\"第六名纯restormer迁移学习alwaysu\"><a class=\"anchor\" href=\"#第六名纯restormer迁移学习alwaysu\">#</a> 第六名：纯 <code>Restormer</code>  + 迁移学习 ( <code>Alwaysu</code> )</h1>\n<ul>\n<li>架构采用原封不动的 Restormer。</li>\n<li>预训练的 Restormer 并未对噪声图像进行强度裁剪，因此性能较差。如果要对比赛的数据集采用强度裁剪，同时又要使用冻结的 Restormer，那么就要解决数据分布不匹配的问题，需要 Restormer 适应新的经过强度裁剪的数据。</li>\n<li>为什么直接让预训练的 Restormer 在新的数据上微调、更新不合适？因为这需要更新所有的参数，计算成本高，而且容易破坏预训练的知识。</li>\n<li>冻结主体的 Restormer 权重，然后在现有网络的层添加偏置参数，作为一种分布适配器，也相当于为原网络增加自适应校准层，将预训练知识适配新的噪声分布。</li>\n<li>在推理依然采用分块推理、小重叠、自集成技术。</li>\n</ul>\n<h1 id=\"第七名xxirxxirxxxxirtcler-denosing\"><a class=\"anchor\" href=\"#第七名xxirxxirxxxxirtcler-denosing\">#</a> 第七名：xxIR+xxIR=xxxxIR（ <code>Tcler Denosing</code> ）</h1>\n<ul>\n<li>模型结构采用 PromptIR+MambaIRv2 的方案，最后融合两个模型的恢复结果。这个好像有点类似于集成学习。</li>\n<li>训练策略依然采用渐进式的训练，使用的损失函数不是一般的 L1 损失，是两个改进的 L1 损失：Charbonnier loss 和 Gradient-weighted L1 loss。</li>\n<li>使用了 USM sharpen 这个数据增强方法。</li>\n<li>推理采用了自集成策略。</li>\n<li>貌似 MambaIRv2 解决了 MambaIR 的因果长局建模问题。</li>\n</ul>\n<h1 id=\"第八名cipher-vision\"><a class=\"anchor\" href=\"#第八名cipher-vision\">#</a> 第八名：（ <code>cipher vision</code> ）</h1>\n<p><img data-src=\"../../assets/2025NtireDenoise8.png\" alt=\"\" /></p>\n",
            "tags": [
                "学术"
            ]
        },
        {
            "id": "https://yunhdan.github.io/cs/Linux/",
            "url": "https://yunhdan.github.io/cs/Linux/",
            "title": "Linux",
            "date_published": "2025-07-01T15:39:51.000Z",
            "content_html": "<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h1 id=\"关于移动命令mv\"><a class=\"anchor\" href=\"#关于移动命令mv\">#</a> 关于移动命令 <code>mv</code></h1>\n<p>移动某个文件夹中所有内容到另一个文件夹内：</p>\n<figure class=\"highlight bash\"><figcaption data-lang=\"bash\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token function\">mv</span> /path/to/source/folder/* /path/to/destination/folder/</pre></td></tr></table></figure><p>但是，如果源文件夹的文件量过大，就会报这样的错误：</p>\n<figure class=\"highlight bash\"><figcaption data-lang=\"bash\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>bash: /usr/bin/mv: 参数列表过长</pre></td></tr></table></figure><p>当使用通配符  <code>*</code>  扩展的文件数量超过了系统允许的最大命令行参数限制时就会报这个错误。这是 Linux 系统的一个保护机制，防止用户意外执行可能破坏系统的命令。</p>\n<p>用这个命令替代以解决：</p>\n<figure class=\"highlight bash\"><figcaption data-lang=\"bash\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token function\">find</span> /path/to/source/folder/* <span class=\"token parameter variable\">-mindepth</span> <span class=\"token number\">1</span> <span class=\"token parameter variable\">-maxdepth</span> <span class=\"token number\">1</span> <span class=\"token parameter variable\">-print0</span> <span class=\"token operator\">|</span> <span class=\"token function\">xargs</span> <span class=\"token parameter variable\">-0</span> <span class=\"token function\">mv</span> <span class=\"token parameter variable\">-t</span> /path/to/destination/folder/</pre></td></tr></table></figure><h1 id=\"查看某个文件夹下有多少个文件\"><a class=\"anchor\" href=\"#查看某个文件夹下有多少个文件\">#</a> 查看某个文件夹下有多少个文件</h1>\n<p>配合 <code>ls</code>  和 <code>wc</code>  使用：</p>\n<figure class=\"highlight bash\"><figcaption data-lang=\"bash\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token function\">ls</span> /path/to/source/folder <span class=\"token operator\">|</span> <span class=\"token function\">wc</span> <span class=\"token parameter variable\">-l</span></pre></td></tr></table></figure><h1 id=\"export-pythonpathpwdpythonpath\"><a class=\"anchor\" href=\"#export-pythonpathpwdpythonpath\">#</a>  <code>export PYTHONPATH=&quot;$PWD:$PYTHONPATH&quot;</code></h1>\n<p>这是一个通过 <code>PYTHONPATH</code>  手动指定项目根目录的命令。</p>\n<p>以深度学习项目 <code>Retinexformer</code>  为例，这个项目文件夹内包含了训练的代码 <code>train.py</code> ，以及模型架构文件等。</p>\n<p>有时候直接在终端通过 <code>python train.py</code>  的绝对路径会报一些代码文件中的库引用错误，但是你反复检查了路径，觉得代码里导包的方式没问题。</p>\n<p>这时候你就可以先通过 <code>cd</code>  命令进入项目文件夹中，然后再执行这个 <code>export</code>  命令手动指定项目根目录：</p>\n<figure class=\"highlight bash\"><figcaption data-lang=\"bash\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token operator\">></span> <span class=\"token builtin class-name\">cd</span> Retinexformer</pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token operator\">></span> <span class=\"token builtin class-name\">export</span> <span class=\"token assign-left variable\">PYTHONPATH</span><span class=\"token operator\">=</span><span class=\"token string\">\"<span class=\"token environment constant\">$PWD</span>:<span class=\"token variable\">$PYTHONPATH</span>\"</span></pre></td></tr></table></figure><h1 id=\"当linux内存不足时扩大交换空间\"><a class=\"anchor\" href=\"#当linux内存不足时扩大交换空间\">#</a> 当 <code>Linux</code>  内存不足时，扩大交换空间</h1>\n<p>首先要处理一下现有的交换文件：</p>\n<figure class=\"highlight bash\"><figcaption data-lang=\"bash\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token comment\"># 查看当前启用的交换空间</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token function\">sudo</span> <span class=\"token function\">swapon</span> <span class=\"token parameter variable\">--show</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre></pre></td></tr><tr><td data-num=\"4\"></td><td><pre><span class=\"token comment\"># 如果存在 /swapfile，先关闭交换文件</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre><span class=\"token function\">sudo</span> swapoff /swapfile</pre></td></tr><tr><td data-num=\"6\"></td><td><pre></pre></td></tr><tr><td data-num=\"7\"></td><td><pre><span class=\"token comment\"># 删除旧的交换文件</span></pre></td></tr><tr><td data-num=\"8\"></td><td><pre><span class=\"token function\">sudo</span> <span class=\"token function\">rm</span> /swapfile</pre></td></tr></table></figure><p>然后创建一个更大内存的交换文件：</p>\n<figure class=\"highlight bash\"><figcaption data-lang=\"bash\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token comment\"># 创建 8GB 交换文件</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token function\">sudo</span> fallocate <span class=\"token parameter variable\">-l</span> 8G /swapfile</pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token function\">sudo</span> <span class=\"token function\">chmod</span> <span class=\"token number\">600</span> /swapfile</pre></td></tr><tr><td data-num=\"4\"></td><td><pre><span class=\"token function\">sudo</span> <span class=\"token function\">mkswap</span> /swapfile</pre></td></tr><tr><td data-num=\"5\"></td><td><pre><span class=\"token function\">sudo</span> <span class=\"token function\">swapon</span> /swapfile</pre></td></tr></table></figure>",
            "tags": [
                "计算机"
            ]
        },
        {
            "id": "https://yunhdan.github.io/other/%E7%BE%BD%E6%AF%9B%E7%90%832/",
            "url": "https://yunhdan.github.io/other/%E7%BE%BD%E6%AF%9B%E7%90%832/",
            "title": "羽毛球2",
            "date_published": "2025-06-23T15:15:22.000Z",
            "content_html": "<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><div class=\"note info\">\n<p>25 年 6 月 23 日，今日单挑一名对手，明明感觉并不强，差不多，但是就是打不过。经过反思，在全方面的技术上要落后一筹，故决定今后系统地学习羽毛球。</p>\n</div>\n<h1 id=\"正手龙卷风搓球\"><a class=\"anchor\" href=\"#正手龙卷风搓球\">#</a> 正手龙卷风搓球</h1>\n<p>\n<div style=\"position: relative; width: 100%; height: 0; padding-bottom: 75%;\">\n<iframe src=\"//player.bilibili.com/player.html?isOutside=true&aid=113409198789085&bvid=BV1Z9SZYdEBn&cid=26571309405&p=1\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\"\nstyle=\"position:absolute; height: 100%; width: 100%;\"></iframe>\n</div>\n</p>\n<p>总结要点：</p>\n<ul>\n<li>跑到位，抢到高点。</li>\n<li>横着搓，搓球点在胸口。如果球低，就是压腿降低重心，搓球点保持在胸口，肩以上。</li>\n<li>龙卷风式搓球转的关键是，引完拍，球就差不多快接触到拍面，这样一刚要发力就会搓到，这样就会很转。</li>\n<li>龙卷风式搓球一般在身体正前面搓。</li>\n</ul>\n<h1 id=\"展搓\"><a class=\"anchor\" href=\"#展搓\">#</a> 展搓</h1>\n<p>\n<div style=\"position: relative; width: 100%; height: 0; padding-bottom: 75%;\">\n<iframe src=\"//player.bilibili.com/player.html?isOutside=true&aid=1606338008&bvid=BV1Tm42137xB&cid=1623571024&p=1\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\"\nstyle=\"position:absolute; height: 100%; width: 100%;\"></iframe>\n</div>\n</p>\n",
            "tags": [
                "琐碎"
            ]
        },
        {
            "id": "https://yunhdan.github.io/cs/Springboot%E3%80%81Mybatis/",
            "url": "https://yunhdan.github.io/cs/Springboot%E3%80%81Mybatis/",
            "title": "Springboot、Mybatis",
            "date_published": "2025-06-19T05:57:13.000Z",
            "content_html": "<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h1 id=\"mybatis是怎么防止sql注入的\"><a class=\"anchor\" href=\"#mybatis是怎么防止sql注入的\">#</a> Mybatis 是怎么防止 SQL 注入的？</h1>\n<ul>\n<li>这篇文章介绍了什么是 SQL 注入，以及 Mybatis 是怎么防止注入的，算是填补一下当时做项目不求甚解带来的漏洞：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzQ4MTIzNC9hcnRpY2xlL2RldGFpbHMvMTIwOTUwMDAz\">https://blog.csdn.net/weixin_43481234/article/details/120950003</span></li>\n</ul>\n",
            "tags": [
                "计算机"
            ]
        },
        {
            "id": "https://yunhdan.github.io/cs/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/",
            "url": "https://yunhdan.github.io/cs/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/",
            "title": "边缘计算",
            "date_published": "2025-06-18T13:10:03.000Z",
            "content_html": "<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h1 id=\"tflite-gpu-delegate\"><a class=\"anchor\" href=\"#tflite-gpu-delegate\">#</a> TFlite GPU Delegate</h1>\n<ul>\n<li>这篇算是比较通俗易懂地简要概览一下 TFlite GPU Delegate 的文章了：<span class=\"exturl\" data-url=\"aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8xNDM1NjEwNDA=\">https://zhuanlan.zhihu.com/p/143561040</span></li>\n</ul>\n",
            "tags": [
                "计算机"
            ]
        },
        {
            "id": "https://yunhdan.github.io/cs/Conda%E3%80%81Pip%E3%80%81Cuda/",
            "url": "https://yunhdan.github.io/cs/Conda%E3%80%81Pip%E3%80%81Cuda/",
            "title": "Conda、Pip、Cuda",
            "date_published": "2025-06-17T16:00:00.000Z",
            "content_html": "<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h1 id=\"安装mamba_ssm和causal_conv1d\"><a class=\"anchor\" href=\"#安装mamba_ssm和causal_conv1d\">#</a> 安装 mamba_ssm 和 causal_conv1d</h1>\n<ul>\n<li>Windows 居然也支持 mamba_ssim 和 causal_conv1d 了：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ1MTAwMjAwL2FydGljbGUvZGV0YWlscy8xMzk3NTQyMzE=\">https://blog.csdn.net/qq_45100200/article/details/139754231</span></li>\n</ul>\n<h1 id=\"cuda系列\"><a class=\"anchor\" href=\"#cuda系列\">#</a> cuda 系列</h1>\n<ul>\n<li>简要介绍 cudatoolkit 和 CUDA Toolkit 的区别：<span class=\"exturl\" data-url=\"aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8yNzQzNDA5MTEzNQ==\">https://zhuanlan.zhihu.com/p/27434091135</span></li>\n<li>进一步详解 CUDA 和 cudatoolkit，拓展至 cudnn 和 nvcc：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDk0MDU4L2FydGljbGUvZGV0YWlscy8xMTYyMDczMzM=\">https://blog.csdn.net/qq_41094058/article/details/116207333</span></li>\n</ul>\n",
            "tags": [
                "计算机"
            ]
        },
        {
            "id": "https://yunhdan.github.io/ai/%E7%8E%B0%E4%BB%A3%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/",
            "url": "https://yunhdan.github.io/ai/%E7%8E%B0%E4%BB%A3%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/",
            "title": "现代深度学习",
            "date_published": "2025-06-17T07:25:31.000Z",
            "content_html": "<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h1 id=\"知识蒸馏方法\"><a class=\"anchor\" href=\"#知识蒸馏方法\">#</a> 知识蒸馏方法</h1>\n<ul>\n<li>一文了解知识蒸馏，通俗易懂：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzY5NDA5Ni9hcnRpY2xlL2RldGFpbHMvMTI3NTA1OTQ2\">https://blog.csdn.net/weixin_43694096/article/details/127505946</span></li>\n</ul>\n<h1 id=\"重参数化技术\"><a class=\"anchor\" href=\"#重参数化技术\">#</a> 重参数化技术</h1>\n<ul>\n<li>重参数化的 2 篇根基论文 <code>RepVGG</code>  和 <code>RepMLP</code> ：<span class=\"exturl\" data-url=\"aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8zNzA0Mzg5OTk=\">https://zhuanlan.zhihu.com/p/370438999</span></li>\n</ul>\n<h1 id=\"ntire图像恢复赛事\"><a class=\"anchor\" href=\"#ntire图像恢复赛事\">#</a>  <code>NTIRE</code>  图像恢复赛事</h1>\n<ul>\n<li>23 年超分辨率赛道技术报告解读，文末有赛事原报告：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzgwMDU3Ny9hcnRpY2xlL2RldGFpbHMvMTMxNjY4Nzgy\">https://blog.csdn.net/weixin_43800577/article/details/131668782</span></li>\n</ul>\n<h1 id=\"显存占用\"><a class=\"anchor\" href=\"#显存占用\">#</a> 显存占用</h1>\n<ul>\n<li>分析显存占用的内在机理：<span class=\"exturl\" data-url=\"aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC82NDE4OTQwMTQ=\">https://zhuanlan.zhihu.com/p/641894014</span></li>\n</ul>\n",
            "tags": [
                "人工智能"
            ]
        },
        {
            "id": "https://yunhdan.github.io/cs/Pytorch/",
            "url": "https://yunhdan.github.io/cs/Pytorch/",
            "title": "Pytorch",
            "date_published": "2025-06-13T05:14:06.000Z",
            "content_html": "<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h1 id=\"pytorch算子\"><a class=\"anchor\" href=\"#pytorch算子\">#</a> Pytorch 算子</h1>\n<ul>\n<li>简要介绍算子：<span class=\"exturl\" data-url=\"aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC81NzQxNjY5MjA=\">https://zhuanlan.zhihu.com/p/574166920</span></li>\n</ul>\n<h1 id=\"矩阵乘法-矩阵点乘\"><a class=\"anchor\" href=\"#矩阵乘法-矩阵点乘\">#</a> 矩阵乘法、矩阵点乘</h1>\n<ul>\n<li>这篇分别为矩阵乘法和矩阵点乘介绍了两种广义算子：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwNzI4NjY3L2FydGljbGUvZGV0YWlscy8xMzQwMTM4OTk=\">https://blog.csdn.net/qq_40728667/article/details/134013899</span></li>\n</ul>\n<h1 id=\"关于num_worker\"><a class=\"anchor\" href=\"#关于num_worker\">#</a> 关于 num_worker</h1>\n<ul>\n<li>简明扼要地介绍 pytorch 中 dataloader 的 num_worker：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI4MDU3Mzc5L2FydGljbGUvZGV0YWlscy8xMTU0MjcwNTI=\">https://blog.csdn.net/qq_28057379/article/details/115427052</span></li>\n<li>一些关于 num_worker 的独特思考：<span class=\"exturl\" data-url=\"aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC82NDUzNjQwODk=\">https://zhuanlan.zhihu.com/p/645364089</span></li>\n</ul>\n",
            "tags": [
                "计算机"
            ]
        },
        {
            "id": "https://yunhdan.github.io/cs/Flask/",
            "url": "https://yunhdan.github.io/cs/Flask/",
            "title": "Flask",
            "date_published": "2025-06-13T05:10:41.000Z",
            "content_html": "<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h1 id=\"安装\"><a class=\"anchor\" href=\"#安装\">#</a> 安装</h1>\n<figure class=\"highlight bash\"><figcaption data-lang=\"bash\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>pip <span class=\"token function\">install</span> Flask</pre></td></tr></table></figure><h1 id=\"overview-铺垫资料\"><a class=\"anchor\" href=\"#overview-铺垫资料\">#</a> Overview、铺垫资料</h1>\n<p>参考资料：</p>\n<ul>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cucnVub29iLmNvbS9mbGFzay9mbGFzay10dXRvcmlhbC5odG1s\">Flask 教程 | 菜鸟教程</span></li>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cucnVub29iLmNvbS9mbGFzay9mbGFzay1iYXNpYy1jb25jZXB0Lmh0bWw=\">Flask 基本概念 | 菜鸟教程</span></li>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cucnVub29iLmNvbS9mbGFzay9mbGFzay1sYXlvdXQuaHRtbA==\">Flask 项目结构 | 菜鸟教程</span></li>\n</ul>\n<p>这部分主要是了解 Flask 的背景。同时了解路由、视图函数、请求对象和响应对象、模板文件、应用工厂、蓝图、配置对象、静态文件是什么。了解 Flask 的项目结构如何。</p>\n",
            "tags": [
                "计算机"
            ]
        },
        {
            "id": "https://yunhdan.github.io/cs/Vue/",
            "url": "https://yunhdan.github.io/cs/Vue/",
            "title": "Vue",
            "date_published": "2025-06-13T05:07:55.000Z",
            "content_html": "<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h1 id=\"循环语句\"><a class=\"anchor\" href=\"#循环语句\">#</a> 循环语句</h1>\n<p>参考资料：</p>\n<ul>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1hTTF9IUi9hcnRpY2xlL2RldGFpbHMvMTI3MzEyNjMyP29wc19yZXF1ZXN0X21pc2M9JTdCJTIycmVxdWVzdCU1RmlkJTIyJTNBJTIyY2JhNzE1ZjNiNzcwOTk1MDVlNzRhOGNlMmUzYzE3MmMlMjIlMkMlMjJzY20lMjIlM0ElMjIyMDE0MDcxMy4xMzAxMDIzMzQuLiUyMiU3RCZhbXA7cmVxdWVzdF9pZD1jYmE3MTVmM2I3NzA5OTUwNWU3NGE4Y2UyZTNjMTcyYyZhbXA7Yml6X2lkPTAmYW1wO3V0bV9tZWRpdW09ZGlzdHJpYnV0ZS5wY19zZWFyY2hfcmVzdWx0Lm5vbmUtdGFzay1ibG9nLTJ+YWxsfnRvcF9wb3NpdGl2ZX5kZWZhdWx0LTEtMTI3MzEyNjMyLW51bGwtbnVsbC4xNDIlNUV2MTAyJTVFcGNfc2VhcmNoX3Jlc3VsdF9iYXNlMyZhbXA7dXRtX3Rlcm09di1mb3ImYW1wO3NwbT0xMDE4LjIyMjYuMzAwMS40MTg3\">vue3【列表渲染】v-for 详细介绍（vue 中的 “循环”）_vue3 v-for-CSDN 博客</span></li>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cucnVub29iLmNvbS92dWUzL3Z1ZTMtdi1mb3IuaHRtbA==\">Vue3 循环语句 | 菜鸟教程</span></li>\n</ul>\n<p>主要围绕如何使用 v-for 遍历数组、对象。v-for 的几种基本用法要掌握。</p>\n<h1 id=\"安装vue项目\"><a class=\"anchor\" href=\"#安装vue项目\">#</a> 安装 Vue 项目</h1>\n<p>参考资料：</p>\n<ul>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cucnVub29iLmNvbS92dWUzL3Z1ZTMtaW5zdGFsbC5odG1s\">Vue3 安装 | 菜鸟教程</span></li>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cucnVub29iLmNvbS92dWUzL3Z1ZTMtY3JlYXRlLXByb2plY3QuaHRtbA==\">Vue3 创建项目 | 菜鸟教程</span></li>\n</ul>\n<h1 id=\"vue项目结构说明\"><a class=\"anchor\" href=\"#vue项目结构说明\">#</a> Vue 项目结构说明</h1>\n<p>参考资料：</p>\n<ul>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cucnVub29iLmNvbS92dWUzL3Z1ZTMtcHJvamVjdC1pbnRyby5odG1s\">Vue3 项目说明 | 菜鸟教程</span></li>\n</ul>\n",
            "tags": [
                "计算机"
            ]
        },
        {
            "id": "https://yunhdan.github.io/research/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%AE%BA%E6%96%87%E7%B2%BE%E7%82%BC/",
            "url": "https://yunhdan.github.io/research/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%AE%BA%E6%96%87%E7%B2%BE%E7%82%BC/",
            "title": "多模态论文精炼",
            "date_published": "2025-06-04T03:04:06.000Z",
            "content_html": "<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h1 id=\"clip工作\"><a class=\"anchor\" href=\"#clip工作\">#</a>  <code>CLIP</code>  工作</h1>\n<h2 id=\"2021-learning-transferable-visual-models-from-natural-language-supervisionclip\"><a class=\"anchor\" href=\"#2021-learning-transferable-visual-models-from-natural-language-supervisionclip\">#</a>  <code>2021 Learning Transferable Visual Models From Natural Language Supervision(CLIP)</code></h2>\n<p><img data-src=\"../../assets/clip.png\" alt=\"image\" /></p>\n<blockquote>\n<p>学习自朱毅老师的 <code>CLIP</code>  逐段精读。</p>\n</blockquote>\n<p><strong>贡献</strong>：</p>\n<ul>\n<li>（1）通过文本和图像的对比学习，模型学习到文本 - 图像对的匹配关系。</li>\n<li>（2）能够实现通过给定一张图像，在多个文本标签中选择出与图像最相关的文本。也可以实现给定一个文本，选择最符合相关的图像。</li>\n</ul>\n<p><strong>创新</strong>：</p>\n<ul>\n<li>实现文本与图像的多模态学习。</li>\n<li>实现实现无标签限制的图像分类，也可以实现无图像限制的文本 - 图像配对。前者可以用于图像中的物体识别，后者可以用于文本检索图像。</li>\n</ul>\n<p><strong> <code>CLIP</code>  对比学习训练代码</strong>：</p>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token comment\"># image_encoder - ResNet 或者 Vision Transformer</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token comment\"># text_encoder - CBOW 或者 Text Transformer</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token comment\"># I [n, h, w, c] - 图像形状</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre><span class=\"token comment\"># T [n, l] - 文本形状，l 是序列长度</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre><span class=\"token comment\"># W_i [d_i, d_e] - 图像的线性投射矩阵</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre><span class=\"token comment\"># W_t [d_t, d_e] - 文本的线性投射矩阵</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre><span class=\"token comment\"># t - learned temperature parameter</span></pre></td></tr><tr><td data-num=\"8\"></td><td><pre></pre></td></tr><tr><td data-num=\"9\"></td><td><pre><span class=\"token comment\"># 分别提取图像特征和文本特征</span></pre></td></tr><tr><td data-num=\"10\"></td><td><pre>I_f <span class=\"token operator\">=</span> image_encoder<span class=\"token punctuation\">(</span>I<span class=\"token punctuation\">)</span> <span class=\"token comment\">#[n, d_i]</span></pre></td></tr><tr><td data-num=\"11\"></td><td><pre>T_f <span class=\"token operator\">=</span> text_encoder<span class=\"token punctuation\">(</span>T<span class=\"token punctuation\">)</span> <span class=\"token comment\">#[n, d_t]</span></pre></td></tr><tr><td data-num=\"12\"></td><td><pre></pre></td></tr><tr><td data-num=\"13\"></td><td><pre><span class=\"token comment\"># 对两个特征进行线性投射，得到相同维度的特征，并进行 l2 归一化</span></pre></td></tr><tr><td data-num=\"14\"></td><td><pre>I_e <span class=\"token operator\">=</span> l2_normalize<span class=\"token punctuation\">(</span>np<span class=\"token punctuation\">.</span>dot<span class=\"token punctuation\">(</span>I_f<span class=\"token punctuation\">,</span> W_i<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> axis<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"15\"></td><td><pre>T_e <span class=\"token operator\">=</span> l2_normalize<span class=\"token punctuation\">(</span>np<span class=\"token punctuation\">.</span>dot<span class=\"token punctuation\">(</span>T_f<span class=\"token punctuation\">,</span> W_t<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> axis<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"16\"></td><td><pre></pre></td></tr><tr><td data-num=\"17\"></td><td><pre><span class=\"token comment\"># 计算缩放的余弦相似度：[n, n]</span></pre></td></tr><tr><td data-num=\"18\"></td><td><pre>logits <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>dot<span class=\"token punctuation\">(</span>I_e<span class=\"token punctuation\">,</span> T_e<span class=\"token punctuation\">.</span>T<span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> np<span class=\"token punctuation\">.</span>exp<span class=\"token punctuation\">(</span>t<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"19\"></td><td><pre></pre></td></tr><tr><td data-num=\"20\"></td><td><pre><span class=\"token comment\"># 对称的对比学习损失：等价于 N 个类别的 cross_entropy_loss</span></pre></td></tr><tr><td data-num=\"21\"></td><td><pre>labels <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>arange<span class=\"token punctuation\">(</span>n<span class=\"token punctuation\">)</span> <span class=\"token comment\"># 对角线元素的 labels</span></pre></td></tr><tr><td data-num=\"22\"></td><td><pre>loss_i <span class=\"token operator\">=</span> cross_entropy_loss<span class=\"token punctuation\">(</span>logits<span class=\"token punctuation\">,</span> labels<span class=\"token punctuation\">,</span> axis<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"23\"></td><td><pre>loss_t <span class=\"token operator\">=</span> cross_entropy_loss<span class=\"token punctuation\">(</span>logits<span class=\"token punctuation\">,</span> labels<span class=\"token punctuation\">,</span> axis<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"24\"></td><td><pre>loss <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>loss_i <span class=\"token operator\">+</span> loss_t<span class=\"token punctuation\">)</span><span class=\"token operator\">/</span><span class=\"token number\">2</span></pre></td></tr></table></figure><h1 id=\"多模态在分割的应用\"><a class=\"anchor\" href=\"#多模态在分割的应用\">#</a> 多模态在分割的应用</h1>\n<h2 id=\"2022-iclr-language-driven-semantic-segmentationlseg\"><a class=\"anchor\" href=\"#2022-iclr-language-driven-semantic-segmentationlseg\">#</a>  <code>2022 ICLR Language-driven semantic segmentation(LSeg)</code></h2>\n<p><img data-src=\"../../assets/lseg1.jpg\" alt=\"image\" /></p>\n<blockquote>\n<p>学习自朱毅老师的逐段精读。</p>\n</blockquote>\n<p><strong>贡献</strong>：</p>\n<ul>\n<li>将 <code>CLIP</code>  的原始文本编码器作为需分割的物体标签的文本编码器，以充分提取文本特征。</li>\n<li>将文本特征与图像特征通过矩阵相乘融合得到多模态特征，上采样后与 <code>Ground-truth</code>  在像素级使用 <code>cross entropy loss</code>  进行训练。</li>\n<li>测试时可以实现，根据需要分割的对象的文本标签，分割特定图像的内容。</li>\n</ul>\n<p><strong>创新</strong>：</p>\n<ul>\n<li>一篇把 <code>CLIP</code>  模型运用到分割任务且有效果的工作。</li>\n<li>采用监督学习的方式训练，而不是对比学习去训练，也是为了更好地与特定分割任务适应。</li>\n</ul>\n<p><strong>不足</strong>：</p>\n<ul>\n<li>依然是有监督学习，目标函数不是对比学习的目标函数。</li>\n<li>文本特征只是用于融合多模态特征，并没有提供监督信号。</li>\n<li>依然依赖于手工标注 <code>segmentation mask</code> 。</li>\n</ul>\n<p>（可以做识别物体位置的实践）</p>\n<h2 id=\"2022-cvpr-groupvitsemantic-segmentation-emerges-from-text-supervisiongroupvit\"><a class=\"anchor\" href=\"#2022-cvpr-groupvitsemantic-segmentation-emerges-from-text-supervisiongroupvit\">#</a>  <code>2022 CVPR GroupViT:Semantic Segmentation Emerges from Text Supervision(GroupViT)</code></h2>\n<p><img data-src=\"../../assets/groupvit1.jpg\" alt=\"image\" /></p>\n<blockquote>\n<p>学习自朱毅老师的逐段精读。</p>\n<p>这篇是分割采用无监督学习的思路。主要使用的是分割中的 <code>Grouping</code>  思想。展开来讲， <code>Grouping</code>  将图像分割做为一种聚类任务，首先在图像确定聚类中心点，然后在模型训练的过程中，不断学习聚类中心周围像素点与聚类中心的相互关系，将与聚类中心相关的像素点并入该聚类中心的 <code>Group</code>  中。</p>\n</blockquote>\n<p><strong>贡献</strong>：</p>\n<ul>\n<li>使用了文本作为监督信号训练分割任务，不再依赖人工标注的图像 <code>Ground-Truth</code> 。</li>\n<li>使用 <code>Vision Transformer</code>  作为图像编码器。在每个 <code>Transformer</code>  层的输入 <code>tokens</code>  中加入若干个 <code>group tokens</code> ，这些 <code>group tokens</code>  实际上就是预先设想的聚类中心数，也就是猜测的图像有哪些物体类别。经过多个 <code>Transformer Layer</code> ， <code>Image tokens</code>  和这几个 <code>group tokens</code>  之间的关系被自注意力不断建模与学习。与特定聚类中心接近的 <code>image tokens</code> ，其特征也越接近该 <code>group token</code>  的特征。</li>\n<li>多个 <code>Transformer</code>  层后跟一个 <code>Grouping Block</code>  层。 <code>Grouping Block</code>  的本质是一个交叉注意力机制，将 <code>Image tokens</code>  并入所属的 <code>Group tokens</code> 。每个 <code>Grouping Block</code>  都将总 <code>tokens</code>  数降低，因此也减小了计算成本。</li>\n<li>使用对比学习的方式进行训练，带监督信号的文本被编码后的特征与最后的图像 <code>tokens</code>  特征两者交叉熵损失。</li>\n</ul>\n<p><strong>创新</strong>：</p>\n<ul>\n<li>首先使用了文本标注分割的 <code>Ground-Truth</code> ，不再依赖繁琐的手工标注。</li>\n</ul>\n<p><strong>不足</strong>：</p>\n<ul>\n<li>只能分割特定数量的类别，无法分割任意数量的物体。测试时，必须指定分割物体的数目，最后得到模型输出的 <code>tokens</code>  与文本标签进行余弦相似度的计算，确定分割物体的文本标签。</li>\n<li>训练中没有侧重语义信息，仅训练出了较好的分割能力。</li>\n</ul>\n<p><img data-src=\"../../assets/groupvit2.jpg\" alt=\"image\" /></p>\n<p>（测试时，模型输出了两个 <code>token</code> ，我们指定分割物体的文本标签有 <code>table、dog...potted plant</code> ，于是可以使用余弦相似度计算得到一个相似度矩阵。对每行取最大的值，对应的文本标签即为该 <code>token</code>  的类别）</p>\n<h1 id=\"多模态在检测的应用\"><a class=\"anchor\" href=\"#多模态在检测的应用\">#</a> 多模态在检测的应用</h1>\n<h2 id=\"2022-cvpr-grounded-language-image-pre-trainingglip\"><a class=\"anchor\" href=\"#2022-cvpr-grounded-language-image-pre-trainingglip\">#</a>  <code>2022 CVPR Grounded Language-Image Pre-training(Glip)</code></h2>\n<p><img data-src=\"../../assets/glip1.jpg\" alt=\"image\" /></p>\n<blockquote>\n<p>学习自朱毅老师的逐段精读。</p>\n<p>与常规目标检测任务相关的一个任务是 <code>Vision Grounding</code> 。具体是根据提供的文本，在图片中找到文本中出现的物体的位置。</p>\n</blockquote>\n<p><strong>贡献</strong>：</p>\n<ul>\n<li>参考 <code>CLIP</code>  范式，将图像的 <code>Bounding box</code>  的 <code>region</code>  输入图像编码器，将提供的文本输入文本编码器，最后得到每个 <code>Bounding box</code>  与单词的相似度矩阵。在相似度矩阵上与 <code>Ground-Truth</code>  的相似度矩阵求定位损失 <code>Localization Loss</code>  和分类损失 <code>Alignment Loss</code>  即可完成训练。</li>\n<li>为了更加充分地学习 <code>Bounding box</code>  和文本的 <code>Joint Feature</code> ，也就是多模态特征。在最后的特征相似度计算前，使用交叉注意力对图像特征和文本特征进行多层交互学习，即 <code>Deep Fusion</code> 。</li>\n</ul>\n<p><strong>创新</strong>：</p>\n<ul>\n<li>使用 <code>Deep Fusion</code>  技术以辅助学习多模态特征。</li>\n<li>将 <code>Gounding</code>  任务与目标检测任务很好地结合，并借鉴 <code>CLIP</code>  的思想做大规模数据的预训练，成功取得了很好的 <code>Zero-shot</code>  效果。</li>\n</ul>\n",
            "tags": [
                "学术"
            ]
        }
    ]
}