{
    "version": "https://jsonfeed.org/version/1",
    "title": "枯萎的花将在另一彼岸悄然绽放",
    "subtitle": null,
    "icon": "https://yunhdan.github.io/images/favicon.ico",
    "description": "计算机视觉 & 图像恢复",
    "home_page_url": "https://yunhdan.github.io",
    "items": [
        {
            "id": "https://yunhdan.github.io/book/%E4%B9%A0%E6%80%9D%E6%83%B3%E7%90%86%E8%AE%BA/",
            "url": "https://yunhdan.github.io/book/%E4%B9%A0%E6%80%9D%E6%83%B3%E7%90%86%E8%AE%BA/",
            "title": "新思想理论",
            "date_published": "2025-06-23T03:30:47.000Z",
            "content_html": "<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h1 id=\"导论\"><a class=\"anchor\" href=\"#导论\">#</a> 导论</h1>\n<h2 id=\"新思想是两个结合的重大成果\"><a class=\"anchor\" href=\"#新思想是两个结合的重大成果\">#</a> 新思想是 “两个结合” 的重大成果</h2>\n<ul>\n<li>新思想是马克思主义基本原理同中国具体实际相结合、同中国优秀传统文化相结合的重大成果。</li>\n<li>“两个结合” 我们党在探索中国特色社会主义道路得出的规律性认识，是对坚持和发展马克思主义做出的重大理论贡献，是我们取得成功的重大法宝。</li>\n</ul>\n<h2 id=\"新思想是完整的科学体系\"><a class=\"anchor\" href=\"#新思想是完整的科学体系\">#</a> 新思想是完整的科学体系</h2>\n<ul>\n<li>新思想回答了新时代坚持和发展什么样的中国特色社会主义、怎样坚持和发展中国特色社会主义、建设什么样的社会主义现代化强国、怎样建设社会主义现代化强国、建设什么样的长期执政的马克思主义政党、怎样建设长期执政的马克思主义政党等重大时代课题。</li>\n<li>十个明确概括了新思想的主要内容。</li>\n</ul>\n<details class=\"info\"><summary>展开看内容</summary><div>\n<ol>\n<li>明确中国特色社会主义的最本质特征是中国共产党领导。</li>\n<li>明确总任务是实现社会主义现代化和中华民族伟大复兴。</li>\n<li>明确新时代下我国社会的主要矛盾是人民日益增长的美好生活需要和不平衡不充分的发展之间的矛盾。</li>\n<li>明确中国特色社会主义事业总体布局是经济建设、政治建设、文化建设、社会建设、生态文明建设五位一体。</li>\n<li>明确全面深化改革的总目标是完善和发展中国特色社会主义制度、推进国家治理体系和治理能力的现代化。</li>\n<li>明确全面推进依法治国的总目标是建设中国特色社会主义</li>\n</ol>\n</div></details>\n",
            "tags": [
                "读书"
            ]
        },
        {
            "id": "https://yunhdan.github.io/cs/Springboot%E3%80%81Mybatis/",
            "url": "https://yunhdan.github.io/cs/Springboot%E3%80%81Mybatis/",
            "title": "Springboot、Mybatis",
            "date_published": "2025-06-19T05:57:13.000Z",
            "content_html": "<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h1 id=\"Mybatis是怎么防止SQL注入的？\"><a href=\"#Mybatis是怎么防止SQL注入的？\" class=\"headerlink\" title=\"Mybatis是怎么防止SQL注入的？\"></a>Mybatis是怎么防止SQL注入的？</h1><ul>\n<li>这篇文章介绍了什么是SQL注入，以及Mybatis是怎么防止注入的，算是填补一下当时做项目不求甚解带来的漏洞：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzQ4MTIzNC9hcnRpY2xlL2RldGFpbHMvMTIwOTUwMDAz\">https://blog.csdn.net/weixin_43481234/article/details/120950003</span></li>\n</ul>\n",
            "tags": [
                "计算机"
            ]
        },
        {
            "id": "https://yunhdan.github.io/cs/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/",
            "url": "https://yunhdan.github.io/cs/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/",
            "title": "边缘计算",
            "date_published": "2025-06-18T13:10:03.000Z",
            "content_html": "<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h1 id=\"TFlite-GPU-Delegate\"><a href=\"#TFlite-GPU-Delegate\" class=\"headerlink\" title=\"TFlite GPU Delegate\"></a>TFlite GPU Delegate</h1><ul>\n<li>这篇算是比较通俗易懂地简要概览一下TFlite GPU Delegate的文章了：<span class=\"exturl\" data-url=\"aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8xNDM1NjEwNDA=\">https://zhuanlan.zhihu.com/p/143561040</span></li>\n</ul>\n",
            "tags": [
                "计算机"
            ]
        },
        {
            "id": "https://yunhdan.github.io/cs/Conda%E3%80%81Pip%E3%80%81Cuda/",
            "url": "https://yunhdan.github.io/cs/Conda%E3%80%81Pip%E3%80%81Cuda/",
            "title": "Conda、Pip、Cuda",
            "date_published": "2025-06-17T16:00:00.000Z",
            "content_html": "<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h1 id=\"安装mamba-ssm和causal-conv1d\"><a href=\"#安装mamba-ssm和causal-conv1d\" class=\"headerlink\" title=\"安装mamba_ssm和causal_conv1d\"></a>安装mamba_ssm和causal_conv1d</h1><ul>\n<li>Windows居然也支持mamba_ssim和causal_conv1d了：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ1MTAwMjAwL2FydGljbGUvZGV0YWlscy8xMzk3NTQyMzE=\">https://blog.csdn.net/qq_45100200/article/details/139754231</span></li>\n</ul>\n<h1 id=\"cuda系列\"><a href=\"#cuda系列\" class=\"headerlink\" title=\"cuda系列\"></a>cuda系列</h1><ul>\n<li>简要介绍cudatoolkit和CUDA Toolkit的区别：<span class=\"exturl\" data-url=\"aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8yNzQzNDA5MTEzNQ==\">https://zhuanlan.zhihu.com/p/27434091135</span></li>\n<li>进一步详解CUDA和cudatoolkit，拓展至cudnn和nvcc：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDk0MDU4L2FydGljbGUvZGV0YWlscy8xMTYyMDczMzM=\">https://blog.csdn.net/qq_41094058/article/details/116207333</span></li>\n</ul>\n",
            "tags": [
                "计算机"
            ]
        },
        {
            "id": "https://yunhdan.github.io/ai/%E7%8E%B0%E4%BB%A3%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/",
            "url": "https://yunhdan.github.io/ai/%E7%8E%B0%E4%BB%A3%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/",
            "title": "现代深度学习",
            "date_published": "2025-06-17T07:25:31.000Z",
            "content_html": "<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h1 id=\"知识蒸馏方法\"><a href=\"#知识蒸馏方法\" class=\"headerlink\" title=\"知识蒸馏方法\"></a>知识蒸馏方法</h1><ul>\n<li>一文了解知识蒸馏，通俗易懂：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzY5NDA5Ni9hcnRpY2xlL2RldGFpbHMvMTI3NTA1OTQ2\">https://blog.csdn.net/weixin_43694096/article/details/127505946</span></li>\n</ul>\n<h1 id=\"重参数化技术\"><a href=\"#重参数化技术\" class=\"headerlink\" title=\"重参数化技术\"></a>重参数化技术</h1><ul>\n<li>重参数化的2篇根基论文<code>RepVGG</code>和<code>RepMLP</code>：<span class=\"exturl\" data-url=\"aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8zNzA0Mzg5OTk=\">https://zhuanlan.zhihu.com/p/370438999</span></li>\n</ul>\n<h1 id=\"NTIRE图像恢复赛事\"><a href=\"#NTIRE图像恢复赛事\" class=\"headerlink\" title=\"NTIRE图像恢复赛事\"></a><code>NTIRE</code>图像恢复赛事</h1><ul>\n<li>23年超分辨率赛道技术报告解读，文末有赛事原报告：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzgwMDU3Ny9hcnRpY2xlL2RldGFpbHMvMTMxNjY4Nzgy\">https://blog.csdn.net/weixin_43800577/article/details/131668782</span></li>\n</ul>\n<h1 id=\"显存占用\"><a href=\"#显存占用\" class=\"headerlink\" title=\"显存占用\"></a>显存占用</h1><ul>\n<li>分析显存占用的内在机理：<span class=\"exturl\" data-url=\"aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC82NDE4OTQwMTQ=\">https://zhuanlan.zhihu.com/p/641894014</span></li>\n</ul>\n",
            "tags": [
                "人工智能"
            ]
        },
        {
            "id": "https://yunhdan.github.io/cs/Pytorch/",
            "url": "https://yunhdan.github.io/cs/Pytorch/",
            "title": "Pytorch",
            "date_published": "2025-06-13T05:14:06.000Z",
            "content_html": "<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h1 id=\"Pytorch算子\"><a href=\"#Pytorch算子\" class=\"headerlink\" title=\"Pytorch算子\"></a>Pytorch算子</h1><ul>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC81NzQxNjY5MjA=\">https://zhuanlan.zhihu.com/p/574166920</span></li>\n</ul>\n<h1 id=\"矩阵乘法、矩阵点乘\"><a href=\"#矩阵乘法、矩阵点乘\" class=\"headerlink\" title=\"矩阵乘法、矩阵点乘\"></a>矩阵乘法、矩阵点乘</h1><ul>\n<li>这篇分别为矩阵乘法和矩阵点乘介绍了两种广义算子：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwNzI4NjY3L2FydGljbGUvZGV0YWlscy8xMzQwMTM4OTk=\">https://blog.csdn.net/qq_40728667/article/details/134013899</span></li>\n</ul>\n",
            "tags": [
                "计算机"
            ]
        },
        {
            "id": "https://yunhdan.github.io/cs/Flask/",
            "url": "https://yunhdan.github.io/cs/Flask/",
            "title": "Flask",
            "date_published": "2025-06-13T05:10:41.000Z",
            "content_html": "<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h1 id=\"安装\"><a href=\"#安装\" class=\"headerlink\" title=\"安装\"></a>安装</h1><pre><code class=\"lang-bash\">pip install Flask\n</code></pre>\n<h1 id=\"Overview、铺垫资料\"><a href=\"#Overview、铺垫资料\" class=\"headerlink\" title=\"Overview、铺垫资料\"></a>Overview、铺垫资料</h1><p>参考资料：</p>\n<ul>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cucnVub29iLmNvbS9mbGFzay9mbGFzay10dXRvcmlhbC5odG1s\">Flask 教程 | 菜鸟教程</span></li>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cucnVub29iLmNvbS9mbGFzay9mbGFzay1iYXNpYy1jb25jZXB0Lmh0bWw=\">Flask 基本概念 | 菜鸟教程</span></li>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cucnVub29iLmNvbS9mbGFzay9mbGFzay1sYXlvdXQuaHRtbA==\">Flask 项目结构 | 菜鸟教程</span></li>\n</ul>\n<p>这部分主要是了解Flask的背景。同时了解路由、视图函数、请求对象和响应对象、模板文件、应用工厂、蓝图、配置对象、静态文件是什么。了解Flask的项目结构如何。</p>\n",
            "tags": [
                "计算机"
            ]
        },
        {
            "id": "https://yunhdan.github.io/cs/Vue/",
            "url": "https://yunhdan.github.io/cs/Vue/",
            "title": "Vue",
            "date_published": "2025-06-13T05:07:55.000Z",
            "content_html": "<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h1 id=\"循环语句\"><a href=\"#循环语句\" class=\"headerlink\" title=\"循环语句\"></a>循环语句</h1><p>参考资料：</p>\n<ul>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1hTTF9IUi9hcnRpY2xlL2RldGFpbHMvMTI3MzEyNjMyP29wc19yZXF1ZXN0X21pc2M9JTdCJTIycmVxdWVzdCU1RmlkJTIyJTNBJTIyY2JhNzE1ZjNiNzcwOTk1MDVlNzRhOGNlMmUzYzE3MmMlMjIlMkMlMjJzY20lMjIlM0ElMjIyMDE0MDcxMy4xMzAxMDIzMzQuLiUyMiU3RCZhbXA7cmVxdWVzdF9pZD1jYmE3MTVmM2I3NzA5OTUwNWU3NGE4Y2UyZTNjMTcyYyZhbXA7Yml6X2lkPTAmYW1wO3V0bV9tZWRpdW09ZGlzdHJpYnV0ZS5wY19zZWFyY2hfcmVzdWx0Lm5vbmUtdGFzay1ibG9nLTJ+YWxsfnRvcF9wb3NpdGl2ZX5kZWZhdWx0LTEtMTI3MzEyNjMyLW51bGwtbnVsbC4xNDJedjEwMl5wY19zZWFyY2hfcmVzdWx0X2Jhc2UzJmFtcDt1dG1fdGVybT12LWZvciZhbXA7c3BtPTEwMTguMjIyNi4zMDAxLjQxODc=\">vue3【列表渲染】v-for 详细介绍（vue中的“循环”）_vue3 v-for-CSDN博客</span></li>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cucnVub29iLmNvbS92dWUzL3Z1ZTMtdi1mb3IuaHRtbA==\">Vue3 循环语句 | 菜鸟教程</span></li>\n</ul>\n<p>主要围绕如何使用v-for遍历数组、对象。v-for的几种基本用法要掌握。</p>\n<h1 id=\"安装Vue项目\"><a href=\"#安装Vue项目\" class=\"headerlink\" title=\"安装Vue项目\"></a>安装Vue项目</h1><p>参考资料：</p>\n<ul>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cucnVub29iLmNvbS92dWUzL3Z1ZTMtaW5zdGFsbC5odG1s\">Vue3 安装 | 菜鸟教程</span></li>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cucnVub29iLmNvbS92dWUzL3Z1ZTMtY3JlYXRlLXByb2plY3QuaHRtbA==\">Vue3 创建项目 | 菜鸟教程</span></li>\n</ul>\n<h1 id=\"Vue项目结构说明\"><a href=\"#Vue项目结构说明\" class=\"headerlink\" title=\"Vue项目结构说明\"></a>Vue项目结构说明</h1><p>参考资料：</p>\n<ul>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cucnVub29iLmNvbS92dWUzL3Z1ZTMtcHJvamVjdC1pbnRyby5odG1s\">Vue3 项目说明 | 菜鸟教程</span></li>\n</ul>\n",
            "tags": [
                "计算机"
            ]
        },
        {
            "id": "https://yunhdan.github.io/research/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%AE%BA%E6%96%87%E7%B2%BE%E7%82%BC/",
            "url": "https://yunhdan.github.io/research/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%AE%BA%E6%96%87%E7%B2%BE%E7%82%BC/",
            "title": "多模态论文精炼",
            "date_published": "2025-06-04T03:04:06.000Z",
            "content_html": "<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h1 id=\"CLIP工作\"><a href=\"#CLIP工作\" class=\"headerlink\" title=\"CLIP工作\"></a><code>CLIP</code>工作</h1><h2 id=\"2021-Learning-Transferable-Visual-Models-From-Natural-Language-Supervision-CLIP\"><a href=\"#2021-Learning-Transferable-Visual-Models-From-Natural-Language-Supervision-CLIP\" class=\"headerlink\" title=\"2021 Learning Transferable Visual Models From Natural Language Supervision(CLIP)\"></a><code>2021 Learning Transferable Visual Models From Natural Language Supervision(CLIP)</code></h2><p><img data-src=\"../../assets/clip.png\" alt=\"image\"></p>\n<blockquote>\n<p>学习自朱毅老师的<code>CLIP</code>逐段精读。</p>\n</blockquote>\n<p><strong>贡献</strong>：</p>\n<ul>\n<li>（1）通过文本和图像的对比学习，模型学习到文本-图像对的匹配关系。</li>\n<li>（2）能够实现通过给定一张图像，在多个文本标签中选择出与图像最相关的文本。也可以实现给定一个文本，选择最符合相关的图像。</li>\n</ul>\n<p><strong>创新</strong>：</p>\n<ul>\n<li>实现文本与图像的多模态学习。</li>\n<li>实现实现无标签限制的图像分类，也可以实现无图像限制的文本-图像配对。前者可以用于图像中的物体识别，后者可以用于文本检索图像。</li>\n</ul>\n<p><strong><code>CLIP</code>对比学习训练代码</strong>：</p>\n<pre><code class=\"lang-python\"># image_encoder - ResNet 或者 Vision Transformer\n# text_encoder - CBOW 或者 Text Transformer\n# I[n, h, w, c] - 图像形状\n# T[n, l] - 文本形状，l是序列长度\n# W_i[d_i, d_e] - 图像的线性投射矩阵\n# W_t[d_t, d_e] - 文本的线性投射矩阵\n# t - learned temperature parameter\n\n# 分别提取图像特征和文本特征\nI_f = image_encoder(I) #[n, d_i]\nT_f = text_encoder(T) #[n, d_t]\n\n# 对两个特征进行线性投射，得到相同维度的特征，并进行l2归一化\nI_e = l2_normalize(np.dot(I_f, W_i), axis=1)\nT_e = l2_normalize(np.dot(T_f, W_t), axis=1)\n\n# 计算缩放的余弦相似度：[n, n]\nlogits = np.dot(I_e, T_e.T) * np.exp(t)\n\n# 对称的对比学习损失：等价于N个类别的cross_entropy_loss\nlabels = np.arange(n) # 对角线元素的labels\nloss_i = cross_entropy_loss(logits, labels, axis=0)\nloss_t = cross_entropy_loss(logits, labels, axis=1)\nloss = (loss_i + loss_t)/2\n</code></pre>\n<h1 id=\"多模态在分割的应用\"><a href=\"#多模态在分割的应用\" class=\"headerlink\" title=\"多模态在分割的应用\"></a>多模态在分割的应用</h1><h2 id=\"2022-ICLR-Language-driven-semantic-segmentation-LSeg\"><a href=\"#2022-ICLR-Language-driven-semantic-segmentation-LSeg\" class=\"headerlink\" title=\"2022 ICLR Language-driven semantic segmentation(LSeg)\"></a><code>2022 ICLR Language-driven semantic segmentation(LSeg)</code></h2><p><img data-src=\"../../assets/lseg1.jpg\" alt=\"image\"></p>\n<blockquote>\n<p>学习自朱毅老师的逐段精读。</p>\n</blockquote>\n<p><strong>贡献</strong>：</p>\n<ul>\n<li>将<code>CLIP</code>的原始文本编码器作为需分割的物体标签的文本编码器，以充分提取文本特征。</li>\n<li>将文本特征与图像特征通过矩阵相乘融合得到多模态特征，上采样后与<code>Ground-truth</code>在像素级使用<code>cross entropy loss</code>进行训练。</li>\n<li>测试时可以实现，根据需要分割的对象的文本标签，分割特定图像的内容。</li>\n</ul>\n<p><strong>创新</strong>：</p>\n<ul>\n<li>一篇把<code>CLIP</code>模型运用到分割任务且有效果的工作。</li>\n<li>采用监督学习的方式训练，而不是对比学习去训练，也是为了更好地与特定分割任务适应。</li>\n</ul>\n<p><strong>不足</strong>：</p>\n<ul>\n<li>依然是有监督学习，目标函数不是对比学习的目标函数。</li>\n<li>文本特征只是用于融合多模态特征，并没有提供监督信号。</li>\n<li>依然依赖于手工标注<code>segmentation mask</code>。</li>\n</ul>\n<p>（可以做识别物体位置的实践）</p>\n<h2 id=\"2022-CVPR-GroupViT-Semantic-Segmentation-Emerges-from-Text-Supervision-GroupViT\"><a href=\"#2022-CVPR-GroupViT-Semantic-Segmentation-Emerges-from-Text-Supervision-GroupViT\" class=\"headerlink\" title=\"2022 CVPR GroupViT:Semantic Segmentation Emerges from Text Supervision(GroupViT)\"></a><code>2022 CVPR GroupViT:Semantic Segmentation Emerges from Text Supervision(GroupViT)</code></h2><p><img data-src=\"../../assets/groupvit1.jpg\" alt=\"image\"></p>\n<blockquote>\n<p>学习自朱毅老师的逐段精读。</p>\n<p>这篇是分割采用无监督学习的思路。主要使用的是分割中的<code>Grouping</code>思想。展开来讲，<code>Grouping</code>将图像分割做为一种聚类任务，首先在图像确定聚类中心点，然后在模型训练的过程中，不断学习聚类中心周围像素点与聚类中心的相互关系，将与聚类中心相关的像素点并入该聚类中心的<code>Group</code>中。</p>\n</blockquote>\n<p><strong>贡献</strong>：</p>\n<ul>\n<li>使用了文本作为监督信号训练分割任务，不再依赖人工标注的图像<code>Ground-Truth</code>。</li>\n<li>使用<code>Vision Transformer</code>作为图像编码器。在每个<code>Transformer</code>层的输入<code>tokens</code>中加入若干个<code>group tokens</code>，这些<code>group tokens</code>实际上就是预先设想的聚类中心数，也就是猜测的图像有哪些物体类别。经过多个<code>Transformer Layer</code>，<code>Image tokens</code>和这几个<code>group tokens</code>之间的关系被自注意力不断建模与学习。与特定聚类中心接近的<code>image tokens</code>，其特征也越接近该<code>group token</code>的特征。</li>\n<li>多个<code>Transformer</code>层后跟一个<code>Grouping Block</code>层。<code>Grouping Block</code>的本质是一个交叉注意力机制，将<code>Image tokens</code>并入所属的<code>Group tokens</code>。每个<code>Grouping Block</code>都将总<code>tokens</code>数降低，因此也减小了计算成本。</li>\n<li>使用对比学习的方式进行训练，带监督信号的文本被编码后的特征与最后的图像<code>tokens</code>特征两者交叉熵损失。</li>\n</ul>\n<p><strong>创新</strong>：</p>\n<ul>\n<li>首先使用了文本标注分割的<code>Ground-Truth</code>，不再依赖繁琐的手工标注。</li>\n</ul>\n<p><strong>不足</strong>：</p>\n<ul>\n<li>只能分割特定数量的类别，无法分割任意数量的物体。测试时，必须指定分割物体的数目，最后得到模型输出的<code>tokens</code>与文本标签进行余弦相似度的计算，确定分割物体的文本标签。</li>\n<li>训练中没有侧重语义信息，仅训练出了较好的分割能力。</li>\n</ul>\n<p><img data-src=\"../../assets/groupvit2.jpg\" alt=\"image\"></p>\n<p>（测试时，模型输出了两个<code>token</code>，我们指定分割物体的文本标签有<code>table、dog...potted plant</code>，于是可以使用余弦相似度计算得到一个相似度矩阵。对每行取最大的值，对应的文本标签即为该<code>token</code>的类别）</p>\n<h1 id=\"多模态在检测的应用\"><a href=\"#多模态在检测的应用\" class=\"headerlink\" title=\"多模态在检测的应用\"></a>多模态在检测的应用</h1><h2 id=\"2022-CVPR-Grounded-Language-Image-Pre-training-Glip\"><a href=\"#2022-CVPR-Grounded-Language-Image-Pre-training-Glip\" class=\"headerlink\" title=\"2022 CVPR Grounded Language-Image Pre-training(Glip)\"></a><code>2022 CVPR Grounded Language-Image Pre-training(Glip)</code></h2><p><img data-src=\"../../assets/glip1.jpg\" alt=\"image\"></p>\n<blockquote>\n<p>学习自朱毅老师的逐段精读。</p>\n<p>与常规目标检测任务相关的一个任务是<code>Vision Grounding</code>。具体是根据提供的文本，在图片中找到文本中出现的物体的位置。</p>\n</blockquote>\n<p><strong>贡献</strong>：</p>\n<ul>\n<li>参考<code>CLIP</code>范式，将图像的<code>Bounding box</code>的<code>region</code>输入图像编码器，将提供的文本输入文本编码器，最后得到每个<code>Bounding box</code>与单词的相似度矩阵。在相似度矩阵上与<code>Ground-Truth</code>的相似度矩阵求定位损失<code>Localization Loss</code>和分类损失<code>Alignment Loss</code>即可完成训练。</li>\n<li>为了更加充分地学习<code>Bounding box</code>和文本的<code>Joint Feature</code>，也就是多模态特征。在最后的特征相似度计算前，使用交叉注意力对图像特征和文本特征进行多层交互学习，即<code>Deep Fusion</code>。</li>\n</ul>\n<p><strong>创新</strong>：</p>\n<ul>\n<li>使用<code>Deep Fusion</code>技术以辅助学习多模态特征。</li>\n<li>将<code>Gounding</code>任务与目标检测任务很好地结合，并借鉴<code>CLIP</code>的思想做大规模数据的预训练，成功取得了很好的<code>Zero-shot</code>效果。</li>\n</ul>\n",
            "tags": [
                "学术"
            ]
        },
        {
            "id": "https://yunhdan.github.io/baoyan/Low-level-Vision-Group/",
            "url": "https://yunhdan.github.io/baoyan/Low-level-Vision-Group/",
            "title": "Low-level-Vision-Group",
            "date_published": "2025-06-03T03:08:17.000Z",
            "content_html": "<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p>:::info </p>\n<p>做底层视觉的团队，供保研用。</p>\n<p>:::</p>\n<p>+++danger 注意</p>\n<p>请做好心理准备，这些导师都很强，申请做他们的学生，在面试考核中一定是会被拷打的。对方的学术水平，能力水平本身就在你之上，你的任何漏洞、问题、毛病都会被看得一清二楚。所以想要做到让对方完全满意是不太可能的，要做好这个心理建设。</p>\n<p>+++</p>\n<h1 id=\"中山大学-网络空间安全学院\"><a href=\"#中山大学-网络空间安全学院\" class=\"headerlink\" title=\"中山大学-网络空间安全学院\"></a>中山大学-网络空间安全学院</h1><ul>\n<li>任文琦：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9zaXRlcy5nb29nbGUuY29tL3ZpZXcvd2VucWlyZW4vaG9tZXBhZ2U=\">https://sites.google.com/view/wenqiren/homepage</span></li>\n</ul>\n<p>这个老师特别强，我发过email2次都是已阅没回，没看上我。</p>\n<h1 id=\"厦门大学-信息学院\"><a href=\"#厦门大学-信息学院\" class=\"headerlink\" title=\"厦门大学-信息学院\"></a>厦门大学-信息学院</h1><ul>\n<li>丁兴号：<span class=\"exturl\" data-url=\"aHR0cHM6Ly94bXUtc21hcnRkc3AuZ2l0aHViLmlvLw==\">https://xmu-smartdsp.github.io/</span></li>\n</ul>\n<p>没有联系过，没有实验室主页，情况未知。</p>\n<h1 id=\"南开大学-密码与网络空间安全学院\"><a href=\"#南开大学-密码与网络空间安全学院\" class=\"headerlink\" title=\"南开大学-密码与网络空间安全学院\"></a>南开大学-密码与网络空间安全学院</h1><ul>\n<li>程明明实验室李重仪，郭春乐：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9tbWNoZW5nLm5ldC9yZWNydWl0L2NvbW1lbnQtcGFnZS0yNS8=\">https://mmcheng.net/recruit/comment-page-25/</span></li>\n</ul>\n<p>底层视觉顶级组，考核贼强，六级要480+。考核差不多相当于用c++复现一篇传统论文，感觉非常困难故未考虑。</p>\n<h1 id=\"东南大学-计算机科学与工程学院\"><a href=\"#东南大学-计算机科学与工程学院\" class=\"headerlink\" title=\"东南大学-计算机科学与工程学院\"></a>东南大学-计算机科学与工程学院</h1><ul>\n<li>薛晖：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9wYWxtLnNldS5lZHUuY24vaHh1ZS8=\">https://palm.seu.edu.cn/hxue/</span></li>\n</ul>\n<p>这个是有名的palm实验室，不放实习，不建议去。</p>\n<h1 id=\"中山大学-计算机学院\"><a href=\"#中山大学-计算机学院\" class=\"headerlink\" title=\"中山大学-计算机学院\"></a>中山大学-计算机学院</h1><ul>\n<li>李冠彬：<span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cuc3lzdS1oY3AubmV0L2ZhY3VsdHkvbGlndWFuYmluLmh0bWw=\">https://www.sysu-hcp.net/faculty/liguanbin.html</span></li>\n<li>张冬雨：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9jc2Uuc3lzdS5lZHUuY24vdGVhY2hlci9aaGFuZ0Rvbmd5dQ==\">https://cse.sysu.edu.cn/teacher/ZhangDongyu</span></li>\n</ul>\n<p>联系过李老师2次，但是都没有看上我，没回信。</p>\n<h1 id=\"南京理工大学-计算机科学与工程学院\"><a href=\"#南京理工大学-计算机科学与工程学院\" class=\"headerlink\" title=\"南京理工大学-计算机科学与工程学院\"></a>南京理工大学-计算机科学与工程学院</h1><ul>\n<li>潘金山：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9qc3Bhbi5naXRodWIuaW8v\">https://jspan.github.io/</span></li>\n</ul>\n<p>这个老师做low level特别强。</p>\n<h1 id=\"北京大学-信息工程学院（深圳）\"><a href=\"#北京大学-信息工程学院（深圳）\" class=\"headerlink\" title=\"北京大学-信息工程学院（深圳）\"></a>北京大学-信息工程学院（深圳）</h1><ul>\n<li>张健：<span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cuZWNlLnBrdS5lZHUuY24vaW5mby8xMDQ2LzI1MDYuaHRt\">https://www.ece.pku.edu.cn/info/1046/2506.htm</span></li>\n</ul>\n<p>老师很强，听说对学生也很温和友好。我联系过2次，均未搭理，没回信。看了一下他实验室的学生的来向，基本全是9和2，双非和四非根本不可能。</p>\n<h1 id=\"哈尔滨工业大学-计算机学院\"><a href=\"#哈尔滨工业大学-计算机学院\" class=\"headerlink\" title=\"哈尔滨工业大学-计算机学院\"></a>哈尔滨工业大学-计算机学院</h1><ul>\n<li>左旺孟：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9ob21lcGFnZS5oaXQuZWR1LmNuL3dhbmdtZW5nenVv\">https://homepage.hit.edu.cn/wangmengzuo</span></li>\n</ul>\n<p>这个老师懂得都懂，low level泰斗。听说对学生也相当好，人品也不错。但因为在本部太远，未考虑。</p>\n<h1 id=\"南京大学-智能科学与技术学院（苏州）\"><a href=\"#南京大学-智能科学与技术学院（苏州）\" class=\"headerlink\" title=\"南京大学-智能科学与技术学院（苏州）\"></a>南京大学-智能科学与技术学院（苏州）</h1><ul>\n<li>张凯：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9jc3puLmdpdGh1Yi5pby8=\">https://cszn.github.io/</span></li>\n<li>邰颖：<span class=\"exturl\" data-url=\"aHR0cHM6Ly90eXNoaXdvLmdpdGh1Yi5pby9pbmRleC5odG1s\">https://tyshiwo.github.io/index.html</span></li>\n</ul>\n<p>联系过张凯老师，张凯老师人很好。第一次信没有回复，第二次在follow信发过去后很快就回信了，这个是第一个回信的老师，表示欢迎我来。虽然有点类似官回，但给我很大的鼓励。</p>\n<h1 id=\"北京师范大学-人工智能学院\"><a href=\"#北京师范大学-人工智能学院\" class=\"headerlink\" title=\"北京师范大学-人工智能学院\"></a>北京师范大学-人工智能学院</h1><ul>\n<li>黄华：<span class=\"exturl\" data-url=\"aHR0cHM6Ly92bWNsLmJudS5lZHUuY24vZ3JvdXAvdGVhY2hlci9kY2RhZWE3OWI1ZTU0Yjc1YjUzMjc5NTEwOWE4NWEzNC5odG0=\">https://vmcl.bnu.edu.cn/group/teacher/dcdaea79b5e54b75b532795109a85a34.htm</span></li>\n</ul>\n<p>有点偏low level中的底层，与相机有关，感觉干不来。</p>\n<h1 id=\"南京大学-计算机学院\"><a href=\"#南京大学-计算机学院\" class=\"headerlink\" title=\"南京大学-计算机学院\"></a>南京大学-计算机学院</h1><ul>\n<li>路通：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9jcy5uanUuZWR1LmNuL2x1dG9uZy9pbmRleC5odG0=\">https://cs.nju.edu.cn/lutong/index.htm</span></li>\n</ul>\n<p>联系过但没有回复。</p>\n<h1 id=\"大连理工大学-国际信息科学与工程学院\"><a href=\"#大连理工大学-国际信息科学与工程学院\" class=\"headerlink\" title=\"大连理工大学-国际信息科学与工程学院\"></a>大连理工大学-国际信息科学与工程学院</h1><ul>\n<li>刘日升：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9yc2xpdS50ZWNoLw==\">https://rsliu.tech/</span></li>\n</ul>\n<p>这个老师同时也做机器人，很强。因为位置偏远，没有考虑。</p>\n<h1 id=\"四川大学-计算机学院\"><a href=\"#四川大学-计算机学院\" class=\"headerlink\" title=\"四川大学-计算机学院\"></a>四川大学-计算机学院</h1><ul>\n<li>彭玺：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9wZW5neGkubWUv\">https://pengxi.me/</span></li>\n</ul>\n<h1 id=\"电子科技大学-计算机科学与工程学院\"><a href=\"#电子科技大学-计算机科学与工程学院\" class=\"headerlink\" title=\"电子科技大学-计算机科学与工程学院\"></a>电子科技大学-计算机科学与工程学院</h1><ul>\n<li>顾舒航：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9zaHVoYW5nZ3UuZ2l0aHViLmlvLw==\">https://shuhanggu.github.io/</span></li>\n</ul>\n<h1 id=\"天津大学-智能与计算学部\"><a href=\"#天津大学-智能与计算学部\" class=\"headerlink\" title=\"天津大学-智能与计算学部\"></a>天津大学-智能与计算学部</h1><ul>\n<li>郭晓杰：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9zaXRlcy5nb29nbGUuY29tL3ZpZXcveGpndW8=\">https://sites.google.com/view/xjguo</span></li>\n</ul>\n<p>很强，LIME论文的团队。听说对学生也很好，联系过一次，没有回信，再次未被看上。</p>\n<h1 id=\"电子科技大学-信息与通信工程学院\"><a href=\"#电子科技大学-信息与通信工程学院\" class=\"headerlink\" title=\"电子科技大学-信息与通信工程学院\"></a>电子科技大学-信息与通信工程学院</h1><ul>\n<li>刘帅成：<span class=\"exturl\" data-url=\"aHR0cDovL3d3dy5saXVzaHVhaWNoZW5nLm9yZy8=\">http://www.liushuaicheng.org/</span></li>\n</ul>\n<p>很强，GLARE论文的团队，依然是联系过后未被看上，没有回信。</p>\n<h1 id=\"中国科学技术大学\"><a href=\"#中国科学技术大学\" class=\"headerlink\" title=\"中国科学技术大学\"></a>中国科学技术大学</h1><ul>\n<li>李厚强：<span class=\"exturl\" data-url=\"aHR0cDovL3N0YWZmLnVzdGMuZWR1LmNuL35saWhxL2VuLw==\">http://staff.ustc.edu.cn/~lihq/en/</span></li>\n<li>熊志伟：<span class=\"exturl\" data-url=\"aHR0cDovL3N0YWZmLnVzdGMuZWR1LmNuL356d3hpb25nLw==\">http://staff.ustc.edu.cn/~zwxiong/</span></li>\n<li>刘东：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9mYWN1bHR5LnVzdGMuZWR1LmNuL2RvbmdlbGl1Lw==\">https://faculty.ustc.edu.cn/dongeliu/</span></li>\n<li>陈志波：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9mYWN1bHR5LnVzdGMuZWR1LmNuL2NoZW56aGliby8=\">https://faculty.ustc.edu.cn/chenzhibo/</span></li>\n</ul>\n<h1 id=\"武汉大学\"><a href=\"#武汉大学\" class=\"headerlink\" title=\"武汉大学\"></a>武汉大学</h1><ul>\n<li>马佳义：<span class=\"exturl\" data-url=\"aHR0cDovL212cC53aHUuZWR1LmNuL2ppYXlpbWEv\">http://mvp.whu.edu.cn/jiayima/</span></li>\n</ul>\n<h1 id=\"西安电子科技大学\"><a href=\"#西安电子科技大学\" class=\"headerlink\" title=\"西安电子科技大学\"></a>西安电子科技大学</h1><ul>\n<li>董伟生：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9zZWUueGlkaWFuLmVkdS5jbi9mYWN1bHR5L3dzZG9uZy8=\">https://see.xidian.edu.cn/faculty/wsdong/</span></li>\n</ul>\n<p>联系过董老师2次，但都是没有打开看我的信。</p>\n<h1 id=\"西安交通大学\"><a href=\"#西安交通大学\" class=\"headerlink\" title=\"西安交通大学\"></a>西安交通大学</h1><ul>\n<li>孟德宇：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9nci54anR1LmVkdS5jbi93ZWIvZHltZW5n\">https://gr.xjtu.edu.cn/web/dymeng</span></li>\n</ul>\n<p>西交夏令营要本科学校计算机a评估。</p>\n<h1 id=\"华东师范大学\"><a href=\"#华东师范大学\" class=\"headerlink\" title=\"华东师范大学\"></a>华东师范大学</h1><ul>\n<li>谢源：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9mYWN1bHR5LmVjbnUuZWR1LmNuL19zMTYveHkyXzExMzQyL21haW4ucHNw\">https://faculty.ecnu.edu.cn/_s16/xy2_11342/main.psp</span></li>\n</ul>\n<p>风评未知，也没有团队主页，暂不考虑。</p>\n<h1 id=\"中科院深先所\"><a href=\"#中科院深先所\" class=\"headerlink\" title=\"中科院深先所\"></a>中科院深先所</h1><ul>\n<li>MMLab：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9tbWxhYi5zaWF0LmFjLmNuLw==\">https://mmlab.siat.ac.cn/</span></li>\n<li>董超：<span class=\"exturl\" data-url=\"aHR0cDovL3hwaXhlbC5ncm91cC8=\">http://xpixel.group/</span></li>\n</ul>\n<p>联系过董超老师2次，依然没有打开信看。</p>\n<h1 id=\"同济大学-计算机科学与技术学院（软件学院）\"><a href=\"#同济大学-计算机科学与技术学院（软件学院）\" class=\"headerlink\" title=\"同济大学-计算机科学与技术学院（软件学院）\"></a>同济大学-计算机科学与技术学院（软件学院）</h1><ul>\n<li>张林：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9zc2UudG9uZ2ppLmVkdS5jbi9pbmZvLzEyMTIvNTA1Mi5odG0=\">https://sse.tongji.edu.cn/info/1212/5052.htm</span></li>\n</ul>\n<p>同济大学bar过高。</p>\n<h1 id=\"北京邮电大学-计算机学院\"><a href=\"#北京邮电大学-计算机学院\" class=\"headerlink\" title=\"北京邮电大学-计算机学院\"></a>北京邮电大学-计算机学院</h1><ul>\n<li>明安龙：<span class=\"exturl\" data-url=\"aHR0cHM6Ly90ZWFjaGVyLmJ1cHQuZWR1LmNuL21hbA==\">https://teacher.bupt.edu.cn/mal</span></li>\n</ul>\n<p>联系过明老师，明老师做计算摄影、美学评估方向为主，方向有点不太match。</p>\n<h1 id=\"北京邮电大学-人智学院\"><a href=\"#北京邮电大学-人智学院\" class=\"headerlink\" title=\"北京邮电大学-人智学院\"></a>北京邮电大学-人智学院</h1><ul>\n<li>鲁鹏：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9jdi14dWViYS5jbHViL3BhZ2VzL21lbWJlcnMvcGx1Lmh0bWw=\">https://cv-xueba.club/pages/members/plu.html</span></li>\n</ul>\n",
            "tags": [
                "保研"
            ]
        },
        {
            "id": "https://yunhdan.github.io/project/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%B0%8F%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94%E6%9E%84%E5%BB%BAEnv%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%96%B9%E6%B3%95/",
            "url": "https://yunhdan.github.io/project/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%B0%8F%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94%E6%9E%84%E5%BB%BAEnv%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%96%B9%E6%B3%95/",
            "title": "强化学习小实践——构建Env的基本方法",
            "date_published": "2025-05-22T10:50:06.000Z",
            "content_html": "<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p>:::info</p>\n<p>这个小实践以一个示例说明构建Env的基本方法。同样能够涉及到Gymnasium的基本用法。</p>\n<p>:::</p>\n<h1 id=\"环境准备-rainbow\"><a href=\"#环境准备-rainbow\" class=\"headerlink\" title=\"[环境准备]{.rainbow}\"></a>[环境准备]{.rainbow}</h1><p>:::warning</p>\n<p>最好新建一个虚拟环境，在这个环境下进行环境配置。</p>\n<p>:::</p>\n<p>首先，需要在终端执行如下命令安装这个示例需要的包：</p>\n<pre><code class=\"lang-shell\">pip install copier\ncopier copy https://github.com/Farama-Foundation/gymnasium-env-template.git &quot;path/to/directory&quot;\n</code></pre>\n<p>其中，”path/to/directory”更改为你自定义的放项目的文件夹位置。执行完毕后，项目文件夹下会出现如下内容：</p>\n<pre><code class=\"lang-raw\">.\n├── gymnasium_env\n│   ├── envs\n│   │   ├── grid_world.py\n│   │   └── __init__.py\n│   ├── __init__.py\n│   └── wrappers\n│       ├── clip_reward.py\n│       ├── discrete_actions.py\n│       ├── __init__.py\n│       ├── reacher_weighted_reward.py\n│       └── relative_position.py\n├── LICENSE\n├── pyproject.toml\n└── README.md\n</code></pre>\n<p>然后确保你的电脑安装了Microsoft Visual C++ Build Tools。<br>安装方法：</p>\n<ul>\n<li>在浏览器打开<span class=\"exturl\" data-url=\"aHR0cHM6Ly92aXN1YWxzdHVkaW8ubWljcm9zb2Z0LmNvbS96aC1oYW5zL3Zpc3VhbC1jcHAtYnVpbGQtdG9vbHMv\">https://visualstudio.microsoft.com/zh-hans/visual-cpp-build-tools/</span></li>\n<li>点击“下载生成工具”，接着会下载vs_BuildTools.exe。</li>\n<li>下载完毕后执行vs_BuildTools.exe，在工作负载勾选第一个。</li>\n<li>在右侧勾选以下组件：MSVC v143 - VS 2022 C++ x64/x86 build tools、Windows 11 SDK。</li>\n<li>点击安装即可。</li>\n</ul>\n<p>最后，在终端执行：</p>\n<pre><code class=\"lang-shell\">cd &quot;path/to/directory&quot;\npip install swig\npip install &quot;gymnasium[box2d]&quot;\ncd gymnasium_env\npip install -e .\n</code></pre>\n<p>自此相关环境已经配置完毕。</p>\n<h1 id=\"创建环境实例-rainbow\"><a href=\"#创建环境实例-rainbow\" class=\"headerlink\" title=\"[创建环境实例]{.rainbow}\"></a>[创建环境实例]{.rainbow}</h1><p>在与gymnasim_env同级下，编写run.py文件：</p>\n<pre><code class=\"lang-python\">import gymnasium \nimport gymnasium_env\n\nenv = gymnasium.make(&#39;gymnasium_env/GridWorld-v0&#39;, render_mode=&#39;human&#39;)\nenv.reset()\n\nepisode_over = False\nwhile not episode_over:\n    action = env.action_space.sample()\n    observation, reward, terminated, truncated, info = env.step(action)\n\n    episode_over = terminated or truncated\n\nenv.close()\n</code></pre>\n<p>点击运行，就可以看到一个网格，Agent是蓝色的圆圈，Agent想要到达红色方块处，这就是Agent运行的环境Env。这个Agent因为是一个未训练的模型，所以并不能高效地完成这个任务，它经过了一段时间才“随机”地到达了红色方块处。本实践主要是展示如何构建一个环境实例。</p>\n<p><img data-src=\"../../assets/rl_env1.png\" alt=\"image\"></p>\n<h1 id=\"相关说明-rainbow\"><a href=\"#相关说明-rainbow\" class=\"headerlink\" title=\"[相关说明]{.rainbow}\"></a>[相关说明]{.rainbow}</h1><p>下面说明一些重要的方法以帮助进一步理解环境创建的过程。</p>\n<pre><code class=\"lang-python\">env = gymnasium.make(&#39;gymnasium_env/GridWorld-v0&#39;, render_mode=&#39;human&#39;)\n</code></pre>\n<p>这个语句，根据’gymnasium_env/GridWorld-v0’路径下自定义的环境类创建一个环境，render_mode是可视化的模式，‘human’表示用人性化的方式展现出来。</p>\n<p>很明显，你发现并没有这个路径’gymnasium_env/GridWorld-v0’，我们打开gymnasium_env文件夹下的__init__.py，可以看到如下代码：</p>\n<pre><code class=\"lang-python\">from gymnasium.envs.registration import register\n\nregister(\n    id=&quot;gymnasium_env/GridWorld-v0&quot;,\n    entry_point=&quot;gymnasium_env.envs:GridWorldEnv&quot;,\n)\n</code></pre>\n<p>因为是自定义的环境类，而非gymnasium库内置的环境类，所以通常需要用register类进行环境注册。entry_point指定了类的位置，id根据其创建一个路径。但如此做还不够，因为gymnasium不一定能够通过”gymnasium_env.envs:GridWorldEnv”找到自定义的GridWorldEnv。</p>\n<p>因为这条语句的意思是，向gymnasium_env.envs文件夹寻找GridWorldEnv这个类，但envs文件夹自己能不能知道自己有GridWorldEnv这个类？我们还要再做一步，在envs文件夹内的__init__.py文件导入GridWorldEnv，正如代码所示的那样：</p>\n<pre><code class=\"lang-python\">from gymnasium_env.envs.grid_world import GridWorldEnv\n</code></pre>\n<p>这样，”gymnasium_env.envs:GridWorldEnv”就能生效了，GridWorldEnv便可以被注册为”gymnasium_env/GridWorld-v0”这个路径。</p>\n<pre><code class=\"lang-python\">env.reset()\n</code></pre>\n<p>环境类通常有内置方法reset()，这个方法用于初始化环境。当环境类被实例化后，使用该方法生成第一个观察状态。</p>\n<pre><code class=\"lang-python\">action = env.action_space.sample()\n</code></pre>\n<p>当前实践的Agent的代码没有定义，所以暂时用环境类内置的action_space方法去生成Agent的动作。一般情况下，Agent的action是Agent观察环境后得出的，是Agent的方法。</p>\n<pre><code class=\"lang-python\">observation, reward, terminated, truncated, info = env.step(action)\n</code></pre>\n<p>环境类需要有step方法，根据Agent的action去生成激励reward，更新旧观察状态为新观察状态observation。terminated是检查是否已经结束游戏，truncated检查是否应该中途停止游戏，info是游戏有关的信息。</p>\n<pre><code class=\"lang-python\">env.close()\n</code></pre>\n<p>没什么说的，关闭环境，释放资源。</p>\n",
            "tags": [
                "项目与实践"
            ]
        },
        {
            "id": "https://yunhdan.github.io/baoyan/%E4%BF%9D%E7%A0%94%E5%8E%86%E7%A8%8B/",
            "url": "https://yunhdan.github.io/baoyan/%E4%BF%9D%E7%A0%94%E5%8E%86%E7%A8%8B/",
            "title": "保研历程",
            "date_published": "2025-05-16T13:31:49.000Z",
            "content_html": "<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h1 id=\"个人基本情况-rainbow\"><a href=\"#个人基本情况-rainbow\" class=\"headerlink\" title=\"[个人基本情况]{.rainbow}\"></a>[个人基本情况]{.rainbow}</h1><ul>\n<li>本科：江西四非，计算机科学与技术，2022级</li>\n<li>排名：前5%。</li>\n<li>竞赛：若干国奖，若干省奖。</li>\n<li>科研经历：2段，low-level vision相关，一篇Q2期刊一作，一篇竞赛产出的CVPR Workshop。</li>\n<li>英语：四级558，六级擦边428。无雅思。</li>\n</ul>\n<h1 id=\"定位与计划-rainbow\"><a href=\"#定位与计划-rainbow\" class=\"headerlink\" title=\"[定位与计划]{.rainbow}\"></a>[定位与计划]{.rainbow}</h1><ul>\n<li>院校倾向：主上2，末九。冲中9。有一定的地域考虑，深圳优先。</li>\n<li>方向选择：做AI，主要是Low Level Vision相关，可3d，可多模态。</li>\n<li>导师选择：人好肯带我最重要，个人倾向于年轻的导师，我感觉我不是那种给我资源自己找出路就能很好的人，还是希望有年轻的导师多带带我。导师人好，读研生活愉快些很重要！</li>\n<li>其他的一些考虑：title强一些更好、学硕最佳、暂时没有直博想法。</li>\n</ul>\n<h1 id=\"夏令营经历-rainbow\"><a href=\"#夏令营经历-rainbow\" class=\"headerlink\" title=\"[夏令营经历]{.rainbow}\"></a>[夏令营经历]{.rainbow}</h1><div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>院校</th>\n<th>入营情况</th>\n<th>备注</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>南大CS</td>\n<td>寄</td>\n<td>没收到邮件默认为寄</td>\n</tr>\n<tr>\n<td>中大网安</td>\n<td>寄</td>\n<td>刚报完名一两天，夏令营就取消了</td>\n</tr>\n<tr>\n<td>北大信工</td>\n<td>寄</td>\n<td>没收到邮件默认为寄</td>\n</tr>\n<tr>\n<td>上科大信院VDI</td>\n<td>未知</td>\n<td>07.07、07.14、07.17</td>\n</tr>\n<tr>\n<td>华中科大</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>川大</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>中科院深先院</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>南京航空航天</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>华东师大</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>武大</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>暨南大</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>深大</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>中科大</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>北京师大</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>人大信院</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>电子科大</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>北交</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n</div>\n<h1 id=\"预推免\"><a href=\"#预推免\" class=\"headerlink\" title=\"预推免\"></a>预推免</h1><h1 id=\"最后总结\"><a href=\"#最后总结\" class=\"headerlink\" title=\"最后总结\"></a>最后总结</h1>",
            "tags": [
                "保研"
            ]
        },
        {
            "id": "https://yunhdan.github.io/ai/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/",
            "url": "https://yunhdan.github.io/ai/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/",
            "title": "深度学习理论",
            "date_published": "2025-05-09T15:49:32.000Z",
            "content_html": "<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p>:::info</p>\n<p>复习深度学习必要的理论，可参考该复习线路。该文内容学自李沐动手学深度学习，更基础详尽的理论可以学习吴恩达深度学习。</p>\n<p>:::</p>\n<h1 id=\"线性神经网络-rainbow\"><a href=\"#线性神经网络-rainbow\" class=\"headerlink\" title=\"[线性神经网络]{.rainbow}\"></a>[线性神经网络]{.rainbow}</h1><h2 id=\"线性回归-dot\"><a href=\"#线性回归-dot\" class=\"headerlink\" title=\"++线性回归++{.dot}\"></a>++线性回归++{.dot}</h2><p>:::info Summary</p>\n<p>学习视频链接：<span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMVBYNHkxZzdLQy8/c3BtX2lkX2Zyb209MzMzLjEzODcuY29sbGVjdGlvbi52aWRlb19jYXJkLmNsaWNrJmFtcDt2ZF9zb3VyY2U9M2QxNDU2MGMyOGY5MGVmZGQxZjNlNmNhZjdiZjQyNzc=\">https://www.bilibili.com/video/BV1PX4y1g7KC/?spm_id_from=333.1387.collection.video_card.click&amp;vd_source=3d14560c28f90efdd1f3e6caf7bf4277</span></p>\n<p>:::</p>\n<ul>\n<li>线性回归是对n维输入的加权，外加偏差，通常用于预测，方程形式：<script type=\"math/tex; mode=display\">\ny = w_1x_1 + w_2x_2 + ... + b</script></li>\n<li><p>通常使用<code>MSE</code>损失去衡量预测的精确性，即预测值$\\hat{y}$和真实值y的均方差：</p>\n<script type=\"math/tex; mode=display\">\nL_{MSE} = \\frac{1}{2}||\\hat{y} - y||^2</script></li>\n<li><p>线性回归一般有显式解，显式解是损失导数为0的点。</p>\n</li>\n<li>线性回归可以看做是单层神经网络，$w_i$实际上是唯一的一层神经元的权重。</li>\n</ul>\n<p>pytorch实现线性回归很简单。线性回归可以被看成是一层神经网络，因此可以用全连接层实现：</p>\n<pre><code class=\"lang-python\">net = nn.Linear(2, 1)\n</code></pre>\n<p><code>Linear</code>第一个参数是输入数据形状的最后一个维度，比如输入数据features.shape是[4,2]，那么<code>Linear</code>第一个参数就是2。通常输入数据的最后一个维度是数据的特征，2代表输入数据有两个维度的特征，如买房数据有房价和占地面积两个维度的特征。第二个参数就是输出数据形状的最后一个维度。</p>\n<h2 id=\"基础优化算法概览-dot\"><a href=\"#基础优化算法概览-dot\" class=\"headerlink\" title=\"++基础优化算法概览++{.dot}\"></a>++基础优化算法概览++{.dot}</h2><p>:::info Summary</p>\n<p>学习视频链接：<span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMVBYNHkxZzdLQz9zcG1faWRfZnJvbT0zMzMuNzg4LnZpZGVvcG9kLmVwaXNvZGVzJmFtcDt2ZF9zb3VyY2U9M2QxNDU2MGMyOGY5MGVmZGQxZjNlNmNhZjdiZjQyNzcmYW1wO3A9Mg==\">https://www.bilibili.com/video/BV1PX4y1g7KC?spm_id_from=333.788.videopod.episodes&amp;vd_source=3d14560c28f90efdd1f3e6caf7bf4277&amp;p=2</span></p>\n<p>:::</p>\n<h3 id=\"梯度下降\"><a href=\"#梯度下降\" class=\"headerlink\" title=\"梯度下降\"></a>梯度下降</h3><p>基本思想是，对一组初始化的参数，反复迭代训练，按照下面的公式进行参数更新，使得最小化损失函数：</p>\n<script type=\"math/tex; mode=display\">\nw_t = w_{t-1} - \\alpha \\frac{\\partial Loss}{\\partial w_{t-1}}</script><p>$\\alpha$是学习率。学习率是一个很重要的超参，设置太大会导致模型无法收敛，设置太小会导致收敛过慢。</p>\n<h3 id=\"小批量梯度下降\"><a href=\"#小批量梯度下降\" class=\"headerlink\" title=\"小批量梯度下降\"></a>小批量梯度下降</h3><p>在整个训练集上进行求梯度、求导会很慢。我们可以随机采样b个样本，计算损失来近似整个训练集上的损失。这个b就是<code>batch_size</code>（批量大小），不能设置太大，太大导致内存占用过高，设置太小又无法充分发挥硬件潜力。</p>\n<h2 id=\"基本深度学习训练流程-dot\"><a href=\"#基本深度学习训练流程-dot\" class=\"headerlink\" title=\"++基本深度学习训练流程++{.dot}\"></a>++基本深度学习训练流程++{.dot}</h2><p>以线性回归为例，假设我们要建立一个这样的模型：</p>\n<script type=\"math/tex; mode=display\">\ny = 2x_1 -3.4x_2 + 4.2</script><p>事先导入需要用的包：</p>\n<pre><code class=\"lang-python\">import numpy as np\nimport torch\nfrom torch.utils import data\nfrom d2l import torch as d2l\n</code></pre>\n<p>人工生成数据：</p>\n<pre><code class=\"lang-python\">def synthetic_data(w, b, num_examples):\n    &quot;&quot;&quot;生成y=Xw+b+噪声&quot;&quot;&quot;\n    X = torch.normal(0, 1, (num_examples, len(w)))\n    y = torch.matmul(X, w) + b\n    y += torch.normal(0, 0.01, y.shape)\n    return X, y.reshape((-1, 1))\n\ntrue_w = torch.tensor([2, -3.4])\ntrue_b = 4.2\nfeatures, labels = synthetic_data(true_w, true_b, 1000)    # 生成1k个样本。\n</code></pre>\n<p>数据案例：</p>\n<pre><code class=\"lang-python\">print(&#39;features:&#39;, features[0],&#39;\\nlabel:&#39;, labels[0])\n# 返回\n# features: tensor([-0.3679, -1.8471]) \n# label: tensor([9.7361])\n</code></pre>\n<p><code>features</code>是样本的特征，本质是一个二维数组，长度为1000，而<code>label</code>是样本的预测真实值。这里每个样本有两个特征，对每个特征单独分析，都会发现其与<code>label</code>存在线性关系：</p>\n<p><img data-src=\"../../assets/d2l1.png\" alt=\"image\"></p>\n<p>读取数据集的函数，返回一个dataloader：</p>\n<pre><code class=\"lang-python\">def load_array(data_arrays, batch_size, is_train=True):\n    &quot;&quot;&quot;构造一个PyTorch数据迭代器&quot;&quot;&quot;\n    dataset = data.TensorDataset(*data_arrays)\n    return data.DataLoader(dataset, batch_size, shuffle=is_train)\n</code></pre>\n<p>数据情况示例：</p>\n<pre><code class=\"lang-python\">batch_size = 10\ndata_iter = load_array((features, labels), batch_size)\n\nnext(iter(data_iter))\n\n# 结果分别是features和labels：\n# [tensor([[ 0.1554, -0.2034],\n#          [-0.2140,  1.0352],\n#          [-0.4209,  0.0428],\n#          [ 0.1887,  0.6141],\n#          [ 0.4987, -0.2314],\n#          [ 0.0653,  1.6406],\n#          [-1.1881,  0.2900],\n#          [-0.2824,  0.5910],\n#          [ 0.9963, -0.1816],\n#          [-1.6830, -1.3963]]),\n#  tensor([[ 5.2116],\n#          [ 0.2479],\n#          [ 3.2188],\n#          [ 2.4845],\n#          [ 5.9884],\n#          [-1.2453],\n#          [ 0.8441],\n#          [ 1.6217],\n#          [ 6.8072],\n#         [ 5.5692]])]\n</code></pre>\n<p>我们的目的就是使用这1000个样本，训练出一个线性回归模型，也就是求出w和b，以最大化预测的精度，即给定一个样本特征，能够尽可能估计出其对应<code>label</code>的值。</p>\n<p>初始化线性回归模型的参数，然后在训练过程中，这些参数会被学习、调整。定义模型和损失函数，并初始化模型参数：</p>\n<pre><code class=\"lang-python\"># nn是神经网络的缩写\nfrom torch import nn\n\nnet = nn.Sequential(nn.Linear(2, 1))\nloss = nn.MSELoss()\n\nnet[0].weight.data.normal_(0, 0.01)\nnet[0].bias.data.fill_(0)\n</code></pre>\n<p>正如前面所说的，对整个数据集进行梯度求导会相当费时，所以通常采用小批量梯度下降——SGD。</p>\n<pre><code class=\"lang-python\">trainer = torch.optim.SGD(net.parameters(), lr=0.03)\n</code></pre>\n<p>这样就可以开始训练了：</p>\n<pre><code class=\"lang-python\">num_epochs = 3\nfor epoch in range(num_epochs):\n    for X, y in data_iter:\n        l = loss(net(X) ,y)\n        trainer.zero_grad()\n        l.backward()\n        trainer.step()\n    l = loss(net(features), labels)\n    print(f&#39;epoch &#123;epoch + 1&#125;, loss &#123;l:f&#125;&#39;)\n\n# 训练过程举例：\n# epoch 1, loss 0.043705\n# epoch 2, loss 0.000172\n# epoch 3, loss 0.000047\n</code></pre>\n<h2 id=\"回归、分类与独热编码-dot\"><a href=\"#回归、分类与独热编码-dot\" class=\"headerlink\" title=\"++回归、分类与独热编码++{.dot}\"></a>++回归、分类与独热编码++{.dot}</h2><p>:::info</p>\n<p>学习视频：<span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMUs2NHkxUTd3dT9zcG1faWRfZnJvbT0zMzMuNzg4LnZpZGVvcG9kLmVwaXNvZGVzJmFtcDt2ZF9zb3VyY2U9M2QxNDU2MGMyOGY5MGVmZGQxZjNlNmNhZjdiZjQyNzc=\">https://www.bilibili.com/video/BV1K64y1Q7wu?spm_id_from=333.788.videopod.episodes&amp;vd_source=3d14560c28f90efdd1f3e6caf7bf4277</span></p>\n<p>:::</p>\n<p>回归可以用于预测的问题，比如预测房屋被售出价格，或者棒球队可能获得的胜场数，又或者患者住院的天数，回归的输出是一个连续的数值。</p>\n<p><img data-src=\"../../assets/d2l2.png\" alt=\"image\"></p>\n<p>分类则更倾向于问“哪一个”。比如，某个电子邮件是否属于垃圾邮件，某张图像是驴、狗、猫还是鸡。分类问题通常是多个输出，输出i是模型预测输入为第i类的置信度。</p>\n<p><img data-src=\"../../assets/d2l3.png\" alt=\"image\"></p>\n<p>独热编码能够很好地应用到分类问题上，比如有三个类别：{狗，猫，鸡}。在计算机中，可以用(1,0,0)代表狗，用(0,1,0)代表猫，用(0,0,1)代表鸡。也就是说，用向量表示标签，分量和类别一样多，都是3。类别对应的分量设置为1，其他所有分量不是这个类别的设置为0，这就是独热编码。</p>\n<h2 id=\"Softmax运算与全连接层-dot\"><a href=\"#Softmax运算与全连接层-dot\" class=\"headerlink\" title=\"++Softmax运算与全连接层++{.dot}\"></a>++Softmax运算与全连接层++{.dot}</h2><p>:::info</p>\n<p>学习视频：<span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMUs2NHkxUTd3dT9zcG1faWRfZnJvbT0zMzMuNzg4LnZpZGVvcG9kLmVwaXNvZGVzJmFtcDt2ZF9zb3VyY2U9M2QxNDU2MGMyOGY5MGVmZGQxZjNlNmNhZjdiZjQyNzc=\">https://www.bilibili.com/video/BV1K64y1Q7wu?spm_id_from=333.788.videopod.episodes&amp;vd_source=3d14560c28f90efdd1f3e6caf7bf4277</span></p>\n<p>:::</p>\n<p>softmax运算将数据转换为[0,1]区间的数值，可以理解为一种标准化，数值可以认为是概率。比如，一个长度为3的向量经过softmax操作后，3个数值都会被转换为0到1的区间值，并且相加和为1。</p>\n<p>softmax公式如下：</p>\n<script type=\"math/tex; mode=display\">\nsoftmax(x) = \\frac{exp(x_i)}{\\sum_{k}^{len(x)}exp(x_k)}</script><p>全连接层无处不在，前面提到全连接层可以很方便地通过nn.Linear实现。但是全连接层也不是没有缺点，参数量冗余是问题。对于任何具有d个输入和q个输出的全连接层（对应在最后一个维度上，输入和输出的特征数分别为d和q），参数量开销为O(dq)。</p>\n<p>后续会提到用Dropout方法处理这个问题。</p>\n<h2 id=\"经典损失函数-dot\"><a href=\"#经典损失函数-dot\" class=\"headerlink\" title=\"++经典损失函数++{.dot}\"></a>++经典损失函数++{.dot}</h2><h3 id=\"L1损失\"><a href=\"#L1损失\" class=\"headerlink\" title=\"L1损失\"></a>L1损失</h3><h3 id=\"L2损失\"><a href=\"#L2损失\" class=\"headerlink\" title=\"L2损失\"></a>L2损失</h3><h3 id=\"Huber鲁棒损失\"><a href=\"#Huber鲁棒损失\" class=\"headerlink\" title=\"Huber鲁棒损失\"></a>Huber鲁棒损失</h3><h3 id=\"交叉熵损失\"><a href=\"#交叉熵损失\" class=\"headerlink\" title=\"交叉熵损失\"></a>交叉熵损失</h3><h2 id=\"信息论基础-dot\"><a href=\"#信息论基础-dot\" class=\"headerlink\" title=\"++信息论基础++{.dot}\"></a>++信息论基础++{.dot}</h2><h1 id=\"多层感知机-rainbow\"><a href=\"#多层感知机-rainbow\" class=\"headerlink\" title=\"[多层感知机]{.rainbow}\"></a>[多层感知机]{.rainbow}</h1><h3 id=\"多层感知机理论-dot\"><a href=\"#多层感知机理论-dot\" class=\"headerlink\" title=\"++多层感知机理论++{.dot}\"></a>++多层感知机理论++{.dot}</h3><p>前面提到的线性回归是一种线性神经网络，这样的网络存在一种假设：输入和输出是线性相关的。这种假设下，任何输入的特征增大都会导致模型的输出增大或减小。但很多时候，输入和输出并非是线性相关的。比如一张图像，增加某个位置的像素的强度值能否总是增大其分类为狗的概率？    </p>\n<p>我们可以在网络中加入一个或多个隐藏层来突破线性模型的限制，使其能处理更普遍的函数关系类型。这种架构称为多层感知机（<code>multilayer perceptron, MLP</code>），这是堆叠许多全连接层的神经网络。</p>\n<p><img data-src=\"../../assets/d2l4.png\" alt=\"image\"></p>\n<p>每两个层都是全连接的，每个输入都会影响隐藏层中的每个神经元，而隐藏层中的每个神经元又会影响输出层中的每个神经元。</p>\n<p>如果输入$X \\in \\mathbb{R}^{n \\times d}$，n是小样本数，d是输入特征。对于隐藏层有权重$W_1 \\in \\mathbb{R}^{d \\times h}$、偏置$b_1 \\in \\mathbb{R}^{1 \\times h}$，输出层也有权重$W_2 \\in \\mathbb{R}^{h \\times q}$和偏置$b_2 \\in \\mathbb{R}^{1 \\times q}$。其中，h通常是这个隐藏层的隐藏单元数，q是输出的输出特征，如果要分类为10类，q就是10。因此多层感知机（单个隐藏层）的数学表达式可以表示为，O是输出，H称为隐藏表征（<code>hidden representation</code>）：</p>\n<script type=\"math/tex; mode=display\">\nH = XW_1 + b_1    \\\\\nO = HW_2 + b_2</script><h3 id=\"激活函数-dot\"><a href=\"#激活函数-dot\" class=\"headerlink\" title=\"++激活函数++{.dot}\"></a>++激活函数++{.dot}</h3><p>没有激活函数的多层感知机相当于线性神经网络。观察上面的表达式，隐藏单元由输入的仿射变换给出，而输出也只是隐藏单元的仿射函数。仿射函数的仿射函数还是仿射函数，可以如下证明上面的多层感知机等价于单层模型：</p>\n<script type=\"math/tex; mode=display\">\nO = (XW_1 + b_1)W_2 + b_2 = XW_1W_2 + b_1W_2 + b_2 = XW + b</script><p>这是因为，之前说到的线性回归模型已经可以表示任何仿射函数。通过合并，多层感知机退化为单层的线性回归模型。为了发挥多层架构的潜力，可以在仿射变换后应用非线性的激活函数（<code>activation function</code>），即：</p>\n<script type=\"math/tex; mode=display\">\nH = \\sigma(XW_1 + b_1)    \\\\\nO = \\sigma(HW_2 + b_2)</script><p>这样，多层感知机避免了线性计算退化为单层的线性模型的风险。通过隐藏层中的神经元，多层感知机可以捕获输入之间复杂的相互作用，这些神经元依赖每个输入的值。如果给定足够的神经元和正确的权重，我们就可以对任意函数进行建模，尽管实际应用中学习该函数是很困难的。</p>\n<h4 id=\"ReLU函数\"><a href=\"#ReLU函数\" class=\"headerlink\" title=\"ReLU函数\"></a><code>ReLU</code>函数</h4><p>最受欢迎的激活函数：修正线性单元（<code>rectified linear unit, ReLU</code>）。数学表达式为：</p>\n<script type=\"math/tex; mode=display\">\nReLU(x) = max(x, 0)</script><p><code>ReLU</code>函数通过将相应的激活值设为0，仅保留正元素并丢弃所有负元素。当输入值精确为0试，<code>ReLU</code>函数不可导。<code>ReLU</code>函数求导很方便，优化表现好，并一定程度上缓解了以往神经网络的梯度消失问题。代码实现和函数曲线图如下：</p>\n<pre><code class=\"lang-python\">y = torch.relu(x)\n</code></pre>\n<p><img data-src=\"../../assets/d2l5.png\" alt=\"image\"></p>\n<h4 id=\"sigmoid函数\"><a href=\"#sigmoid函数\" class=\"headerlink\" title=\"sigmoid函数\"></a>sigmoid函数</h4><p>对于一个定义在$\\mathbb{R}$的输入，<code>sigmoid</code>激活函数将输入变换到(0,1)区间。数学表达式为：</p>\n<script type=\"math/tex; mode=display\">\nsigmoid(x) = \\frac{1}{1+exp(-x)}</script><p>这是一个平滑的、可微的阈值单元的近似函数。sigmoid常被用做输出层的激活函数，这个时候，它输出二元分类的概率，因此sigmoid可以看作是softmax的一个特例。但是，隐藏层中的激活函数还是不选择sigmoid，因为<code>ReLU</code>更合适。当输入接近0时，sigmoid函数接近线性变换。代码实现和函数曲线图如下：</p>\n<pre><code class=\"lang-python\">y = torch.sigmoid(x)\n</code></pre>\n<p><img data-src=\"../../assets/d2l6.png\" alt=\"image\"></p>\n<h4 id=\"Tanh函数\"><a href=\"#Tanh函数\" class=\"headerlink\" title=\"Tanh函数\"></a><code>Tanh</code>函数</h4><p><code>Tanh</code>（双曲正切）函数也是将输入压缩转换到区间(-1,1)上。函数公式如下：</p>\n<script type=\"math/tex; mode=display\">\ntanh(x) = \\frac{1 - exp(-2x)}{1+exp(-2x)}</script><p>输入在0附近时，它和sigmoid函数一样，接近线性变换。代码实现和函数曲线图如下：</p>\n<pre><code class=\"lang-python\">y = torch.tanh(x)\n</code></pre>\n<p><img data-src=\"../../assets/d2l7.png\" alt=\"image\"></p>\n",
            "tags": [
                "人工智能"
            ]
        },
        {
            "id": "https://yunhdan.github.io/other/%E5%8D%9A%E5%AE%A2%E6%96%87%E6%A1%A3%E7%BE%8E%E5%8C%96%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E/",
            "url": "https://yunhdan.github.io/other/%E5%8D%9A%E5%AE%A2%E6%96%87%E6%A1%A3%E7%BE%8E%E5%8C%96%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E/",
            "title": "博客文档美化使用说明",
            "date_published": "2025-03-06T13:04:00.000Z",
            "content_html": "<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p>:::info</p>\n<p>摘自<code>Hexo-Shoka</code>主题拥有者的使用说明：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9zaG9rYS5sb3N0eXUubWUvY29tcHV0ZXItc2NpZW5jZS9ub3RlL3RoZW1lLXNob2thLWRvYy9zcGVjaWFsLw==\">Step.4 主题特殊功能 - Theme Shoka Documentation - 二进制杂谈 - 计算机科学 | Yume Shoka = 有夢書架 = 吾乃天，壶中之天 (lostyu.me)</span></p>\n<p>:::</p>\n<h1 id=\"文字特效\"><a href=\"#文字特效\" class=\"headerlink\" title=\"文字特效\"></a>文字特效</h1><pre><code class=\"lang-raw\">++下划线++\n++波浪线++&#123;.wavy&#125;\n++着重点++&#123;.dot&#125;\n++紫色下划线++&#123;.primary&#125;\n++绿色波浪线++&#123;.wavy .success&#125;\n++黄色着重点++&#123;.dot .warning&#125;\n~~删除线～～\n~~红色删除线～～&#123;.danger&#125;\n==荧光高亮==\n[赤橙黄绿青蓝紫]&#123;.rainbow&#125;\n[红色]&#123;.red&#125;\n[粉色]&#123;.pink&#125;\n[橙色]&#123;.orange&#125;\n[黄色]&#123;.yellow&#125;\n[绿色]&#123;.green&#125;\n[靛青]&#123;.aqua&#125;\n[蓝色]&#123;.blue&#125;\n[紫色]&#123;.purple&#125;\n[灰色]&#123;.grey&#125;\n快捷键 [Ctrl]&#123;.kbd&#125; + [C]&#123;.kbd .red&#125;\nH~2~0\n29^th^\n</code></pre>\n<p>++下划线++<br>++波浪线++{.wavy}<br>++着重点++{.dot}<br>++紫色下划线++{.primary}<br>++绿色波浪线++{.wavy .success}<br>++黄色着重点++{.dot .warning}<br>~~删除线～～<br>~~红色删除线～～{.danger}<br>==荧光高亮==<br>[赤橙黄绿青蓝紫]{.rainbow}<br>[红色]{.red}<br>[粉色]{.pink}<br>[橙色]{.orange}<br>[黄色]{.yellow}<br>[绿色]{.green}<br>[靛青]{.aqua}<br>[蓝色]{.blue}<br>[紫色]{.purple}<br>[灰色]{.grey}<br>快捷键 [Ctrl]{.kbd} + [C]{.kbd .red}<br>H~2~0<br>29^th^</p>\n<h1 id=\"隐藏文字\"><a href=\"#隐藏文字\" class=\"headerlink\" title=\"隐藏文字\"></a>隐藏文字</h1><pre><code class=\"lang-raw\">!! 黑幕黑幕黑幕黑幕黑幕黑幕 !!： 鼠标滑过显示内容\n!! 模糊模糊模糊模糊模糊模糊 !!&#123;.bulr&#125; ： 选中文字显示内容\n</code></pre>\n<p>!! 黑幕黑幕黑幕黑幕黑幕黑幕 !!： 鼠标滑过显示内容<br>!! 模糊模糊模糊模糊模糊模糊 !!{.bulr} ： 选中文字显示内容</p>\n<h1 id=\"标签块\"><a href=\"#标签块\" class=\"headerlink\" title=\"标签块\"></a>标签块</h1><pre><code class=\"lang-raw\">[default]&#123;.label&#125;\n[primary]&#123;.label .primary&#125;\n[info]&#123;.label .info&#125;\n[:heavy_check_mark:success]&#123;.label .success&#125;\n[warning]&#123;.label .warning&#125;\n[:broken_heart:danger]&#123;.label .danger&#125;\n</code></pre>\n<p>[default]{.label}<br>[primary]{.label .primary}<br>[info]{.label .info}<br>[:heavy_check_mark:success]{.label .success}<br>[warning]{.label .warning}<br>[:broken_heart:danger]{.label .danger}</p>\n<h1 id=\"提醒块\"><a href=\"#提醒块\" class=\"headerlink\" title=\"提醒块\"></a>提醒块</h1><pre><code class=\"lang-raw\">:::default\n默认默认\n:::\n\n:::primary\n基本基本\n:::\n\n:::info\n提示提示\n:::\n\n:::success\n成功成功\n:::\n\n:::warning\n警告警告\n:::\n\n:::danger\n危险危险\n:::\n</code></pre>\n<p>:::default</p>\n<p>默认默认</p>\n<p>:::</p>\n<p>:::primary</p>\n<p>基本基本</p>\n<p>:::</p>\n<p>:::info</p>\n<p>提示提示</p>\n<p>:::</p>\n<p>:::success</p>\n<p>成功成功</p>\n<p>:::</p>\n<p>:::warning</p>\n<p>警告警告</p>\n<p>:::</p>\n<p>:::danger</p>\n<p>危险危险</p>\n<p>:::</p>\n<h1 id=\"折叠块\"><a href=\"#折叠块\" class=\"headerlink\" title=\"折叠块\"></a>折叠块</h1><pre><code class=\"lang-raw\">+++ 默认默认 这里是一段文字\n++ 下划线 ++\n+++\n\n\n+++primary 紫色\n:::info\n参考信息\n:::\n\n- 第一行\n- 第二行\n+++\n\n\n+++info  蓝色\n;;;id3 卡片 1\n这里是卡片 1 的内容\n;;;\n\n;;;id3 卡片 2\n这里是卡片 2 的内容\n;;;\n+++\n\n+++success 绿色\n&#123;% links %&#125;\n- site: 優萌初華\n  url: https://shoka.lostyu.me\n  color: \"#e9546b\"\n&#123;% endlinks %&#125;\n+++\n\n+++warning 黄色\n!! 警告警告警告警告警告！！&#123;.bulr&#125;\n[label]&#123;.label .success&#125;\n+++\n\n+++danger 红色\n[danger]&#123;.label .danger&#125;\n+++\n</code></pre>\n<p>+++ 默认默认 这里是一段文字<br>++ 下划线 ++<br>+++</p>\n<p>+++primary 紫色</p>\n<p>:::info</p>\n<p>参考信息</p>\n<p>:::</p>\n<ul>\n<li>第一行</li>\n<li>第二行<br>+++</li>\n</ul>\n<p>+++info  蓝色<br>;;;id3 卡片 1<br>这里是卡片 1 的内容<br>;;;</p>\n<p>;;;id3 卡片 2<br>这里是卡片 2 的内容<br>;;;<br>+++</p>\n<p>+++success 绿色<br><div class=\"links\"><div class=\"item\" title=\"優萌初華\" style=\"--block-color:#e9546b;\"><span class=\"exturl image\" data-url=\"aHR0cHM6Ly9zaG9rYS5sb3N0eXUubWU=\" data-background-image=\"/images/404.png\"></span>\n          <div class=\"info\">\n          <span class=\"exturl title\" data-url=\"aHR0cHM6Ly9zaG9rYS5sb3N0eXUubWU=\">優萌初華</span>\n          <p class=\"desc\">https://shoka.lostyu.me</p>\n          </div></div></div><br>+++</p>\n<p>+++warning 黄色<br>!! 警告警告警告警告警告！！{.bulr}<br>[label]{.label .success}<br>+++</p>\n<p>+++danger 红色<br>[danger]{.label .danger}<br>+++</p>\n",
            "tags": [
                "琐碎"
            ]
        },
        {
            "id": "https://yunhdan.github.io/ai/Deep-Learning-Experiment-Tricks/",
            "url": "https://yunhdan.github.io/ai/Deep-Learning-Experiment-Tricks/",
            "title": "Deep Learning Experiment Tricks",
            "date_published": "2025-03-02T15:58:57.000Z",
            "content_html": "<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p>:::info</p>\n<p>一般在一个新的<code>trick</code>和<code>experience</code>开坑时，都会先暂时粗略地搬运一些其他地方的内容，或者简略描述。偶尔精进与专门研究时，会特别地丰富和细致化该内容。</p>\n<p>:::</p>\n<h1 id=\"pytorch-mssim-ssim的使用\"><a href=\"#pytorch-mssim-ssim的使用\" class=\"headerlink\" title=\"pytorch_mssim.ssim的使用\"></a><code>pytorch_mssim.ssim</code>的使用</h1><p>以下面计算<code>ssim</code>的代码为例：</p>\n<pre><code class=\"lang-python\">ssim_value = ssim(final, gt_batch, data_range=2.0, size_average=True)\n</code></pre>\n<p><code>data_range</code>表示图像像素值的动态范围（最大值与最小值的差）。如果输入图像经过归一化处理（如 <code>transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])</code>），则像素值范围会被映射到<code>[-1, 1]</code>。此时 <code>data_range</code> 应设为 <code>2.0</code>（因为 <code>1 - (-1) = 2</code>），而不是 <code>1.0</code>。</p>\n<p><code>SSIM</code>默认假设输入范围是<code>[0, 1]</code>（当 <code>data_range=1</code>）或 <code>[0, 255]</code>（当 <code>data_range=255</code>）。</p>\n<p><code>size_average=True</code> 会将 <code>SSIM</code> 值在所有图像和通道上取平均。<code>pytorch_msssim</code>的新版本使用reduction取代这个参数，重写为：<code>reduction=&#39;mean&#39;</code>。</p>\n<p>有时候，应该确保输入给<code>ssim</code>做计算时的数据范围在<code>[-1, 1]</code>之间，不然，则要裁剪处理：</p>\n<pre><code class=\"lang-python\">final_clamped = torch.clamp(final, 0.0, 1.0)\nssim_value = ssim(final_clamped, gt_batch, data_range=1.0, size_average=True)\n</code></pre>\n<h1 id=\"损失函数下降、震荡、上升的原因\"><a href=\"#损失函数下降、震荡、上升的原因\" class=\"headerlink\" title=\"损失函数下降、震荡、上升的原因\"></a>损失函数下降、震荡、上升的原因</h1><blockquote>\n<p>在训练包含多个损失函数的模型时，各子损失函数的变化趋势能够反映模型的学习动态和优化方向。以下是不同趋势的详细分析及应对策略：</p>\n<hr>\n<h4 id=\"一、损失函数整体持续下降\"><a href=\"#一、损失函数整体持续下降\" class=\"headerlink\" title=\"一、损失函数整体持续下降\"></a><strong>一、损失函数整体持续下降</strong></h4><h4 id=\"含义解析\"><a href=\"#含义解析\" class=\"headerlink\" title=\"含义解析\"></a><strong>含义解析</strong></h4><ul>\n<li><strong>良性学习信号</strong>：模型正在有效优化该任务目标，权重分配合理，数据质量良好。</li>\n<li><strong>潜在风险</strong>：<ul>\n<li><strong>过拟合倾向</strong>：若验证集对应指标未同步下降，可能过拟合训练数据。</li>\n<li><strong>任务主导性</strong>：其他损失未充分优化，模型可能偏向该任务。</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"典型案例\"><a href=\"#典型案例\" class=\"headerlink\" title=\"典型案例\"></a><strong>典型案例</strong></h4><ul>\n<li>重建损失（如L1/L2）持续下降，但对抗损失震荡 → 模型过度拟合像素级精度，忽视生成真实性。</li>\n</ul>\n<h4 id=\"应对策略\"><a href=\"#应对策略\" class=\"headerlink\" title=\"应对策略\"></a><strong>应对策略</strong></h4><ol>\n<li><strong>验证泛化性</strong>：检查验证集对应指标是否同步改善</li>\n<li><strong>调整权重</strong>：若其他损失停滞，适当降低该损失权重（如从1.0→0.7）</li>\n<li><strong>早停机制</strong>：当验证损失不再下降时停止训练</li>\n</ol>\n<hr>\n<h4 id=\"二、损失函数震荡波动\"><a href=\"#二、损失函数震荡波动\" class=\"headerlink\" title=\"二、损失函数震荡波动\"></a><strong>二、损失函数震荡波动</strong></h4><h4 id=\"含义解析-1\"><a href=\"#含义解析-1\" class=\"headerlink\" title=\"含义解析\"></a><strong>含义解析</strong></h4><ul>\n<li><strong>优化不稳定</strong>：学习率过高、批次过小或损失间存在冲突。</li>\n<li><strong>数据问题</strong>：噪声数据或类别不均衡导致梯度方向不一致。</li>\n<li><strong>对抗性博弈</strong>：典型于GAN的判别器与生成器损失交替上升。</li>\n</ul>\n<h4 id=\"数值特征\"><a href=\"#数值特征\" class=\"headerlink\" title=\"数值特征\"></a><strong>数值特征</strong></h4><ul>\n<li><strong>高频震荡</strong>（如±5%）：常由学习率过大引起</li>\n<li><strong>低频震荡</strong>（如每5个epoch变化）：多任务目标冲突</li>\n</ul>\n<h4 id=\"典型案例-1\"><a href=\"#典型案例-1\" class=\"headerlink\" title=\"典型案例\"></a><strong>典型案例</strong></h4><ul>\n<li>分类损失下降但正则化损失震荡 → L2正则化强度过高导致参数更新不稳定</li>\n</ul>\n<h4 id=\"应对策略-1\"><a href=\"#应对策略-1\" class=\"headerlink\" title=\"应对策略\"></a><strong>应对策略</strong></h4><ol>\n<li><strong>降低学习率</strong>：将初始学习率减少3-5倍（如2e-4→5e-5）</li>\n<li><strong>增大批次大小</strong>：从32提升至128，稳定梯度估计</li>\n<li><strong>梯度裁剪</strong>：设置<code>max_grad_norm=1.0</code></li>\n<li><strong>冲突分析</strong>：计算损失梯度余弦相似度，对负相关损失解耦训练</li>\n</ol>\n<hr>\n<h4 id=\"三、损失函数持续上升\"><a href=\"#三、损失函数持续上升\" class=\"headerlink\" title=\"三、损失函数持续上升\"></a><strong>三、损失函数持续上升</strong></h4><h4 id=\"含义解析-2\"><a href=\"#含义解析-2\" class=\"headerlink\" title=\"含义解析\"></a><strong>含义解析</strong></h4><ul>\n<li><strong>严重警告信号</strong>：模型在该任务上性能退化，优化方向错误。</li>\n<li><strong>常见诱因</strong>：<ul>\n<li><strong>损失权重倒置</strong>：如误将权重设为负数</li>\n<li><strong>任务本质冲突</strong>：如超分辨率任务中，L1损失下降但感知损失上升</li>\n<li><strong>数值不稳定</strong>：梯度爆炸导致损失进入病态区域</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"典型案例-2\"><a href=\"#典型案例-2\" class=\"headerlink\" title=\"典型案例\"></a><strong>典型案例</strong></h4><ul>\n<li>对抗损失上升而重建损失下降 → 判别器过强导致生成器无法有效学习</li>\n</ul>\n<h4 id=\"应对策略-2\"><a href=\"#应对策略-2\" class=\"headerlink\" title=\"应对策略\"></a><strong>应对策略</strong></h4><ol>\n<li><strong>立即暂停训练</strong>：检查损失计算代码和权重符号</li>\n<li><strong>损失权重热力图</strong>：可视化各损失对总损失的贡献比例</li>\n<li><strong>渐进式训练</strong>：分阶段引入上升的损失项（如先用L1预训练，第50epoch加入对抗损失）</li>\n<li><strong>架构改进</strong>：对于根本性冲突，修改网络结构（如增加多尺度特征融合模块）</li>\n</ol>\n<hr>\n<h4 id=\"四、综合优化建议\"><a href=\"#四、综合优化建议\" class=\"headerlink\" title=\"四、综合优化建议\"></a><strong>四、综合优化建议</strong></h4><ol>\n<li><strong>动态权重调整</strong>：采用不确定性加权法（如《Multi-Task Learning Using Uncertainty to Weigh Losses》）<pre><code class=\"lang-python\"># 各损失自动加权示例\nlog_var = torch.nn.Parameter(torch.zeros(3)) # 3个损失\nloss = 0.5*(loss1/torch.exp(log_var[0]) + loss2/torch.exp(log_var[1]) + log_var.sum())\n</code></pre>\n</li>\n<li><strong>损失相关性监控</strong>：计算各损失间的Pearson相关系数矩阵，识别冲突组合</li>\n<li><strong>课程学习策略</strong>：早期侧重易优化损失（如L1），后期加强高阶损失（如SSIM、VGG感知损失）</li>\n<li><strong>可视化工具</strong>：使用TensorBoard的并行坐标视图对比超参数与损失关系</li>\n</ol>\n<hr>\n<h4 id=\"五、调试检查清单\"><a href=\"#五、调试检查清单\" class=\"headerlink\" title=\"五、调试检查清单\"></a><strong>五、调试检查清单</strong></h4><p>当出现异常损失趋势时，按以下顺序排查：</p>\n<ol>\n<li><strong>数值检查</strong>：<ul>\n<li>确认损失计算未出现NaN/Inf</li>\n<li>检查梯度幅值（<code>torch.nn.utils.clip_grad_norm_</code>）</li>\n</ul>\n</li>\n<li><strong>数据流验证</strong>：<pre><code class=\"lang-python\"># 数据检查代码片段\nfor batch in val_loader:\n    print(batch[&#39;image&#39;].min(), batch[&#39;image&#39;].max()) # 应为[0,1]或[-1,1]\n    visualize(batch[&#39;image&#39;][0]) # 肉眼验证图像质量\n</code></pre>\n</li>\n<li><strong>权重合理性</strong>：确保各损失量级匹配（如L1≈0.1，对抗损失≈2.0时，需调整权重平衡）</li>\n<li><strong>模型容量测试</strong>：在小数据集（如100样本）上过拟合，验证能否达到预期损失</li>\n</ol>\n<p>通过系统分析损失动态，可精准定位模型优化瓶颈，实现多目标协同优化。</p>\n</blockquote>\n<h1 id=\"transforms-Resize\"><a href=\"#transforms-Resize\" class=\"headerlink\" title=\"transforms.Resize()\"></a><code>transforms.Resize()</code></h1><p>这个与<code>RandomCrop()</code>还不太一样，<code>Resize()</code>是等比例的缩放原图。所以存在一定的信息损失，一般不使用这种操作。</p>\n<h1 id=\"不确定性损失加权法——多任务损失均衡\"><a href=\"#不确定性损失加权法——多任务损失均衡\" class=\"headerlink\" title=\"不确定性损失加权法——多任务损失均衡\"></a>不确定性损失加权法——多任务损失均衡</h1><blockquote>\n<p>以下是使用不确定性加权法改造后的损失函数实现：</p>\n<pre><code class=\"lang-python\">import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom pytorch_msssim import ssim\n\nclass UncertaintyWeightedLoss(nn.Module):\n    def __init__(self, trans, num_tasks=5):\n        &quot;&quot;&quot;\n        trans: HVI转换器实例\n        num_tasks: 需要加权的损失项数量（这里包含L1, SSIM, Res, Cons, HVI）\n        &quot;&quot;&quot;\n        super().__init__()\n        self.trans = trans\n        # 初始化可学习的不确定性参数（log方差）\n        self.log_vars = nn.Parameter(torch.zeros(num_tasks))\n        # 初始化参数（可选）\n        nn.init.uniform_(self.log_vars, -3, -1)  # 初始方差在0.05~0.37之间\n\n    def compute_losses(self, final, gt, output, S1, P1, S2, P2):\n        &quot;&quot;&quot;分解计算各个基础损失项&quot;&quot;&quot;\n        # RGB空间损失\n        l1_loss = F.l1_loss(final, gt)\n        ssim_loss = 1 - ssim(final, gt, data_range=1.0, size_average=True)\n\n        # 残差一致性损失\n        loss_res1 = F.mse_loss(S1, P1 + S2) / (S1.detach().var() + 1e-6)\n        loss_res2 = F.mse_loss(S2, P2 + S1) / (S2.detach().var() + 1e-6)\n        res_loss = loss_res1 + loss_res2\n\n        # 下采样一致性损失\n        g1, g2 = pair_downsampler(output)\n        cons_loss = F.l1_loss(S1, P1 + g1) + F.l1_loss(S2, P2 + g2)\n\n        # HVI空间损失\n        final_hvi = self.trans.HVIT(final)\n        gt_hvi = self.trans.HVIT(gt)\n        hvi_l1 = F.l1_loss(final_hvi, gt_hvi)\n        hvi_edge = edge_loss(final_hvi, gt_hvi)\n\n        return [l1_loss, ssim_loss, res_loss, cons_loss, hvi_l1, hvi_edge]\n\n    def forward(self, final, gt, output, S1, P1, S2, P2):\n        # 获取所有基础损失项\n        losses = self.compute_losses(final, gt, output, S1, P1, S2, P2)\n\n        # 应用不确定性加权\n        total_loss = 0.0\n        for i, loss in enumerate(losses):\n            precision = torch.exp(-self.log_vars[i])\n            total_loss += precision * loss + self.log_vars[i]\n\n        # 返回总损失和详细损失项（用于监控）\n        loss_details = &#123;\n            &#39;total&#39;: total_loss,\n            &#39;l1&#39;: losses[0],\n            &#39;ssim&#39;: losses[1],\n            &#39;res&#39;: losses[2],\n            &#39;cons&#39;: losses[3],\n            &#39;hvi_l1&#39;: losses[4],\n            &#39;hvi_edge&#39;: losses[5],\n            &#39;log_vars&#39;: self.log_vars\n        &#125;\n        return total_loss, loss_details\n</code></pre>\n<p>主要改进点说明：</p>\n<ol>\n<li><strong>模块化设计</strong>：<br>```python<h1 id=\"初始化方式变化\"><a href=\"#初始化方式变化\" class=\"headerlink\" title=\"初始化方式变化\"></a>初始化方式变化</h1>trans = RGB_HVI().to(device)<br>criterion = UncertaintyWeightedLoss(trans).to(device)</li>\n</ol>\n<h1 id=\"前向计算变化\"><a href=\"#前向计算变化\" class=\"headerlink\" title=\"前向计算变化\"></a>前向计算变化</h1><p>total_loss, loss_details = criterion(final, gt, output, S1, P1, S2, P2)</p>\n<pre><code>\n2. **动态权重机制**：\n- 每个损失项自动获得权重：weight = exp(-log_var)\n- 包含正则项：log_var 防止方差无限增大\n- 初始权重范围：exp(-3)=0.05 ~ exp(-1)=0.37\n\n3. **训练监控增强**：\n```python\n# 在训练循环中添加监控\nwriter.add_scalars(&#39;Loss/Train&#39;, &#123;\n    &#39;total&#39;: loss_details[&#39;total&#39;].item(),\n    &#39;l1&#39;: loss_details[&#39;l1&#39;].item(),\n    &#39;ssim&#39;: loss_details[&#39;ssim&#39;].item(),\n    &#39;res&#39;: loss_details[&#39;res&#39;].item(),\n    &#39;cons&#39;: loss_details[&#39;cons&#39;].item(),\n    &#39;hvi_l1&#39;: loss_details[&#39;hvi_l1&#39;].item(),\n    &#39;hvi_edge&#39;: loss_details[&#39;hvi_edge&#39;].item()\n&#125;, total_iter)\n\n# 监控不确定性参数\nfor i, log_var in enumerate(loss_details[&#39;log_vars&#39;]):\n    writer.add_scalar(f&#39;Uncertainty/var_&#123;i&#125;&#39;, torch.exp(log_var).item(), epoch)\n</code></pre><ol>\n<li><p><strong>优化器调整</strong>：</p>\n<pre><code class=\"lang-python\"># 需要将损失参数加入优化器\noptimizer = optim.Adam(\n list(model.parameters()) + list(criterion.parameters()),\n lr=args.lr, \n betas=(0.9, 0.999)\n)\n</code></pre>\n</li>\n<li><p><strong>梯度稳定性处理</strong>：</p>\n<pre><code class=\"lang-python\"># 在训练步骤中添加梯度裁剪\ntorch.nn.utils.clip_grad_norm_(\n list(model.parameters()) + list(criterion.parameters()),\n max_norm=1.0\n)\n</code></pre>\n</li>\n</ol>\n<p>该实现通过以下方式解决原代码问题：</p>\n<ol>\n<li><strong>自动平衡机制</strong>：</li>\n</ol>\n<ul>\n<li>难任务（高方差项）自动获得低权重</li>\n<li>简单任务（低方差项）获得高权重</li>\n<li>通过反向传播自动调整权重</li>\n</ul>\n<ol>\n<li><strong>数值稳定性改进</strong>：</li>\n</ol>\n<ul>\n<li>使用log方差代替直接学习方差，避免除零错误</li>\n<li>初始值限制在合理范围</li>\n<li>梯度裁剪防止参数爆炸</li>\n</ul>\n<ol>\n<li><strong>可解释性增强</strong>：</li>\n</ol>\n<ul>\n<li>通过TensorBoard可直接观察每个任务的：  <ul>\n<li>损失值变化曲线</li>\n<li>自适应权重变化（exp(-log_var)）</li>\n<li>方差参数演化</li>\n</ul>\n</li>\n</ul>\n<p>实际使用时需要注意：</p>\n<ol>\n<li><p><strong>参数初始化</strong>：</p>\n<pre><code class=\"lang-python\"># 如果某些任务需要强调，可以调整初始值\nnn.init.constant_(self.log_vars[4], -2.0)  # 强调HVI_L1损失\n</code></pre>\n</li>\n<li><p><strong>学习率设置</strong>：</p>\n<pre><code class=\"lang-python\"># 为损失参数设置更小的学习率\noptimizer = optim.Adam([\n    &#123;&#39;params&#39;: model.parameters(), &#39;lr&#39;: 1e-4&#125;,\n    &#123;&#39;params&#39;: criterion.parameters(), &#39;lr&#39;: 1e-5&#125;\n])\n</code></pre>\n</li>\n<li><p><strong>多阶段训练</strong>：</p>\n<pre><code class=\"lang-python\"># 初始阶段固定部分损失权重\nif epoch &lt; warmup_epochs:\n    criterion.log_vars.requires_grad_(False)\nelse:\n    criterion.log_vars.requires_grad_(True)\n</code></pre>\n</li>\n</ol>\n<p>这种实现方式比手动调整lambda参数更科学，尤其在处理类似HVI这类新增损失时，能自动平衡其与原有损失项的关系。</p>\n</blockquote>\n<h1 id=\"余弦退火学习率调度器\"><a href=\"#余弦退火学习率调度器\" class=\"headerlink\" title=\"余弦退火学习率调度器\"></a>余弦退火学习率调度器</h1><p>一代经典的学习率调度器。</p>\n<p>模板代码参考如：</p>\n<pre><code class=\"lang-python\"># ... 其他导入 ...\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\n\ndef main():\n    # 参数解析\n    parser = argparse.ArgumentParser(&quot;MAI_Denoised_Train&quot;)\n    # ... 原有参数 ...\n    parser.add_argument(&#39;--min_lr&#39;, type=float, default=1e-6, help=&#39;minimum learning rate for cosine annealing&#39;)\n    args = parser.parse_args()\n\n    # ... 模型初始化 ...\n\n    # 初始化优化器和调度器\n    optimizer = optim.Adam(model.parameters(), lr=args.lr, betas=(0.9, 0.999), weight_decay=1e-6)\n    scheduler = CosineAnnealingLR(optimizer, T_max=args.epochs, eta_min=args.min_lr)\n\n    # 训练循环\n    for epoch in range(args.epochs):\n        # 训练过程...\n\n        # 验证过程...\n\n        # 更新学习率\n        scheduler.step()\n\n        # 记录学习率\n        current_lr = scheduler.get_last_lr()[0]\n        writer.add_scalar(&#39;Learning Rate&#39;, current_lr, epoch)\n        logging.info(f&#39;Epoch [&#123;epoch+1&#125;/&#123;args.epochs&#125;] Learning Rate: &#123;current_lr:.7f&#125;&#39;)\n\n    # ... 后续代码 ...\n</code></pre>\n<h1 id=\"Optuna自动化调参\"><a href=\"#Optuna自动化调参\" class=\"headerlink\" title=\"Optuna自动化调参\"></a><code>Optuna</code>自动化调参</h1><blockquote>\n<p>为了使用 Optuna 进行超参数调优，我们需要对代码进行以下主要修改。这里展示关键修改步骤（完整代码见最后）：</p>\n<pre><code class=\"lang-python\">import optuna\nfrom optuna.trial import Trial\n\ndef objective(trial):\n    # 定义可调参数范围\n    args = argparse.Namespace(\n        batch_size=trial.suggest_categorical(&#39;batch_size&#39;, [32, 64, 128]),\n        lr=trial.suggest_float(&#39;lr&#39;, 1e-6, 1e-3, log=True),\n        lambda_hvi1=trial.suggest_float(&#39;lambda_hvi1&#39;, 0.001, 0.1),\n        lambda_hvi2=trial.suggest_float(&#39;lambda_hvi2&#39;, 0.001, 0.1),\n        lambda_rgb1=trial.suggest_float(&#39;lambda_rgb1&#39;, 0.1, 2),\n        lambda_rgb2=trial.suggest_float(&#39;lambda_rgb2&#39;, 0.1, 2),\n        # 保留其他固定参数...\n        cuda=True,\n        gpu=&#39;cuda:0&#39;,\n        epochs=100,  # 调试时可减少epoch\n        save=f&#39;./EXP/&#123;trial.number&#125;&#39;  # 为每个trial创建独立目录\n    )\n\n    # 运行训练并返回验证指标\n    final_psnr = train_model(args, trial)\n    return final_psnr\n\ndef train_model(args, trial=None):\n    # [原main()函数内容在这里重构]\n    # 在验证循环中报告中间结果\n    if trial:\n        trial.report(val_psnr, epoch)\n        if trial.should_prune():\n            raise optuna.exceptions.TrialPruned()\n\n    return best_val_psnr  # 返回需要优化的指标\n\nif __name__ == &quot;__main__&quot;:\n    # 创建Optuna study\n    study = optuna.create_study(\n        direction=&#39;maximize&#39;,\n        sampler=optuna.samplers.TPESampler(),\n        pruner=optuna.pruners.MedianPruner()\n    )\n\n    # 运行优化\n    study.optimize(objective, n_trials=50, timeout=3600*24)\n\n    # 输出结果\n    print(&quot;Best trial:&quot;)\n    trial = study.best_trial\n    print(f&quot;  Value: &#123;trial.value&#125;&quot;)\n    print(&quot;  Params: &quot;)\n    for key, value in trial.params.items():\n        print(f&quot;    &#123;key&#125;: &#123;value&#125;&quot;)\n</code></pre>\n<p>主要修改点说明：</p>\n<ol>\n<li><p><strong>参数建议</strong>：使用 trial.suggest_* 方法替代原argparse参数：</p>\n<pre><code class=\"lang-python\">trial.suggest_categorical(&#39;batch_size&#39;, [32, 64, 128])\ntrial.suggest_float(&#39;lr&#39;, 1e-6, 1e-3, log=True)\n</code></pre>\n</li>\n<li><p><strong>目标函数</strong>：将训练过程封装到目标函数中，返回验证PSNR作为优化指标</p>\n</li>\n<li><p><strong>中间报告</strong>：在训练过程中定期报告验证指标，支持提前终止：</p>\n<pre><code class=\"lang-python\">trial.report(val_psnr, epoch)\nif trial.should_prune():\n    raise optuna.exceptions.TrialPruned()\n</code></pre>\n</li>\n<li><p><strong>独立目录</strong>：为每个trial创建独立的保存目录，避免文件冲突：</p>\n<pre><code class=\"lang-python\">args.save = f&#39;./EXP/&#123;trial.number&#125;&#39;\n</code></pre>\n</li>\n<li><p><strong>Study配置</strong>：创建优化study时指定优化方向（最大化PSNR）和采样策略：</p>\n<pre><code class=\"lang-python\">study = optuna.create_study(direction=&#39;maximize&#39;)\n</code></pre>\n</li>\n</ol>\n<p>完整整合后的代码示例：</p>\n<pre><code class=\"lang-python\">import optuna\nfrom optuna.trial import Trial\nimport argparse\nimport torch\n\ndef main():\n    # 原始训练代码...\n\ndef objective(trial):\n    # 定义超参数搜索空间\n    params = &#123;\n        &#39;batch_size&#39;: trial.suggest_categorical(&#39;batch_size&#39;, [32, 64, 128]),\n        &#39;lr&#39;: trial.suggest_float(&#39;lr&#39;, 1e-6, 1e-3, log=True),\n        &#39;lambda_hvi1&#39;: trial.suggest_float(&#39;lambda_hvi1&#39;, 0.001, 0.1),\n        &#39;lambda_hvi2&#39;: trial.suggest_float(&#39;lambda_hvi2&#39;, 0.001, 0.1),\n        &#39;lambda_rgb1&#39;: trial.suggest_float(&#39;lambda_rgb1&#39;, 0.5, 2.0),\n        &#39;lambda_rgb2&#39;: trial.suggest_float(&#39;lambda_rgb2&#39;, 0.5, 2.0),\n        &#39;lambda_res&#39;: trial.suggest_float(&#39;lambda_res&#39;, 0.5, 2.0),\n        &#39;lambda_cons&#39;: trial.suggest_float(&#39;lambda_cons&#39;, 0.5, 2.0),\n    &#125;\n\n    # 固定参数\n    fixed_params = &#123;\n        &#39;cuda&#39;: True,\n        &#39;gpu&#39;: &#39;cuda:0&#39;,\n        &#39;epochs&#39;: 100,  # 调优时epoch可以适当减少\n        &#39;data_dir&#39;: &#39;/path/to/data&#39;,\n        &#39;save&#39;: f&#39;./EXP/trial_&#123;trial.number&#125;&#39;,\n    &#125;\n\n    # 合并参数\n    args = argparse.Namespace(**&#123;**params, **fixed_params&#125;)\n\n    # 运行训练\n    best_psnr = train_with_args(args, trial)\n    return best_psnr\n\ndef train_with_args(args, trial=None):\n    # [原main()函数内容重构到这里]\n    # 初始化模型、数据加载等...\n\n    best_psnr = 0\n    for epoch in range(args.epochs):\n        # 训练循环...\n\n        # 验证循环\n        with torch.no_grad():\n            val_psnr = evaluate(model, val_loader)\n\n            # 向Optuna报告中间结果\n            if trial:\n                trial.report(val_psnr, epoch)\n\n                # 提前终止\n                if trial.should_prune():\n                    raise optuna.exceptions.TrialPruned()\n\n        if val_psnr &gt; best_psnr:\n            best_psnr = val_psnr\n            torch.save(model.state_dict(), f&quot;&#123;args.save&#125;/best_model.pth&quot;)\n\n    return best_psnr\n\nif __name__ == &quot;__main__&quot;:\n    # 创建Optuna study\n    study = optuna.create_study(\n        direction=&#39;maximize&#39;,\n        sampler=optuna.samplers.TPESampler(),\n        pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=10)\n    )\n\n    # 运行优化\n    study.optimize(objective, n_trials=50, timeout=3600*24)\n\n    # 输出最佳结果\n    print(&quot;Best trial:&quot;)\n    trial = study.best_trial\n    print(f&quot;  PSNR: &#123;trial.value&#125;&quot;)\n    print(&quot;  Params: &quot;)\n    for key, value in trial.params.items():\n        print(f&quot;    &#123;key&#125;: &#123;value&#125;&quot;)\n</code></pre>\n<p>关键优化技巧：</p>\n<ol>\n<li><p><strong>参数范围设置</strong>：根据参数性质选择适当的范围：</p>\n<pre><code class=\"lang-python\">trial.suggest_float(&#39;lr&#39;, 1e-6, 1e-3, log=True)  # 对数尺度更适合学习率\ntrial.suggest_categorical(&#39;batch_size&#39;, [32, 64, 128])\n</code></pre>\n</li>\n<li><p><strong>提前终止</strong>：使用MedianPruner避免资源浪费：</p>\n<pre><code class=\"lang-python\">pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=10)\n</code></pre>\n</li>\n<li><p><strong>并行优化</strong>：通过指定n_jobs并行运行：</p>\n<pre><code class=\"lang-python\">study.optimize(objective, n_trials=100, n_jobs=4)\n</code></pre>\n</li>\n<li><p><strong>持久化存储</strong>：使用数据库保存进度：</p>\n<pre><code class=\"lang-python\">study = optuna.create_study(\n    storage=&#39;sqlite:///optuna.db&#39;,\n    study_name=&#39;denoising_study&#39;,\n    load_if_exists=True\n)\n</code></pre>\n</li>\n</ol>\n<p>注意事项：</p>\n<ol>\n<li><p><strong>资源管理</strong>：调优时适当减少epoch数量（如50-100），最终训练时再用完整epoch</p>\n</li>\n<li><p><strong>参数空间</strong>：初始搜索使用较宽范围，后期可基于初步结果缩小范围</p>\n</li>\n<li><p><strong>指标选择</strong>：建议使用验证集PSNR作为优化目标，而非训练损失</p>\n</li>\n<li><p><strong>随机种子</strong>：为保持可比性，可在每个trial中固定随机种子：</p>\n<pre><code class=\"lang-python\">torch.manual_seed(trial.suggest_int(&#39;seed&#39;, 0, 1000))\n</code></pre>\n</li>\n<li><p><strong>GPU内存</strong>：注意batch_size与GPU显存的匹配，建议在suggest_categorical中包含可行值</p>\n</li>\n</ol>\n<p>这种集成方式可以在不破坏原有训练逻辑的基础上，系统性地探索超参数空间。最终可以通过study.best_trial.params获取最佳参数组合，用于最终模型的训练。</p>\n</blockquote>\n<h1 id=\"混合精度训练\"><a href=\"#混合精度训练\" class=\"headerlink\" title=\"混合精度训练\"></a>混合精度训练</h1><h1 id=\"梯度累积\"><a href=\"#梯度累积\" class=\"headerlink\" title=\"梯度累积\"></a>梯度累积</h1><p>通过多次小批量迭代累积梯度，模拟大 <code>Batch Size</code> 的效果，模板代码参考可见下方：</p>\n<pre><code class=\"lang-python\">accumulation_steps = 4  # 累积4个batch的梯度\nfor i, (inputs, labels) in enumerate(dataloader):\n    outputs = model(inputs)\n    loss = criterion(outputs, labels)\n    loss = loss / accumulation_steps  # 损失按累积步数缩放\n    loss.backward()\n\n    if (i + 1) % accumulation_steps == 0:\n        optimizer.step()\n        optimizer.zero_grad()\n</code></pre>\n<h1 id=\"torchinfo统计模型的显存占用\"><a href=\"#torchinfo统计模型的显存占用\" class=\"headerlink\" title=\"torchinfo统计模型的显存占用\"></a><code>torchinfo</code>统计模型的显存占用</h1><p>参考如下代码：</p>\n<pre><code class=\"lang-python\">from torchinfo import summary\nmodel = MyModel().cuda()\nsummary(model, input_size=(batch_size, 3, 256, 256))\n</code></pre>\n<h1 id=\"显存分析器memory-profiler\"><a href=\"#显存分析器memory-profiler\" class=\"headerlink\" title=\"显存分析器memory_profiler\"></a>显存分析器<code>memory_profiler</code></h1><p>这个工具可以可以统计每行代码的显存变化</p>\n<pre><code class=\"lang-python\">from pytorch_memlab import LineProfiler\n\n@profile\ndef train_batch(inputs, labels):\n    outputs = model(inputs)\n    loss = criterion(outputs, labels)\n    loss.backward()\n    optimizer.step()\n\n# 运行后会打印每行代码的显存变化\ntrain_batch(inputs, labels)\n</code></pre>\n<h1 id=\"Dataloader的num-workers设置\"><a href=\"#Dataloader的num-workers设置\" class=\"headerlink\" title=\"Dataloader的num_workers设置\"></a><code>Dataloader</code>的<code>num_workers</code>设置</h1><p><code>num_workers</code>通常设置为0，<code>CPU</code>线程数的<code>75%</code>，<code>CPU</code>线程数，<code>CPU</code>线程数的两倍。</p>\n<p><code>CPU</code>的线程数计算公式为：线程数 = 逻辑核心数 = 物理核心数 * 单核线程数。</p>\n<p><code>num_workers</code>很大程度上影响<code>GPU</code>的占用率。保持长时间的高<code>GPU</code>占用率是高效率训练深度学习的基础。</p>\n<h1 id=\"损失为Nan的分析\"><a href=\"#损失为Nan的分析\" class=\"headerlink\" title=\"损失为Nan的分析\"></a>损失为<code>Nan</code>的分析</h1><blockquote>\n<p>损失值出现NaN（Not a Number）通常由数值不稳定引起，以下是可能原因及解决方案：</p>\n<h4 id=\"1-输入数据问题\"><a href=\"#1-输入数据问题\" class=\"headerlink\" title=\"1. 输入数据问题\"></a>1. <strong>输入数据问题</strong></h4><ul>\n<li><p><strong>检查数据中的NaN或异常值</strong>：确保输入数据无缺失或无效值。</p>\n<pre><code class=\"lang-python\">import numpy as np\nprint(&quot;NaN in data:&quot;, np.isnan(data).any())\nprint(&quot;数据范围:&quot;, data.min(), data.max())\n</code></pre>\n</li>\n<li><p><strong>数据标准化/归一化</strong>：过大或过小的输入值可能导致梯度爆炸。</p>\n<pre><code class=\"lang-python\">data = (data - data.mean()) / data.std()  # 标准化\n</code></pre>\n</li>\n</ul>\n<h4 id=\"2-学习率过高\"><a href=\"#2-学习率过高\" class=\"headerlink\" title=\"2. 学习率过高\"></a>2. <strong>学习率过高</strong></h4><ul>\n<li><strong>降低学习率</strong>：过大的学习率会导致参数更新不稳定。<pre><code class=\"lang-python\">optimizer = torch.optim.SGD(model.parameters(), lr=0.01)  # 初始学习率设为0.01或更小\n</code></pre>\n</li>\n</ul>\n<h4 id=\"3-损失函数实现问题\"><a href=\"#3-损失函数实现问题\" class=\"headerlink\" title=\"3. 损失函数实现问题\"></a>3. <strong>损失函数实现问题</strong></h4><ul>\n<li><p><strong>避免对零取对数</strong>：在交叉熵损失中增加极小值ε（如1e-8）。</p>\n<pre><code class=\"lang-python\">loss = -tf.reduce_sum(y_true * tf.math.log(y_pred + 1e-8))\n</code></pre>\n</li>\n<li><strong>使用框架内置函数</strong>：如TensorFlow的<code>CategoricalCrossentropy(from_logits=True)</code>，避免手动实现中的错误。</li>\n</ul>\n<h4 id=\"4-梯度爆炸（前提是你的其他代码得写对）\"><a href=\"#4-梯度爆炸（前提是你的其他代码得写对）\" class=\"headerlink\" title=\"4. 梯度爆炸（前提是你的其他代码得写对）\"></a>4. <strong>梯度爆炸（前提是你的其他代码得写对）</strong></h4><ul>\n<li><p><strong>梯度裁剪</strong>：限制梯度最大范数。</p>\n<pre><code class=\"lang-python\"># PyTorch示例\ntorch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n</code></pre>\n<pre><code class=\"lang-python\"># TensorFlow示例\ngradients = tape.gradient(loss, model.trainable_variables)\ngradients, _ = tf.clip_by_global_norm(gradients, 1.0)\noptimizer.apply_gradients(zip(gradients, model.trainable_variables))\n</code></pre>\n</li>\n</ul>\n<h4 id=\"5-模型结构问题\"><a href=\"#5-模型结构问题\" class=\"headerlink\" title=\"5. 模型结构问题\"></a>5. <strong>模型结构问题</strong></h4><ul>\n<li><strong>激活函数与输出层匹配</strong>：分类任务最后一层需用Softmax（或配合<code>from_logits=True</code>）。</li>\n<li><strong>权重初始化</strong>：使用He/Xavier初始化避免初始值过大。<pre><code class=\"lang-python\"># PyTorch示例\ntorch.nn.init.kaiming_normal_(layer.weight)\n</code></pre>\n</li>\n</ul>\n<h4 id=\"6-数值稳定性技巧\"><a href=\"#6-数值稳定性技巧\" class=\"headerlink\" title=\"6. 数值稳定性技巧\"></a>6. <strong>数值稳定性技巧</strong></h4><ul>\n<li><strong>添加Batch Normalization</strong>：稳定层间输出分布。<pre><code class=\"lang-python\">model.add(tf.keras.layers.BatchNormalization())\n</code></pre>\n</li>\n<li><strong>混合精度训练</strong>：使用FP16时，开启梯度缩放。<pre><code class=\"lang-python\">optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)\n</code></pre>\n</li>\n</ul>\n<h4 id=\"7-调试步骤\"><a href=\"#7-调试步骤\" class=\"headerlink\" title=\"7. 调试步骤\"></a>7. <strong>调试步骤</strong></h4><ul>\n<li><strong>小数据集测试</strong>：用少量样本过拟合，快速复现问题。</li>\n<li><strong>打印中间结果</strong>：检查前向传播输出和梯度。<pre><code class=\"lang-python\"># 检查输出层\nprint(&quot;模型输出:&quot;, outputs)\n# 检查梯度\nfor name, param in model.named_parameters():\n    if param.grad is not None:\n        print(f&quot;梯度 &#123;name&#125;: &#123;param.grad.norm()&#125;&quot;)\n</code></pre>\n</li>\n</ul>\n<h4 id=\"8-其他可能原因\"><a href=\"#8-其他可能原因\" class=\"headerlink\" title=\"8. 其他可能原因\"></a>8. <strong>其他可能原因</strong></h4><ul>\n<li><strong>正则化过强</strong>：降低L2正则化系数。</li>\n<li><strong>数据预处理错误</strong>：检查标准化时是否除以零（如方差为零的特征）。</li>\n</ul>\n<h4 id=\"总结流程\"><a href=\"#总结流程\" class=\"headerlink\" title=\"总结流程\"></a>总结流程</h4><ol>\n<li><strong>检查输入数据</strong>：确保无NaN且已标准化。</li>\n<li><strong>降低学习率</strong>：尝试0.001或更低。</li>\n<li><strong>验证损失函数</strong>：使用内置函数或添加ε。</li>\n<li><strong>梯度裁剪</strong>：限制梯度大小。</li>\n<li><strong>检查模型结构</strong>：激活函数、初始化、添加BatchNorm。</li>\n<li><strong>逐步调试</strong>：缩小数据范围，打印中间变量。</li>\n</ol>\n<p>通过以上步骤逐步排查，通常可以定位并解决NaN损失问题。</p>\n</blockquote>\n<h1 id=\"torchvision-utils-save-image\"><a href=\"#torchvision-utils-save-image\" class=\"headerlink\" title=\"torchvision.utils.save_image\"></a><code>torchvision.utils.save_image</code></h1><p>有个参数叫做<code>Normalize</code>，这个将数值映射到<code>[0,255]</code>的区间。相关使用说明如下：</p>\n<pre><code class=\"lang-python\">当设置normalize=True时：\n- 会自动将张量的数值范围从[min, max]线性映射到[0, 255]\n- 例如：输入张量范围是[-1, 1]，会被映射到0-255\n- 例如：输入张量范围是[0, 1]，会被映射到0-255（相当于直接乘以255）\n</code></pre>\n<p><code>torchvision.utils.save_image</code>保存图像要求图像的数值范围必须是指定范围，即在<code>[0,1]</code>或<code>[0,255]</code>。如果数据范围在其他区间，则需要保证数据范围符合<code>torchvision.utils.save_image</code>的要求，可通过设置<code>normalize</code>为<code>True</code>解决这个问题。</p>\n<h1 id=\"export-PYTHONPATH-quot-PWD-PYTHONPATH-quot\"><a href=\"#export-PYTHONPATH-quot-PWD-PYTHONPATH-quot\" class=\"headerlink\" title=\"export PYTHONPATH=&quot;$PWD:$PYTHONPATH&quot;\"></a><code>export PYTHONPATH=&quot;$PWD:$PYTHONPATH&quot;</code></h1><p>这是一个通过<code>PYTHONPATH</code>手动指定项目根目录的命令。</p>\n<p>以深度学习项目<code>Retinexformer</code>为例，这个项目文件夹内包含了训练的代码<code>train.py</code>，以及模型架构文件等。</p>\n<p>有时候直接在终端通过<code>python train.py</code>的绝对路径会报一些代码文件中的库引用错误，但是你反复检查了路径，觉得代码里导包的方式没问题。</p>\n<p>这时候你就可以先通过<code>cd</code>命令进入项目文件夹中，然后再执行这个<code>export</code>命令手动指定项目根目录：</p>\n<pre><code class=\"lang-bash\">&gt; cd Retinexformer\n&gt; export PYTHONPATH=&quot;$PWD:$PYTHONPATH&quot;\n</code></pre>\n<h1 id=\"确保Python优先加载本地项目的代码而不是Anaconda环境中的库\"><a href=\"#确保Python优先加载本地项目的代码而不是Anaconda环境中的库\" class=\"headerlink\" title=\"确保Python优先加载本地项目的代码而不是Anaconda环境中的库\"></a>确保<code>Python</code>优先加载本地项目的代码而不是<code>Anaconda</code>环境中的库</h1><p>情景：<code>TinyNeuralNetwork</code>库代码在项目文件夹<code>Retinexformer</code>下面，<code>Anaconda</code>也有一个<code>TinyNeuralNetwork</code>库。现在我们在本地更新了<code>TinyNeuralNetwork</code>库代码，想要运行更新后的库代码中的<code>convert.py</code>代码。这个时候Python有可能会在执行新库代码<code>convert.py</code>的时候，调用<code>Anaconda</code>环境的旧库代码。</p>\n<p>一种方法是指定优先级，强制优先加载项目中的本地库：</p>\n<pre><code class=\"lang-python\">import sys\nimport os\n\n# 获取当前脚本所在目录（TinyNeuralNetwork文件夹的路径）\nTINYNN_DIR = os.path.dirname(os.path.abspath(__file__))\n# 获取项目根目录（假设 TinyNeuralNetwork 是 Retinexformer 的子目录）\nPROJECT_ROOT = os.path.dirname(TINYNN_DIR)\n\n# 将本地库路径插入到 sys.path 的最前面\nsys.path.insert(0, TINYNN_DIR)\nsys.path.insert(0, PROJECT_ROOT)\n\n# 打印验证路径是否正确添加（可选）\nprint(&quot;当前 Python 路径:&quot;)\nfor p in sys.path:\n    print(p)\n</code></pre>\n<p>另一种方式就是在终端执行<code>convert.py</code>而不是在<code>IDE</code>中运行：</p>\n<pre><code class=\"lang-python\">&gt; cd Retinexformer\n&gt; export PYTHONPATH=&quot;$PWD:$PYTHONPATH&quot;\n&gt; python convert.py\n</code></pre>\n<h1 id=\"当Linux内存不足时，扩大交换空间\"><a href=\"#当Linux内存不足时，扩大交换空间\" class=\"headerlink\" title=\"当Linux内存不足时，扩大交换空间\"></a>当<code>Linux</code>内存不足时，扩大交换空间</h1><p>首先要处理一下现有的交换文件：</p>\n<pre><code class=\"lang-bash\"># 查看当前启用的交换空间\nsudo swapon --show\n\n# 如果存在 /swapfile，先关闭交换文件\nsudo swapoff /swapfile\n\n# 删除旧的交换文件\nsudo rm /swapfile\n</code></pre>\n<p>然后创建一个更大内存的交换文件：</p>\n<pre><code class=\"lang-bash\"># 创建8GB交换文件\nsudo fallocate -l 8G /swapfile\nsudo chmod 600 /swapfile\nsudo mkswap /swapfile\nsudo swapon /swapfile\n</code></pre>\n",
            "tags": [
                "人工智能"
            ]
        },
        {
            "id": "https://yunhdan.github.io/baoyan/Embodied-3D-Labs/",
            "url": "https://yunhdan.github.io/baoyan/Embodied-3D-Labs/",
            "title": "Embodied AI Labs",
            "date_published": "2025-03-02T10:29:16.000Z",
            "content_html": "<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p>:::info</p>\n<p>收集一些做具身智能的导师和实验室。</p>\n<p>:::</p>\n<h2 id=\"中大-电子与通信工程学院（报着乐）-rainbow\"><a href=\"#中大-电子与通信工程学院（报着乐）-rainbow\" class=\"headerlink\" title=\"[中大-电子与通信工程学院（报着乐）]{.rainbow}\"></a>[中大-电子与通信工程学院（报着乐）]{.rainbow}</h2><ul>\n<li>郭裕兰（不回），导师主页：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20vY2l0YXRpb25zP3VzZXI9V1FSTnZkc0FBQUFKJmFtcDtobD16aC1DTuOAgeWunumqjOWupOS4u+mhte+8mmh0dHBzOi8vd3d3Lnl1bGFuZ3VvLmNuLw==\">https://scholar.google.com/citations?user=WQRNvdsAAAAJ&amp;hl=zh-CN、实验室主页：https://www.yulanguo.cn/</span></li>\n<li>人机物智能融合实验室（<code>HCP Lab</code>）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cuc3lzdS1oY3AubmV0L2hvbWUv\">https://www.sysu-hcp.net/home/</span></li>\n</ul>\n<h2 id=\"南科大相关主页-rainbow\"><a href=\"#南科大相关主页-rainbow\" class=\"headerlink\" title=\"[南科大相关主页]{.rainbow}\"></a>[南科大相关主页]{.rainbow}</h2><ul>\n<li>系统设计与智能制造学院夏令营：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9zZGltLnN1c3RlY2guZWR1LmNuL2luZGV4L3Nob3c/aWQ9NDY0\">https://sdim.sustech.edu.cn/index/show?id=464</span></li>\n<li>南科大研招网：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9ncy5zdXN0ZWNoLmVkdS5jbi8jL2FkbWlzc2lvbi9pbmRleA==\">https://gs.sustech.edu.cn/#/admission/index</span></li>\n</ul>\n<h2 id=\"南科大计算机科学与工程系-rainbow\"><a href=\"#南科大计算机科学与工程系-rainbow\" class=\"headerlink\" title=\"[南科大计算机科学与工程系]{.rainbow}\"></a>[南科大计算机科学与工程系]{.rainbow}</h2><ul>\n<li>史玉回：<span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cuc3VzdGVjaC5lZHUuY24vemgvZmFjdWx0aWVzL3NoaXl1aHVpLmh0bWw=\">https://www.sustech.edu.cn/zh/faculties/shiyuhui.html</span></li>\n<li>郝祁：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9mYWN1bHR5LnN1c3RlY2guZWR1LmNuLz9jYXQ9MiZhbXA7dGFnaWQ9aGFvcSZhbXA7b3JkZXJieT1kYXRlJmFtcDtpc2Nzcz0xJmFtcDtzbmFwaWQ9MSZhbXA7Z289Mg==\">https://faculty.sustech.edu.cn/?cat=2&amp;tagid=haoq&amp;orderby=date&amp;iscss=1&amp;snapid=1&amp;go=2</span></li>\n</ul>\n<h2 id=\"南科大-机械与能源工程系-rainbow\"><a href=\"#南科大-机械与能源工程系-rainbow\" class=\"headerlink\" title=\"[南科大-机械与能源工程系]{.rainbow}\"></a>[南科大-机械与能源工程系]{.rainbow}</h2><ul>\n<li>周博宇-<code>STAR</code>实验室（已读未回）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9yb2JvdGljcy1zdGFyLmNvbS8=\">https://robotics-star.com/</span></li>\n</ul>\n<h2 id=\"南科大-电子与电气工程系-rainbow\"><a href=\"#南科大-电子与电气工程系-rainbow\" class=\"headerlink\" title=\"[南科大-电子与电气工程系]{.rainbow}\"></a>[南科大-电子与电气工程系]{.rainbow}</h2><ul>\n<li>张宏，实验室主页：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9yY3ZsYWIuZWVlLnN1c3RlY2guZWR1LmNuL3Jlc2VhcmNoX3BhZ2Uv77yM5a+85biI5Li76aG177yaaHR0cHM6Ly93d3cuc3VzdGVjaC5lZHUuY24vemgvZmFjdWx0aWVzL3poYW5naG9uZy5odG1s\">https://rcvlab.eee.sustech.edu.cn/research_page/，导师主页：https://www.sustech.edu.cn/zh/faculties/zhanghong.html</span></li>\n<li>王建坤：<span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cuc3VzdGVjaC5lZHUuY24vemgvZmFjdWx0aWVzL2ppYW5rdW53YW5nLmh0bWw=\">https://www.sustech.edu.cn/zh/faculties/jiankunwang.html</span></li>\n</ul>\n<h2 id=\"南科大-系统设计与智能制造学院-rainbow\"><a href=\"#南科大-系统设计与智能制造学院-rainbow\" class=\"headerlink\" title=\"[南科大-系统设计与智能制造学院]{.rainbow}\"></a>[南科大-系统设计与智能制造学院]{.rainbow}</h2><ul>\n<li>陈亮名：<span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cuc3VzdGVjaC5lZHUuY24vemgvZmFjdWx0aWVzL2xpYW5nbWluZ2NoZW4uaHRtbA==\">https://www.sustech.edu.cn/zh/faculties/liangmingchen.html</span></li>\n</ul>\n<h2 id=\"南大-智科院-rainbow\"><a href=\"#南大-智科院-rainbow\" class=\"headerlink\" title=\"[南大-智科院]{.rainbow}\"></a>[南大-智科院]{.rainbow}</h2><h4 id=\"导师与实验室\"><a href=\"#导师与实验室\" class=\"headerlink\" title=\"导师与实验室\"></a>导师与实验室</h4><ul>\n<li>龙霄潇（已读不回）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cueHhsb25nLnNpdGUvaW5kZXhfY24uaHRtbA==\">https://www.xxlong.site/index_cn.html</span>)</li>\n<li>朱昊（3D）：<span class=\"exturl\" data-url=\"aHR0cDovL3d3dy56aHVoYW8uY2MvaG9tZV96aC8=\">http://www.zhuhao.cc/home_zh/</span></li>\n<li>模式识别与计算机视觉研究组谢晋（<code>PRCV</code>）</li>\n</ul>\n<h2 id=\"南大-计算机学院-rainbow\"><a href=\"#南大-计算机学院-rainbow\" class=\"headerlink\" title=\"[南大-计算机学院]{.rainbow}\"></a>[南大-计算机学院]{.rainbow}</h2><h4 id=\"导师与实验室-1\"><a href=\"#导师与实验室-1\" class=\"headerlink\" title=\"导师与实验室\"></a>导师与实验室</h4><ul>\n<li>推理与学习小组：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9jcy5uanUuZWR1LmNuL3JsL2luZGV4Lmh0bQ==\">https://cs.nju.edu.cn/rl/index.htm</span></li>\n</ul>\n<h2 id=\"清深研究院-rainbow\"><a href=\"#清深研究院-rainbow\" class=\"headerlink\" title=\"[清深研究院]{.rainbow}\"></a>[清深研究院]{.rainbow}</h2><h4 id=\"相关主页\"><a href=\"#相关主页\" class=\"headerlink\" title=\"相关主页\"></a>相关主页</h4><ul>\n<li>研招网：<span class=\"exturl\" data-url=\"aHR0cHM6Ly95emJtLnRzaW5naHVhLmVkdS5jbi9pbmRleA==\">https://yzbm.tsinghua.edu.cn/index</span></li>\n<li>各学院相关信息简介：<span class=\"exturl\" data-url=\"aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC80MjI0NzAxNjg=\">https://zhuanlan.zhihu.com/p/422470168</span></li>\n</ul>\n<h4 id=\"导师与实验室-2\"><a href=\"#导师与实验室-2\" class=\"headerlink\" title=\"导师与实验室\"></a>导师与实验室</h4><ul>\n<li>唐岩松，导师主页：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9hbmR5dGFuZzE1LmdpdGh1Yi5pby8=\">https://andytang15.github.io/</span></li>\n</ul>\n<h2 id=\"人大-信院\"><a href=\"#人大-信院\" class=\"headerlink\" title=\"人大-信院\"></a>人大-信院</h2><ul>\n<li>多媒体计算实验室：<span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cucnVjLWFpbTMuY29tL2luZGV4Lmh0bWw=\">https://www.ruc-aim3.com/index.html</span></li>\n</ul>\n<h2 id=\"北大-信工-rainbow\"><a href=\"#北大-信工-rainbow\" class=\"headerlink\" title=\"[北大-信工]{.rainbow}\"></a>[北大-信工]{.rainbow}</h2><h4 id=\"导师与实验室-3\"><a href=\"#导师与实验室-3\" class=\"headerlink\" title=\"导师与实验室\"></a>导师与实验室</h4><ul>\n<li>刘宏：<span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cuZWNlLnBrdS5lZHUuY24vaW5mby8xMDYyLzIyNDEuaHRt\">https://www.ece.pku.edu.cn/info/1062/2241.htm</span></li>\n</ul>\n<h2 id=\"上科大-信科院-rainbow\"><a href=\"#上科大-信科院-rainbow\" class=\"headerlink\" title=\"[上科大-信科院]{.rainbow}\"></a>[上科大-信科院]{.rainbow}</h2><h4 id=\"相关主页-1\"><a href=\"#相关主页-1\" class=\"headerlink\" title=\"相关主页\"></a>相关主页</h4><ul>\n<li>夏令营：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9zaXN0LnNoYW5naGFpdGVjaC5lZHUuY24vMjAyNS8wNTA0L2M3MzM5YTExMTA2NjIvcGFnZS5odG0=\">https://sist.shanghaitech.edu.cn/2025/0504/c7339a1110662/page.htm</span></li>\n<li>研招系统：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9nYWRtaXNzaW9uLnNoYW5naGFpdGVjaC5lZHUuY24vZW5yb2xsX3N0dWRlbnQvaG9tZT9jb2xsZWdlX29pZD1jNjkwZGQxZWE3ZmY0YzgwODViZWZlN2ViZWVlODlmMA==\">https://gadmission.shanghaitech.edu.cn/enroll_student/home?college_oid=c690dd1ea7ff4c8085befe7ebeee89f0</span></li>\n<li>导师网：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9zaXN0LnNoYW5naGFpdGVjaC5lZHUuY24vc3pkd3gvbGlzdC5odG0=\">https://sist.shanghaitech.edu.cn/szdwx/list.htm</span></li>\n</ul>\n<h4 id=\"导师与实验室-4\"><a href=\"#导师与实验室-4\" class=\"headerlink\" title=\"导师与实验室\"></a>导师与实验室</h4><ul>\n<li>顾家远（<code>SEALab</code>，已读不回），个人主页：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9zaXN0LnNoYW5naGFpdGVjaC5lZHUuY24vZ2p5L2xpc3QuaHRt77yM5oub55Sf6K+05piO77yaaHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC83MDI4NDQ5NTc=\">https://sist.shanghaitech.edu.cn/gjy/list.htm，招生说明：https://zhuanlan.zhihu.com/p/702844957</span></li>\n<li>石野：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9zaGl5ZTIxLmdpdGh1Yi5pby8=\">https://shiye21.github.io/</span></li>\n<li>马跃欣：<span class=\"exturl\" data-url=\"aHR0cHM6Ly95dWV4aW5tYS5tZS8=\">https://yuexinma.me/</span></li>\n<li>汪婧雅：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9mYWN1bHR5LnNpc3Quc2hhbmdoYWl0ZWNoLmVkdS5jbi9mYWN1bHR5L3dhbmdqaW5neWEv\">https://faculty.sist.shanghaitech.edu.cn/faculty/wangjingya/</span></li>\n<li><code>Robot Interaction and Manipulation Lab</code>：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9yaW0tbGFib3JhdG9yeS5naXRodWIuaW8v\">https://rim-laboratory.github.io/</span></li>\n<li>师泽仁：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9saW5rLnpoaWh1LmNvbS8/dGFyZ2V0PWh0dHBzJTNBLy9yb2JvdGljcy5zaGFuZ2hhaXRlY2guZWR1LmNuLw==\">https://link.zhihu.com/?target=https%3A//robotics.shanghaitech.edu.cn/</span></li>\n</ul>\n<h2 id=\"西湖大学\"><a href=\"#西湖大学\" class=\"headerlink\" title=\"西湖大学\"></a>西湖大学</h2><h4 id=\"相关主页：\"><a href=\"#相关主页：\" class=\"headerlink\" title=\"相关主页：\"></a>相关主页：</h4><ul>\n<li>夏令营：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9lbmdpbmVlcmluZy53ZXN0bGFrZS5lZHUuY24vTmV3c0V2ZW50cy9MYXRlc3ROZXdzLzIwMjUwNC90MjAyNTA0MzBfNTUyMTEuc2h0bWw=\">https://engineering.westlake.edu.cn/NewsEvents/LatestNews/202504/t20250430_55211.shtml</span></li>\n<li>研招系统：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9ncmFkYWRtaXNzaW9uLndlc3RsYWtlLmVkdS5jbi9wc2MvQ1NQUkQvRU1QTE9ZRUUvU0EvYy9XTF9DVVNUT01JWkVfTUVOVS5XTF9BRF9MT0dJTl9DT00uR0JMP1BhZ2U9V0xfQURfTE9HSU4mYW1wO0FjdGlvbj1VJmFtcDtsYW5ndWFnZUNkPVpIUyZhbXA7\">https://gradadmission.westlake.edu.cn/psc/CSPRD/EMPLOYEE/SA/c/WL_CUSTOMIZE_MENU.WL_AD_LOGIN_COM.GBL?Page=WL_AD_LOGIN&amp;Action=U&amp;languageCd=ZHS&amp;</span></li>\n</ul>\n<h4 id=\"导师与实验室-5\"><a href=\"#导师与实验室-5\" class=\"headerlink\" title=\"导师与实验室\"></a>导师与实验室</h4><ul>\n<li>盗梦空间<code>3D</code>实验室：<span class=\"exturl\" data-url=\"aHR0cDovL3d3dy5pbmNlcHRpb24zZC5mdW4vIw==\">http://www.inception3d.fun/#</span></li>\n</ul>\n<h2 id=\"同济大学-电子与信息工程学院-rainbow\"><a href=\"#同济大学-电子与信息工程学院-rainbow\" class=\"headerlink\" title=\"[同济大学-电子与信息工程学院]{.rainbow}\"></a>[同济大学-电子与信息工程学院]{.rainbow}</h2><h4 id=\"导师与实验室-6\"><a href=\"#导师与实验室-6\" class=\"headerlink\" title=\"导师与实验室\"></a>导师与实验室</h4><ul>\n<li>王志鹏，导师主页：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9yb2JvdC50b25namkuZWR1LmNuL2luZm8vMTI1Ni8yMDg2Lmh0bQ==\">https://robot.tongji.edu.cn/info/1256/2086.htm</span></li>\n<li>蒋烁，导师主页：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9yb2JvdC50b25namkuZWR1LmNuL2luZm8vMTI1Ni8yMDkwLmh0bQ==\">https://robot.tongji.edu.cn/info/1256/2090.htm</span></li>\n<li>沈润杰，导师主页：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9yb2JvdC50b25namkuZWR1LmNuL2luZm8vMTI1Ni8yMDg0Lmh0bQ==\">https://robot.tongji.edu.cn/info/1256/2084.htm</span></li>\n</ul>\n<h2 id=\"同济大学-电子与信息工程学院-rainbow-1\"><a href=\"#同济大学-电子与信息工程学院-rainbow-1\" class=\"headerlink\" title=\"[同济大学-电子与信息工程学院]{.rainbow}\"></a>[同济大学-电子与信息工程学院]{.rainbow}</h2><h4 id=\"导师与实验室-7\"><a href=\"#导师与实验室-7\" class=\"headerlink\" title=\"导师与实验室\"></a>导师与实验室</h4><ul>\n<li>范睿（已读不回），实验室主页：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9taWFzLmdyb3VwL21lbWJlcnMv77yM5oub55Sf6LS077yaaHR0cHM6Ly93d3cuc2R4ejIwNTAuY29tLzQyNjY4Lmh0bWw=\">https://mias.group/members/，招生贴：https://www.sdxz2050.com/42668.html</span></li>\n<li>陈启军，实验室主页：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9yYWlsLnRvbmdqaS5lZHUuY24vI2NvbnRhY3Q=\">https://rail.tongji.edu.cn/#contact</span></li>\n<li>陆亮，导师主页：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9mYWN1bHR5LnRvbmdqaS5lZHUuY24vbHVsaWFuZzgyOS96aF9DTi9pbmRleC5odG0=\">https://faculty.tongji.edu.cn/luliang829/zh_CN/index.htm</span></li>\n</ul>\n<h2 id=\"武汉大学相关主页-rainbow\"><a href=\"#武汉大学相关主页-rainbow\" class=\"headerlink\" title=\"[武汉大学相关主页]{.rainbow}\"></a>[武汉大学相关主页]{.rainbow}</h2><ul>\n<li>研究生招生服务平台：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9laGFsbC53aHUuZWR1LmNuL2dzYXBwL3N5cy93ZHlqc2JtL2VudHJhbmNlLmRv\">https://ehall.whu.edu.cn/gsapp/sys/wdyjsbm/entrance.do</span></li>\n</ul>\n<h2 id=\"武汉大学信息学部-卫星导航定位技术研究中心-rainbow\"><a href=\"#武汉大学信息学部-卫星导航定位技术研究中心-rainbow\" class=\"headerlink\" title=\"[武汉大学信息学部-卫星导航定位技术研究中心]{.rainbow}\"></a>[武汉大学信息学部-卫星导航定位技术研究中心]{.rainbow}</h2><h4 id=\"导师与实验室-8\"><a href=\"#导师与实验室-8\" class=\"headerlink\" title=\"导师与实验室\"></a>导师与实验室</h4><ul>\n<li>郭迟，导师主页：<span class=\"exturl\" data-url=\"aHR0cDovL2pzenkud2h1LmVkdS5jbi9ndW9jaGkvemhfQ04vaW5kZXguaHRt77yM5a6e6aqM5a6k5Li76aG177yaaHR0cHM6Ly93d3cuemhpeXV0ZWFtLmNvbS8=\">http://jszy.whu.edu.cn/guochi/zh_CN/index.htm，实验室主页：https://www.zhiyuteam.com/</span></li>\n</ul>\n<h2 id=\"武汉大学-动力与机械学院-rainbow\"><a href=\"#武汉大学-动力与机械学院-rainbow\" class=\"headerlink\" title=\"[武汉大学-动力与机械学院]{.rainbow}\"></a>[武汉大学-动力与机械学院]{.rainbow}</h2><h4 id=\"导师与实验室-9\"><a href=\"#导师与实验室-9\" class=\"headerlink\" title=\"导师与实验室\"></a>导师与实验室</h4><ul>\n<li>郭朝，导师主页：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9wbWMud2h1LmVkdS5jbi9pbmZvLzEwMTcvMTU2MTMxLmh0bQ==\">https://pmc.whu.edu.cn/info/1017/156131.htm</span></li>\n</ul>\n<h2 id=\"武汉大学-计算机学院-rainbow\"><a href=\"#武汉大学-计算机学院-rainbow\" class=\"headerlink\" title=\"[武汉大学-计算机学院]{.rainbow}\"></a>[武汉大学-计算机学院]{.rainbow}</h2><h4 id=\"导师与实验室-10\"><a href=\"#导师与实验室-10\" class=\"headerlink\" title=\"导师与实验室\"></a>导师与实验室</h4><ul>\n<li>张敬，导师主页：<span class=\"exturl\" data-url=\"aHR0cDovL2pzenkud2h1LmVkdS5jbi96aGFuZ2ppbmcxMjM0NTYvemhfQ04vaW5kZXguaHRt\">http://jszy.whu.edu.cn/zhangjing123456/zh_CN/index.htm</span></li>\n</ul>\n<h2 id=\"苏州大学-未来科学与工程学院-rainbow\"><a href=\"#苏州大学-未来科学与工程学院-rainbow\" class=\"headerlink\" title=\"[苏州大学-未来科学与工程学院]{.rainbow}\"></a>[苏州大学-未来科学与工程学院]{.rainbow}</h2><h4 id=\"导师与实验室-11\"><a href=\"#导师与实验室-11\" class=\"headerlink\" title=\"导师与实验室\"></a>导师与实验室</h4><ul>\n<li>杨聪，个人主页：<span class=\"exturl\" data-url=\"aHR0cHM6Ly93ZWIuc3VkYS5lZHUuY24veWFuZ2Nvbmcv\">https://web.suda.edu.cn/yangcong/</span></li>\n</ul>\n<h2 id=\"西电-电子工程学院-rainbow\"><a href=\"#西电-电子工程学院-rainbow\" class=\"headerlink\" title=\"[西电-电子工程学院]{.rainbow}\"></a>[西电-电子工程学院]{.rainbow}</h2><h4 id=\"导师与实验室-12\"><a href=\"#导师与实验室-12\" class=\"headerlink\" title=\"导师与实验室\"></a>导师与实验室</h4><ul>\n<li>多模态增强智能实验室：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9zZWUueGlkaWFuLmVkdS5jbi9mYWN1bHR5L3dzZG9uZy9pbmRleC5odG0=\">https://see.xidian.edu.cn/faculty/wsdong/index.htm</span></li>\n</ul>\n<h2 id=\"西电-人智学院-rainbow\"><a href=\"#西电-人智学院-rainbow\" class=\"headerlink\" title=\"[西电-人智学院]{.rainbow}\"></a>[西电-人智学院]{.rainbow}</h2><h4 id=\"导师与实验室-13\"><a href=\"#导师与实验室-13\" class=\"headerlink\" title=\"导师与实验室\"></a>导师与实验室</h4><ul>\n<li>梁雪峰（多模态的），个人主页：<span class=\"exturl\" data-url=\"aHR0cHM6Ly93ZWIueGlkaWFuLmVkdS5jbi94bGlhbmcvaW5kZXguaHRtbA==\">https://web.xidian.edu.cn/xliang/index.html</span></li>\n</ul>\n<h2 id=\"浙大-计院-rainbow\"><a href=\"#浙大-计院-rainbow\" class=\"headerlink\" title=\"[浙大-计院]{.rainbow}\"></a>[浙大-计院]{.rainbow}</h2><h4 id=\"导师与实验室-14\"><a href=\"#导师与实验室-14\" class=\"headerlink\" title=\"导师与实验室\"></a>导师与实验室</h4><ul>\n<li>王文冠，个人主页：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9zaXRlcy5nb29nbGUuY29tL3ZpZXcvd2VuZ3VhbndhbmfvvIzmi5vnlJ/otLTvvJpodHRwczovL3podWFubGFuLnpoaWh1LmNvbS9wLzgxNTE1NjU5MQ==\">https://sites.google.com/view/wenguanwang，招生贴：https://zhuanlan.zhihu.com/p/815156591</span></li>\n<li>Next实验室：<span class=\"exturl\" data-url=\"aHR0cDovL3d3dy5uZXh0LnpqdS5lZHUuY24v\">http://www.next.zju.edu.cn/</span></li>\n<li>张博，个人主页：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9wZXJzb24uemp1LmVkdS5jbi9Ub255Wmhhbmc=\">https://person.zju.edu.cn/TonyZhang</span></li>\n</ul>\n",
            "tags": [
                "保研"
            ]
        },
        {
            "id": "https://yunhdan.github.io/ai/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/",
            "url": "https://yunhdan.github.io/ai/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/",
            "title": "强化学习",
            "date_published": "2025-03-01T09:04:23.000Z",
            "content_html": "<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p>:::info</p>\n<p>学习自李宏毅与蘑菇书。蘑菇书链接：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9kYXRhd2hhbGVjaGluYS5naXRodWIuaW8vZWFzeS1ybC8jLw==\">蘑菇书EasyRL (datawhalechina.github.io)</span></p>\n<p>搭配李宏毅的强化学习课程使用最佳。</p>\n<p>这篇博文内容结合了自己的理解，不可避免地会存在不当，欢迎指正。</p>\n<p>:::</p>\n<h1 id=\"情景定义-rainbow\"><a href=\"#情景定义-rainbow\" class=\"headerlink\" title=\"[情景定义]{.rainbow}\"></a>[情景定义]{.rainbow}</h1><p>智能体<code>Actor</code>、环境<code>Environment</code>与奖励<code>Reward</code>。在后面的内容中，你都可以将智能体理解为玩游戏的机器人，将环境理解为游戏主机，奖励理解为机器人玩游戏干掉怪兽得到的分数。</p>\n<h1 id=\"策略梯度算法（Policy-gradient-Algorithm）-rainbow\"><a href=\"#策略梯度算法（Policy-gradient-Algorithm）-rainbow\" class=\"headerlink\" title=\"[策略梯度算法（Policy gradient Algorithm）]{.rainbow}\"></a>[策略梯度算法（Policy gradient Algorithm）]{.rainbow}</h1><p>智能体通常作为<code>Actor</code>，在 ++策略++ 不断调整与指引下使得其在环境重获取到最大的奖励。策略在具体的实现上，现代强化学习通常用网络替代之。在前面的比喻中，你可以将策略理解为机器人取得高分的方法和手段。</p>\n<h2 id=\"符号定义\"><a href=\"#符号定义\" class=\"headerlink\" title=\"++符号定义++\"></a>++符号定义++</h2><div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">符号</th>\n<th style=\"text-align:center\">释义</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">$t$</td>\n<td style=\"text-align:center\">同一时间下的某个状态</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">$T$</td>\n<td style=\"text-align:center\">某个轨迹下的所有状态数</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">$a_t$</td>\n<td style=\"text-align:center\">某个状态下智能体的动作</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">$s_t$</td>\n<td style=\"text-align:center\">某个状态下的环境</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">$\\theta$</td>\n<td style=\"text-align:center\">智能体的策略模型参数</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">$r_t$</td>\n<td style=\"text-align:center\">某个状态下智能体采取动作后的奖励</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">$\\tau_i$</td>\n<td style=\"text-align:center\">某次轨迹：所有状态的环境与动作的组合。$\\tau=\\{s_1,a_1,s_2,a_2,…s_t,a_t\\}$</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">$p$</td>\n<td style=\"text-align:center\">概率</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">$R$</td>\n<td style=\"text-align:center\">总奖励</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h2 id=\"手推策略梯度公式\"><a href=\"#手推策略梯度公式\" class=\"headerlink\" title=\"++手推策略梯度公式++\"></a>++手推策略梯度公式++</h2><p>给定智能体<code>Actor</code>的参数$\\theta$，可以计算轨迹$\\tau$的发生的概率：</p>\n<script type=\"math/tex; mode=display\">\np_{\\theta}(\\tau) = p(s_1)p_{\\theta}(a_1|s_1)p(s_2|s_1,a_1)p_{\\theta}(a_2|s_2)p(s_3|s_2,a_2)\\cdots    \\\\\n=p(s_1)\\prod_{t=1}^{T}p_{\\theta}(a_t|s_t)p(s_{t+1}|s_t,a_t)    \\tag{1.1}</script><p> $p_{\\theta}(a_1|s_1)$是策略里面的网络参数$\\theta$决定的、观察到环境$s_1$后采取的动作概率。因此策略网络的输出是一个分布，是智能体采取动作的概率分布。$p(s_2|s_1,a_1)$是环境根据前一个环境的状态和智能体的动作给出的下一个环境状态，通常是环境内部的规则决定。</p>\n<p>一个轨迹$\\tau$会在某个时刻终止，其内部的每一个$s_t$和$a_t$的组合均能够产生对应的奖励$r_t$。所以，所有的组合能够得到一个关于这个轨迹的总奖励$R(\\tau)$。</p>\n<p>我们的目的就是要调整策略网络的参数$\\theta$使得总奖励$R(\\tau)$越大越好。因此，$R(\\tau)$是一个随机的变量，我们可以计算$R(\\tau)$：</p>\n<script type=\"math/tex; mode=display\">\nR(\\tau) = \\sum_{t=1}^{T}r_tp_t(r_t|s_t,a_t)    \\tag{1.2}</script><p>当策略参数$\\theta$给定，那么在这组参数上一定有一个关于轨迹的分布$p_{\\theta}(\\tau)$，智能体的多次尝试都是一个轨迹。对于所有的轨迹$\\tau_i$，存在总奖励$\\overline{R}_{\\theta}$的期望值为：</p>\n<script type=\"math/tex; mode=display\">\n\\overline{R}_{\\theta} = \\sum_{\\tau}R(\\tau)p_{\\theta}(\\tau) = \\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)}[R(\\tau)]    \\tag{1.3}</script><p>为了让 ++期望奖励++ 越大越好，要进行 ++梯度上升++ 。所以要计算期望奖励关于策略参数$\\theta$的梯度：</p>\n<script type=\"math/tex; mode=display\">\n\\nabla \\overline{R}_{\\theta} = \\sum_{\\tau}R(\\tau)\\nabla p_{\\theta}(\\tau)    \\\\\n\\because \\nabla f(x) = f(x) \\nabla logf(x) (不难证得)    \\\\\n\\therefore \\frac{\\nabla p_{\\theta(\\tau)}}{p_{\\theta(\\tau)}} = \\nabla logp_{\\theta}(\\tau)</script><p>因此有：</p>\n<script type=\"math/tex; mode=display\">\n\\nabla \\overline{R}_{\\theta} = \\sum_{\\tau}R(\\tau)p_{\\theta}(\\tau)\\frac{\\nabla p_{\\theta}(\\tau)}{p_{\\theta}(\\tau)}    \\\\\n= \\sum_{\\tau}R(\\tau)p_{\\theta}(\\tau)\\nabla logp_{\\theta}(\\tau)     \\\\\n= \\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)}[R(\\tau)\\nabla logp_{\\theta}(\\tau)]</script><p>但是在实际上，我们是无法直接和准确地求期望值$\\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)}[R(\\tau)\\nabla logp_{\\theta}(\\tau)]$的。但是我们可以通过采样<code>N</code>个$\\tau$，然后计算$R(\\tau)\\nabla logp_{\\theta}(\\tau)$的<code>N</code>个和，来近似地得到这个期望：</p>\n<script type=\"math/tex; mode=display\">\n\\nabla \\overline{R}_{\\theta} = \\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)}[R(\\tau)\\nabla logp_{\\theta}(\\tau)] ≈ \\frac{1}{N}\\sum_{n=1}^{N}R(\\tau^n)\\nabla log p_{\\theta}(\\tau^n)    \\\\\n= \\frac{1}{N}\\sum_{n=1}^{N}\\sum_{t=1}^{T_n}R(\\tau^n)\\nabla log p_{\\theta}(a_t^n|s_t^n)    \\tag{1.4}</script><p>得到最后一行的原因，是因为$\\nabla log p_{\\theta}(\\tau^n)$可以被展开：</p>\n<script type=\"math/tex; mode=display\">\n\\nabla log p_{\\theta}(\\tau^n) = \\nabla \\left( logp(s_1)+\\sum_{t=1}^{T}logp_{\\theta}(a_t|s_t) + \\sum_{t=1}^{T}logp(s_{t+1}|s_t,a_t)\\right)    \\\\\n= \\nabla logp(s_1) + \\nabla \\sum_{t=1}^{T}logp_{\\theta}(a_t|s_t)+\\nabla \\sum_{t=1}^{T}logp(s_{t+1}|s_t,a_t)    \\\\</script><p>因为，$p(s_1)$和$p(s_{t+1}|s_t,a_t)$来自环境，与智能体的策略参数$\\theta$无关，因此第一项和第三项为0，原式化简：</p>\n<script type=\"math/tex; mode=display\">\n= \\nabla \\sum_{t=1}^{T}logp_{\\theta}(a_t|s_t)    \\\\\n= \\sum_{t=1}^{T} \\nabla logp_{\\theta}(a_t|s_t)</script><h2 id=\"训练思路\"><a href=\"#训练思路\" class=\"headerlink\" title=\"++训练思路++\"></a>++训练思路++</h2><p>现在整理一下思路。在给出一个初始策略参数$\\theta$的情况下，经过训练，我们要得到一个不错的策略参数$\\theta$，使得智能体在多个轨迹下得到的总奖励$\\overline{R}_{\\theta}$最大化。如何优化这个$\\theta$呢？我们可以通过梯度上升法优化这个$\\theta$，使得当$s_t$状态下执行$a_t$导致$R(\\tau)$增大时增加在$s_t$下执行$a_t$的概率，反之减小这个概率：</p>\n<script type=\"math/tex; mode=display\">\n\\theta \\leftarrow \\theta + \\eta \\nabla \\overline{R}_{\\theta}    \\tag{1.5}</script><p>根据公式1.4和公式1.5，我们知道优化$\\theta$的关键在于求出$\\nabla \\overline{R}_{\\theta}$，与$\\nabla \\overline{R}_{\\theta}$有关的是$s_t^n$、$a_t^n$和$\\tau^n$。所以我们要采集这些数据：</p>\n<script type=\"math/tex; mode=display\">\n\\tau^1:(s_1^1, a_1^1) \\quad\\quad R(\\tau^1)    \\\\\n\\quad\\quad (s_2^1, a_2^1) \\quad\\quad R(\\tau^1)    \\\\\n\\quad\\quad (s_3^1, a_3^1) \\quad\\quad R(\\tau^1)    \\\\\n\\quad\\quad\\quad \\vdots \\quad\\quad\\quad\\quad\\quad \\vdots    \\\\\n\\tau^2:(s_1^2, a_1^2) \\quad\\quad R(\\tau^2)    \\\\\n\\quad\\quad (s_2^2, a_2^2) \\quad\\quad R(\\tau^2)    \\\\\n\\quad\\quad (s_3^2, a_3^2) \\quad\\quad R(\\tau^2)    \\\\\n\\quad\\quad\\quad \\vdots \\quad\\quad\\quad\\quad\\quad \\vdots    \\\\</script><p>$\\tau$的采集量是人为设定的，采集完一次后，会获得多组$\\tau$的数据，然后使用公式1.4和公式1.5一次性更新模型：</p>\n<p><img data-src=\"../../assets/reinforce.jpg\" alt=\"image\"></p>\n<h2 id=\"直观理解\"><a href=\"#直观理解\" class=\"headerlink\" title=\"++直观理解++\"></a>++直观理解++</h2><p>如何直观理解公式1.4？既然我们要通过求出$\\nabla \\overline{R}_{\\theta}$来得到$\\theta$的更新值，就不得不依赖反向传播得到梯度。你可以想象这样一个场景：智能体在玩一个游戏，控制一个飞机射击外星人，它的策略$\\theta$是智能体内部的网络<code>NN</code>（<code>Neural Network</code>），在游戏过程中保持不变（游戏过程就是在采样若干组轨迹的<code>s</code>，<code>a</code>，<code>R</code>(<script type=\"math/tex\">\\tau</script>)对），智能体看到场景$s$，送给策略网络<code>NN</code>计算出要做的动作$a$的概率。如，向左移动的概率，向右移动的概率，开火的概率，所以这本质上是一个分类问题。</p>\n<p><img data-src=\"../../assets/reinforce2.jpg\" alt=\"\"></p>\n<p>通常在训练过程中，我们需要智能体根据当前场景做出的动作是人为指定的，也就是说对于某个轨迹$\\tau$的一个特定的$s$有个<code>Ground-Truth</code>，这个<code>Ground-Truth</code>就是在这个特定场景应该做出的动作$a$。比如，在某个场景$s_t$下，我们想要智能体向左，于是标签就是<code>[1, 0, 0]</code>。智能体将$s_t$作为策略网络$\\theta$的输入，估计出向左的概率要尽可能接近1，即$p_{\\theta}(a_t|s_t)$要尽可能接近1。</p>\n<p>所以，在采样过程中，大量的$s_t^n$计算出大量的$p_{\\theta}(a_t|s_t)$，那么根据每个$s_t$的标签，就能够计算一个交叉熵损失（<code>cross entropy loss</code>），最小化交叉熵损失就是在最大化似然：</p>\n<script type=\"math/tex; mode=display\">\nLoss = - \\sum_{t=1}^{T} y_i \\hat{y}_i    \\\\\nMin(Loss) = Min(- \\sum_{t=1}^{T} y_i \\hat{y}_i) \\\\\n= Max(\\sum_{t=1}^{T} y_i p_{\\theta}(a_t|s_t))    \\\\\n\\because y_i是常数1    \\\\\n\\therefore Min(Loss) = Max(\\sum_{t=1}^{T} p_{\\theta}(a_t|s_t)) = Max(\\sum_{t=1}^{T} log p_{\\theta}(a_t|s_t))</script><p>所以，在训练过程中，最小化交叉熵损失，实际上在最大化某个轨迹的预测值的和。观察上面的式子，有没有发现在最小化损失，实际上在最大化$\\nabla \\overline{R}_{\\theta}$。</p>\n<p>在计算出损失后，通过反向传播，可以直接计算出$\\nabla \\overline{R}_{\\theta}$的值，因此就能更新参数$\\theta$了。</p>\n<h2 id=\"实现上的一些技巧\"><a href=\"#实现上的一些技巧\" class=\"headerlink\" title=\"++实现上的一些技巧++\"></a>++实现上的一些技巧++</h2><h3 id=\"添加基线-dot\"><a href=\"#添加基线-dot\" class=\"headerlink\" title=\"++添加基线++{.dot}\"></a>++添加基线++{.dot}</h3><p>我们总是希望，对于某一给定的状态$s$采取动作$a$后，整场游戏$\\tau$获得的奖励是正的，我们就增加$(s,a)$的概率。如果这个给定的状态$s$采取的动作$a$使得最后整场游戏$\\tau$的奖励是负的，我们就减小$(s,a)$的概率。</p>\n<p>理想情况是这样，现实是虽然这些动作对整场游戏$\\tau$的奖励贡献有大有小，但整场游戏下来的奖励总是非负的，有的动作采取后得到了20分，有的动作采取后得到0分。这种情况下，一场游戏的奖励$R(\\tau)$总是正的，最低也只是0，而且要求提升贡献度大的动作的概率，降低贡献度低的动作的概率。</p>\n<p>另外一个现实是，由于本质上一个轨迹$\\tau$只是在轨迹空间$p_\\theta(\\tau)$的一个采样，所以采样的数量较少时，一些动作可能未被采样到。那么相较于其他被采样到的动作而言，这个未被采样到的动作的概率就会被下降。这并不意味着这个未被采样到的动作贡献更小，仅仅只是未被采样到而已。相反，贡献度低的动作因为奖励总是正的、经常被采样到，所以概率提升地比未被采样、但贡献度高的动作幅度大。这就导致了不公平的出现。</p>\n<p><img data-src=\"../../assets/reinforce3.jpg\" alt=\"image\"></p>\n<p>奖励总是正的，就会导致丈量动作的贡献度相较于奖励有正有负而更加困难。除此之外，还引出了概率提升膨胀的问题，未被采样到的动作的概率不升反降，而其他采样到的动作因为奖励总是正的而概率得到很大幅度的提升。</p>\n<p>为了解决这个问题，可以把奖励减去一个基线<code>b</code>：</p>\n<script type=\"math/tex; mode=display\">\n\\nabla \\overline{R}_\\theta ≈ \\frac{1}{N}\\sum_{n=1}^{N}\\sum_{t=1}^{T_n}(R(\\tau^n) - b)\\nabla log p_{\\theta}(a_t^n|s_t^n)    \\tag{1.5}</script><p>此时总奖励就是$R(\\tau^n) - b$。我们可以令$b≈E[R(\\tau)]$，也就是说，我们在训练中不断地将$R(\\tau)$的值记录下来，然后不断地记录$R(\\tau)$的平均值，将这个平均值当作$b$来使用。这样一来，总奖励$R(\\tau^n) - b$就会有正有负。</p>\n<h3 id=\"分配合适的分数-dot\"><a href=\"#分配合适的分数-dot\" class=\"headerlink\" title=\"++分配合适的分数++{.dot}\"></a>++分配合适的分数++{.dot}</h3><p>观察公式1.5，只要在同一场游戏里（同一轨迹），所有的动作-状态对都要使用同样的奖励权重进行加权。也就是说，对于$(a_1^1,s_1^1),(a_2^1,s_2^1),(a_3^1,s_3^1),…,(a_T^1,s_T^1)$都使用$R(\\tau^1)-b$进行加权，当$n=2$时同理。</p>\n<p>这是不公平的，在蘑菇书中结合例子解释的非常深入浅出。简单来说就是，一场游戏的结果是好的，并不意味着每一个采取的动作都是好的。相反，若是整场游戏的结果不好，并不代表每一个动作都是不好的。</p>\n<p>一种解决办法是，计算某个动作状态对的奖励权重时，不把整场游戏的奖励加起来，而是只计算从这个动作执行以后到整场游戏结束时得到的奖励。这样做是因为这个动作执行之前发生的事情是与这个动作没有关系的，所以执行当前这个动作之前所获得的所有奖励都不能算作是当前这个动作的贡献。将执行这个动作后获得所有奖励加起来，才算做这个动作真正的贡献。</p>\n<p><img data-src=\"../../assets/reinforce4.jpg\" alt=\"image\"></p>\n<p>比如，这张图$(s_a,a_1)$的权重是$(+5+0-2)=+3$，$(s_b,a_2)$的权重是$(+0-2)=-2$。即使是第二场游戏也是如此规律。</p>\n<p>于是，重写公式1.5：</p>\n<script type=\"math/tex; mode=display\">\n\\nabla \\overline{R}_\\theta ≈ \\frac{1}{N}\\sum_{n=1}^{N}\\sum_{t=1}^{T_n}(\\sum_{t'=t}^{T_n}{r_{t'}^{n}} - b)\\nabla log p_{\\theta}(a_t^n|s_t^n)    \\tag{1.6}</script><p>原来的权重是整场游戏的奖励的总和，现在改成从某个时刻$t$开始，假设这个动作是在开$t$始执行的，从$t$一直到游戏结束所有奖励的总和才能代表这个动作的好坏。</p>\n<p>进一步，我们可以为未来的奖励做折扣，继续改写公式1.6：</p>\n<script type=\"math/tex; mode=display\">\n\\nabla \\overline{R}_\\theta ≈ \\frac{1}{N}\\sum_{n=1}^{N}\\sum_{t=1}^{T_n}(\\sum_{t'=t}^{T_n}{\\gamma^{t'-t}r_{t'}^{n}} - b)\\nabla log p_{\\theta}(a_t^n|s_t^n)    \\tag{1.7}</script><p>为什么为未来时刻的奖励乘一个系数做折扣？因为当前时刻动作对下一时刻影响较大，但是随着时间推移，到某一时刻$t$时所受那个动作影响就越来越小。可以取系数$\\gamma=0.9或0.99 \\in [0,1]$。例如，假设游戏有两个回合，我们在游戏的第二回合的某一个 $s_t$执行$a_t$得到+1分，在$s_{t+1}$执行$a_{t+1}$得到+3分，在$s_{t+2}$执行$a_{t+2}$得到−5分，第二回合结束。$a_t$的分数应该是：$1 + \\gamma \\times 3 + \\gamma^2 \\times (-5)$。</p>\n<h3 id=\"优势函数与评论员-dot\"><a href=\"#优势函数与评论员-dot\" class=\"headerlink\" title=\"++优势函数与评论员++{.dot}\"></a>++优势函数与评论员++{.dot}</h3><p>观察公式1.7。事实上，$b$通常是一个网络估计出来的，是一个网络的输出。而$\\sum_{t’=t}^{T_n}{\\gamma^{t’-t}r_{t’}^{n}} - b$这一项通常简写为$A^\\theta(s_t,a_t)$，被称为优势函数。此时，公式1.7又可以写成：</p>\n<script type=\"math/tex; mode=display\">\n\\nabla \\overline{R}_\\theta = \\mathbb{E}_{\\tau \\sim p_\\theta(\\tau)}[R(\\tau)\\nabla logp_\\theta(\\tau)] ≈ \\frac{1}{N}\\sum_{n=1}^{N}\\sum_{t=1}^{T_n}A^\\theta(s_t,a_t)\\nabla log p_{\\theta}(a_t^n|s_t^n)    \\tag{1.8}</script><p>这个优势函数也通常可以被一个网络估计出来，这个网络被称为评论员。优势函数的意义是，在某个特定状态$s_t$下采取动作$a_t$，相较于其他可能执行的动作，$a_t$有多好。这个优势函数作为权重反映了动作相对的好，而不是绝对的好。</p>\n<h1 id=\"近端策略优化算法（PPO）-rainbow\"><a href=\"#近端策略优化算法（PPO）-rainbow\" class=\"headerlink\" title=\"[近端策略优化算法（PPO）]{.rainbow}\"></a>[近端策略优化算法（PPO）]{.rainbow}</h1><h2 id=\"On-Policy-to-Off-Policy\"><a href=\"#On-Policy-to-Off-Policy\" class=\"headerlink\" title=\"++On Policy to Off Policy++\"></a>++On Policy to Off Policy++</h2><h1 id=\"深度Q网络\"><a href=\"#深度Q网络\" class=\"headerlink\" title=\"深度Q网络\"></a>深度Q网络</h1>",
            "tags": [
                "人工智能"
            ]
        },
        {
            "id": "https://yunhdan.github.io/cs/Pip%E3%80%81Conda%E3%80%81github%E9%95%9C%E5%83%8F/",
            "url": "https://yunhdan.github.io/cs/Pip%E3%80%81Conda%E3%80%81github%E9%95%9C%E5%83%8F/",
            "title": "Pip、Conda、Github镜像",
            "date_published": "2025-02-24T12:36:05.000Z",
            "content_html": "<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h1 id=\"Pip\"><a href=\"#Pip\" class=\"headerlink\" title=\"Pip\"></a><code>Pip</code></h1><p>:::info </p>\n<p>用<code>Pip</code>安装需要的<code>Python</code>包已经是开发过程的刚需，其中，遇到超时和缓慢的问题也是家常便饭。这里总结一下解决这类问题的方法。</p>\n<p>:::</p>\n<p>以<code>Tensorflow</code>的安装为例，只需要增加镜像的使用即可实现。</p>\n<p>有以下命令供选择，具体到其他包可以替换<code>Tensorflow</code>：</p>\n<pre><code class=\"lang-shell\">pip install tensorflow -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.com\n</code></pre>\n<pre><code class=\"lang-shell\">pip install tensorflow -i https://pypi.tuna.tsinghua.edu.cn/simple/\n</code></pre>\n<pre><code class=\"lang-shell\">pip install tensorflow -i http://pypi.douban.com/simple/ --trusted-host douban.com\n</code></pre>\n<pre><code class=\"lang-shell\">pip install tensorflow -i https://pypi.mirrors.ustc.edu.cn/simple/\n</code></pre>\n<p>加入<code>trusted-host</code>是因为有些网站不被<code>pip</code>所信任，所以可能会出现安装包失败的问题，需要手动加入。</p>\n<h1 id=\"Conda\"><a href=\"#Conda\" class=\"headerlink\" title=\"Conda\"></a><code>Conda</code></h1><p>:::info</p>\n<p>有时候创建<code>conda</code>环境时没有镜像源的话很容易报<code>HTTPERROR</code>。</p>\n<p>:::</p>\n<p>可以找到<code>anaconda/bin</code>，然后用<code>vim</code>修改这个文件：<code>vim ~/.condarc</code>，具体是在<code>channels</code>处增加：</p>\n<pre><code class=\"lang-raw\">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/\n  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/\n  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/\n  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/\n  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/\n</code></pre>\n<p>并删除：</p>\n<pre><code class=\"lang-raw\">  - default\n</code></pre>\n<p>如果没有<code>channels</code>就自己添加，保证这个文件有这样的内容：</p>\n<pre><code class=\"lang-raw\">channels:\n  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/\n  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/\n  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/\n  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/\n  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/\n</code></pre>\n<p>然后<code>ESC</code>输入<code>:wq!</code>退出即可。</p>\n<h1 id=\"Github\"><a href=\"#Github\" class=\"headerlink\" title=\"Github\"></a><code>Github</code></h1><p>:::info</p>\n<p>想下载<code>Github</code>上面的资源的一种常见方法是挂代理。在终端还可以考虑另一种方法：以镜像站代替。</p>\n<p>:::</p>\n<p>例如，需要使用<code>wget</code>获取<code>github</code>的某个资源时：</p>\n<pre><code class=\"lang-bash\">wget https://github.com/Dao-AILab/causal-conv1d/releases/download/v1.0.0/causal_conv1d-1.0.0+cu118torch1.13cxx11abiFALSE-cp39-cp39-linux_x86_64.whl\n</code></pre>\n<p>会提示超时或无法连接、无法下载。</p>\n<p>方法十分简单，用镜像站的网址替代<code>github.com</code>，目前可用、好用的镜像站如下：</p>\n<ul>\n<li><code>bgithub.xyz</code></li>\n</ul>\n<p>使用就是如下：</p>\n<pre><code class=\"lang-bash\">wget https://bgithub.xyz/Dao-AILab/causal-conv1d/releases/download/v1.0.0/causal_conv1d-1.0.0+cu118torch1.13cxx11abiFALSE-cp39-cp39-linux_x86_64.whl\n</code></pre>\n<p>可见仅仅替换<code>github.com</code>即可。当然，有些镜像站的食用方式并不是这样简单的替代对应域名，也有在<code>github.com</code>前加域名的。</p>\n",
            "tags": [
                "计算机"
            ]
        },
        {
            "id": "https://yunhdan.github.io/cs/ACM%E7%AE%97%E6%B3%95%E9%A2%98%E5%8D%95/",
            "url": "https://yunhdan.github.io/cs/ACM%E7%AE%97%E6%B3%95%E9%A2%98%E5%8D%95/",
            "title": "ACM算法题单",
            "date_published": "2025-02-20T14:22:05.000Z",
            "content_html": "<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p>:::info</p>\n<p>目的：为了备战蓝桥杯（4月12日）和夏令营预推免的机试。</p>\n<p>每一题目标：彻底弄懂该题思路。</p>\n<p>:::</p>\n<h1 id=\"哈希表-blue\"><a href=\"#哈希表-blue\" class=\"headerlink\" title=\"[哈希表]{.blue}\"></a>[哈希表]{.blue}</h1><p><strong>简单</strong>：</p>\n<p><code>LeetCode——</code>罗马数字转整数：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9yb21hbi10by1pbnRlZ2VyLz9lbnZUeXBlPXByb2JsZW0tbGlzdC12MiZhbXA7ZW52SWQ9aGFzaC10YWJsZQ==\">https://leetcode.cn/problems/roman-to-integer/?envType=problem-list-v2&amp;envId=hash-table</span></p>\n<p><code>LeetCode——</code>多数元素：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9tYWpvcml0eS1lbGVtZW50Lz9lbnZUeXBlPXByb2JsZW0tbGlzdC12MiZhbXA7ZW52SWQ9aGFzaC10YWJsZQ==\">https://leetcode.cn/problems/majority-element/?envType=problem-list-v2&amp;envId=hash-table</span></p>\n<p><code>LeetCode——</code>两数之和：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy90d28tc3VtL2Rlc2NyaXB0aW9uLz9lbnZUeXBlPXByb2JsZW0tbGlzdC12MiZhbXA7ZW52SWQ9aGFzaC10YWJsZQ==\">1. 两数之和 - 力扣（LeetCode）</span></p>\n<p><code>LeetCode——</code>环形链表：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9saW5rZWQtbGlzdC1jeWNsZS8/ZW52VHlwZT1wcm9ibGVtLWxpc3QtdjImYW1wO2VudklkPWhhc2gtdGFibGU=\">141. 环形链表 - 力扣（LeetCode）</span></p>\n<p><code>LeetCode——</code>相交链表：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9pbnRlcnNlY3Rpb24tb2YtdHdvLWxpbmtlZC1saXN0cy9kZXNjcmlwdGlvbi8/ZW52VHlwZT1wcm9ibGVtLWxpc3QtdjImYW1wO2VudklkPWhhc2gtdGFibGU=\">160. 相交链表 - 力扣（LeetCode）</span></p>\n<p><code>LeetCode——</code>快乐数：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9oYXBweS1udW1iZXIvZGVzY3JpcHRpb24v\">202. 快乐数 - 力扣（LeetCode）</span></p>\n<p><code>LeetCode——</code>同构字符串：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9pc29tb3JwaGljLXN0cmluZ3MvZGVzY3JpcHRpb24v\">205. 同构字符串 - 力扣（LeetCode）</span></p>\n<p><code>LeetCode——</code>存在重复元素：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9jb250YWlucy1kdXBsaWNhdGUvZGVzY3JpcHRpb24v\">217. 存在重复元素 - 力扣（LeetCode）</span></p>\n<p><strong>中等</strong>：</p>\n<p><code>LeetCode——</code>字母异位词分组：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9ncm91cC1hbmFncmFtcy9kZXNjcmlwdGlvbi8/ZW52VHlwZT1zdHVkeS1wbGFuLXYyJmFtcDtlbnZJZD10b3AtMTAwLWxpa2Vk\">49. 字母异位词分组 - 力扣（LeetCode）</span>:fire:</p>\n<p><code>LeetCode——</code>最长连续序列：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9sb25nZXN0LWNvbnNlY3V0aXZlLXNlcXVlbmNlLz9lbnZUeXBlPXN0dWR5LXBsYW4tdjImYW1wO2VudklkPXRvcC0xMDAtbGlrZWQ=\">128. 最长连续序列 - 力扣（LeetCode）</span>:fire:</p>\n<h1 id=\"双指针-blue\"><a href=\"#双指针-blue\" class=\"headerlink\" title=\"[双指针]{.blue}\"></a>[双指针]{.blue}</h1><p>[相向双指针]{.rainbow}：</p>\n<p><code>LeetCode——</code>反转字符串（优化相向指针）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9yZXZlcnNlLXN0cmluZy9kZXNjcmlwdGlvbi8/ZW52VHlwZT1wcm9ibGVtLWxpc3QtdjImYW1wO2VudklkPXR3by1wb2ludGVycw==\">https://leetcode.cn/problems/reverse-string/description/?envType=problem-list-v2&amp;envId=two-pointers</span></p>\n<p><code>LeetCode——</code>验证回文串（相向指针）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy92YWxpZC1wYWxpbmRyb21lLz9lbnZUeXBlPXByb2JsZW0tbGlzdC12MiZhbXA7ZW52SWQ9dHdvLXBvaW50ZXJz\">125. 验证回文串 - 力扣（LeetCode）</span> </p>\n<p><code>LeetCode——</code>回文链表（相向指针）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9wYWxpbmRyb21lLWxpbmtlZC1saXN0L2Rlc2NyaXB0aW9uLz9lbnZUeXBlPXByb2JsZW0tbGlzdC12MiZhbXA7ZW52SWQ9dHdvLXBvaW50ZXJz\">234. 回文链表 - 力扣（LeetCode）</span></p>\n<p><code>LeetCode——</code>盛最多水的容器（暴力必超时，相向指针）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9jb250YWluZXItd2l0aC1tb3N0LXdhdGVyLz9lbnZUeXBlPXN0dWR5LXBsYW4tdjImYW1wO2VudklkPXRvcC0xMDAtbGlrZWQ=\">11. 盛最多水的容器 - 力扣（LeetCode）</span>:fire:</p>\n<p><code>LeetCode——</code>三数之和（枚举与相向指针的结合）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy8zc3VtLz9lbnZUeXBlPXN0dWR5LXBsYW4tdjImYW1wO2VudklkPXRvcC0xMDAtbGlrZWQ=\">15. 三数之和 - 力扣（LeetCode）</span>:fire:</p>\n<p>[原地修改]{.rainbow}：</p>\n<p><code>LeetCode——</code>移动零（快慢指针）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9tb3ZlLXplcm9lcy9kZXNjcmlwdGlvbi8/ZW52VHlwZT1zdHVkeS1wbGFuLXYyJmFtcDtlbnZJZD10b3AtMTAwLWxpa2Vk\">283. 移动零 - 力扣（LeetCode）</span></p>\n<p><code>LeetCode——</code>删除有序数组中的重复项：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9yZW1vdmUtZHVwbGljYXRlcy1mcm9tLXNvcnRlZC1hcnJheS9kZXNjcmlwdGlvbi8/ZW52VHlwZT1wcm9ibGVtLWxpc3QtdjImYW1wO2VudklkPXR3by1wb2ludGVycw==\">26. 删除有序数组中的重复项 - 力扣（LeetCode）</span></p>\n<p><code>LeetCode——</code>移除元素：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9yZW1vdmUtZWxlbWVudC9kZXNjcmlwdGlvbi8/ZW52VHlwZT1wcm9ibGVtLWxpc3QtdjImYW1wO2VudklkPXR3by1wb2ludGVycw==\">27. 移除元素 - 力扣（LeetCode）</span></p>\n<p>[其他]{.rainbow}：</p>\n<p><code>LeetCode——</code>找出字符串中第一个匹配项的下标：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9maW5kLXRoZS1pbmRleC1vZi10aGUtZmlyc3Qtb2NjdXJyZW5jZS1pbi1hLXN0cmluZy9kZXNjcmlwdGlvbi8/ZW52VHlwZT1wcm9ibGVtLWxpc3QtdjImYW1wO2VudklkPXR3by1wb2ludGVycw==\">28. 找出字符串中第一个匹配项的下标 - 力扣（LeetCode）</span></p>\n<p><code>LeetCode——</code>合并两个有序数组（三指针）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9tZXJnZS1zb3J0ZWQtYXJyYXkvP2VudlR5cGU9cHJvYmxlbS1saXN0LXYyJmFtcDtlbnZJZD10d28tcG9pbnRlcnM=\">https://leetcode.cn/problems/merge-sorted-array/?envType=problem-list-v2&amp;envId=two-pointers</span> :fire:</p>\n<p><code>LeetCode——</code>反转字符串中的元音字母（双向指针）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9yZXZlcnNlLXZvd2Vscy1vZi1hLXN0cmluZy9kZXNjcmlwdGlvbi8/ZW52VHlwZT1wcm9ibGVtLWxpc3QtdjImYW1wO2VudklkPXR3by1wb2ludGVycw==\">345. 反转字符串中的元音字母 - 力扣（LeetCode）</span></p>\n<h1 id=\"滑动窗口-blue\"><a href=\"#滑动窗口-blue\" class=\"headerlink\" title=\"[滑动窗口]{.blue}\"></a>[滑动窗口]{.blue}</h1><p><strong>简单</strong>：</p>\n<p><code>LeetCode——</code>存在重复元素<code>II</code>（暴力必超时，定长滑窗）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9jb250YWlucy1kdXBsaWNhdGUtaWkvZGVzY3JpcHRpb24vP2VudlR5cGU9cHJvYmxlbS1saXN0LXYyJmFtcDtlbnZJZD1zbGlkaW5nLXdpbmRvdw==\">219. 存在重复元素 II - 力扣（LeetCode）</span></p>\n<p><code>LeetCode——</code>最长和谐子序列（变长滑窗）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9sb25nZXN0LWhhcm1vbmlvdXMtc3Vic2VxdWVuY2UvZGVzY3JpcHRpb24vP2VudlR5cGU9cHJvYmxlbS1saXN0LXYyJmFtcDtlbnZJZD1zbGlkaW5nLXdpbmRvdw==\">594. 最长和谐子序列 - 力扣（LeetCode）</span></p>\n<p><code>LeetCode——</code>子数组最大平均数<code>I</code>（定长滑窗）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9tYXhpbXVtLWF2ZXJhZ2Utc3ViYXJyYXktaS9kZXNjcmlwdGlvbi8/ZW52VHlwZT1wcm9ibGVtLWxpc3QtdjImYW1wO2VudklkPXNsaWRpbmctd2luZG93\">643. 子数组最大平均数 I - 力扣（LeetCode）</span></p>\n<p><code>LeetCode——</code>拆炸弹（定长滑窗）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9kZWZ1c2UtdGhlLWJvbWIvZGVzY3JpcHRpb24vP2VudlR5cGU9cHJvYmxlbS1saXN0LXYyJmFtcDtlbnZJZD1zbGlkaW5nLXdpbmRvdw==\">1652. 拆炸弹 - 力扣（LeetCode）</span>:fire:</p>\n<p><code>LeetCode——</code>找到一个数字的<code>K</code>美丽值（定长滑窗）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9maW5kLXRoZS1rLWJlYXV0eS1vZi1hLW51bWJlci8/ZW52VHlwZT1wcm9ibGVtLWxpc3QtdjImYW1wO2VudklkPXNsaWRpbmctd2luZG93\">2269. 找到一个数字的 K 美丽值 - 力扣（LeetCode）</span>:fire:</p>\n<p><code>LeetCode——</code>每个字符最多出现两次的最长子字符串（变长滑窗）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9tYXhpbXVtLWxlbmd0aC1zdWJzdHJpbmctd2l0aC10d28tb2NjdXJyZW5jZXMvZGVzY3JpcHRpb24v\">https://leetcode.cn/problems/maximum-length-substring-with-two-occurrences/description/</span></p>\n<p><strong>中等</strong>：</p>\n<p><code>LeetCode——</code>定长子串中元音的最大数目（最佳定长滑窗入门题）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9tYXhpbXVtLW51bWJlci1vZi12b3dlbHMtaW4tYS1zdWJzdHJpbmctb2YtZ2l2ZW4tbGVuZ3RoL2Rlc2NyaXB0aW9uLw==\">1456. 定长子串中元音的最大数目 - 力扣（LeetCode）</span></p>\n<p>+++info Summary<br>定长滑窗模板：<br>入——更新——出</p>\n<ul>\n<li>[入]{.blue}：下标为 i 的元素进入窗口，更新相关统计量。如果 i&lt;k−1 则重复第一步，目的是为了让窗口全部进入。窗口全部进入后，循环下标值i==k-1，开始做更新的内容。</li>\n<li>[更新]{.blue}：更新答案。一般是更新最大值/最小值。</li>\n<li>[出]{.blue}：下标为 i−k+1 的尾部元素离开窗口，更新相关统计量。<pre><code class=\"lang-python\">class Solution:\n  def maxVowels(self, s: str, k: int) -&gt; int:\n      ans = vowel = 0\n      for i, c in enumerate(s):\n          # 1. 进入窗口\n          if c in &quot;aeiou&quot;:\n              vowel += 1\n          if i &lt; k - 1:  # 窗口大小不足 k\n              continue\n          # 2. 更新答案\n          ans = max(ans, vowel)\n          # 3. 离开窗口\n          if s[i - k + 1] in &quot;aeiou&quot;:\n              vowel -= 1\n      return ans\n</code></pre>\n+++</li>\n</ul>\n<p><code>LeetCode——</code>无重复字符的最长子串（变长滑窗入门题）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9sb25nZXN0LXN1YnN0cmluZy13aXRob3V0LXJlcGVhdGluZy1jaGFyYWN0ZXJzL2Rlc2NyaXB0aW9uLz9lbnZUeXBlPXN0dWR5LXBsYW4tdjImYW1wO2VudklkPXRvcC0xMDAtbGlrZWQ=\">3. 无重复字符的最长子串 - 力扣（LeetCode）</span></p>\n<p>+++info Summary<br>变长滑窗的核心思想是：维护一个有条件的滑动窗口。滑窗右端点右移的目的是为了扩大窗口，破坏条件。滑窗左侧端点左移的目的是为了维护这个条件，直至条件成立。下面是与哈希集合结合的滑窗去重模板（学习自灵茶山艾府）：</p>\n<pre><code class=\"lang-python\">class Solution:\n    def lengthOfLongestSubstring(self, s: str) -&gt; int:\n        table = set()\n        left = maxValue = 0\n        for right, c in enumerate(s):\n            while c in table:\n                table.remove(s[left])\n                left += 1\n            table.add(c)\n            maxValue = max(maxValue, right - left + 1)\n        return maxValue\n</code></pre>\n<p>变长滑窗需要额外的一个指针或者一个别的手段记录滑窗尾部位置。<br>+++</p>\n<p><code>LeetCode——</code>重复的<code>DNA</code>序列：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9yZXBlYXRlZC1kbmEtc2VxdWVuY2VzL2Rlc2NyaXB0aW9uLz9lbnZUeXBlPXByb2JsZW0tbGlzdC12MiZhbXA7ZW52SWQ9c2xpZGluZy13aW5kb3c=\">187. 重复的DNA序列 - 力扣（LeetCode）</span></p>\n<p><code>LeetCode——</code>长度最小的数组（变长滑窗）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9taW5pbXVtLXNpemUtc3ViYXJyYXktc3VtLz9lbnZUeXBlPXByb2JsZW0tbGlzdC12MiZhbXA7ZW52SWQ9c2xpZGluZy13aW5kb3c=\">209. 长度最小的子数组 - 力扣（LeetCode）</span>:fire:</p>\n<p><code>LeetCode——</code>大小为<code>K</code>且平均值大于等于阈值的子数组数目（定长滑窗练手题）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9udW1iZXItb2Ytc3ViLWFycmF5cy1vZi1zaXplLWstYW5kLWF2ZXJhZ2UtZ3JlYXRlci10aGFuLW9yLWVxdWFsLXRvLXRocmVzaG9sZC9kZXNjcmlwdGlvbi8=\">https://leetcode.cn/problems/number-of-sub-arrays-of-size-k-and-average-greater-than-or-equal-to-threshold/description/</span></p>\n<p><code>LeetCode——</code>半径为<code>K</code>的子数组平均值（定长滑窗检验入门题）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9rLXJhZGl1cy1zdWJhcnJheS1hdmVyYWdlcy9kZXNjcmlwdGlvbi8=\">https://leetcode.cn/problems/k-radius-subarray-averages/description/</span></p>\n<p><code>LeetCode——</code>删掉一个元素以后全为1的最长子数组（考验变长滑窗维护条件的选择）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9sb25nZXN0LXN1YmFycmF5LW9mLTFzLWFmdGVyLWRlbGV0aW5nLW9uZS1lbGVtZW50L2Rlc2NyaXB0aW9uLw==\">https://leetcode.cn/problems/longest-subarray-of-1s-after-deleting-one-element/description/</span> :fire:</p>\n<h1 id=\"二分算法-blue\"><a href=\"#二分算法-blue\" class=\"headerlink\" title=\"[二分算法]{.blue}\"></a>[二分算法]{.blue}</h1><p>[二分查找]{.rainbow}：</p>\n<p><code>LeetCode——</code>搜索插入位置（闭区间二分查找）<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9zZWFyY2gtaW5zZXJ0LXBvc2l0aW9uL2Rlc2NyaXB0aW9uLw==\">35. 搜索插入位置 - 力扣（LeetCode）</span></p>\n<p><code>LeetCode——</code>二分查找：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9iaW5hcnktc2VhcmNoL2Rlc2NyaXB0aW9uLw==\">704. 二分查找 - 力扣（LeetCode）</span></p>\n<p><code>LeetCode——</code>寻找比目标字母大的最小字母（左闭右开区间二分查找）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9maW5kLXNtYWxsZXN0LWxldHRlci1ncmVhdGVyLXRoYW4tdGFyZ2V0L2Rlc2NyaXB0aW9uLw==\">744. 寻找比目标字母大的最小字母 - 力扣（LeetCode）</span></p>\n<p><code>LeetCode——</code>正整数和负整数的最大计数（重用二分查找）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9tYXhpbXVtLWNvdW50LW9mLXBvc2l0aXZlLWludGVnZXItYW5kLW5lZ2F0aXZlLWludGVnZXIvZGVzY3JpcHRpb24v\">2529. 正整数和负整数的最大计数 - 力扣（LeetCode）</span></p>\n<p><code>LeetCode——</code>两个数组间的距离值（二分查找）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9maW5kLXRoZS1kaXN0YW5jZS12YWx1ZS1iZXR3ZWVuLXR3by1hcnJheXMvZGVzY3JpcHRpb24v\">1385. 两个数组间的距离值 - 力扣（LeetCode）</span> :fire:</p>\n<p><code>LeetCode——</code>在排序数组中查找元素的第一个和最后一个位置（二分查找入门题）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9maW5kLWZpcnN0LWFuZC1sYXN0LXBvc2l0aW9uLW9mLWVsZW1lbnQtaW4tc29ydGVkLWFycmF5L2Rlc2NyaXB0aW9uLw==\">34. 在排序数组中查找元素的第一个和最后一个位置 - 力扣（LeetCode）</span></p>\n<p>+++info Summary<br>（学习自灵茶山艾府）</p>\n<ul>\n<li>闭区间二分查找模板：<pre><code class=\"lang-python\">class Solution:\n  # lower_bound 返回最小的满足 nums[i] &gt;= target 的下标 i\n  # 如果数组为空，或者所有数都 &lt; target，则返回 len(nums)\n  # 要求 nums 是非递减的，即 nums[i] &lt;= nums[i + 1]\n  def lower_bound(self, nums: List[int], target: int) -&gt; int:\n      left, right = 0, len(nums) - 1  # 闭区间 [left, right]\n      while left &lt;= right:  # 区间不为空\n          # 循环不变量：\n          # nums[left-1] &lt; target\n          # nums[right+1] &gt;= target\n          mid = (left + right) // 2\n          if nums[mid] &gt;= target:\n              right = mid - 1  # 范围缩小到 [left, mid-1]\n          else:\n              left = mid + 1  # 范围缩小到 [mid+1, right]\n      # 循环结束后 left = right+1\n      # 此时 nums[left-1] &lt; target 而 nums[left] = nums[right+1] &gt;= target\n      # 所以 left 就是第一个 &gt;= target 的元素下标\n      return left\n</code></pre>\n</li>\n<li>开区间二分查找模板：<pre><code class=\"lang-python\">class Solution:\n  # lower_bound 返回最小的满足 nums[i] &gt;= target 的下标 i\n  # 如果数组为空，或者所有数都 &lt; target，则返回 len(nums)\n  # 要求 nums 是非递减的，即 nums[i] &lt;= nums[i + 1]\n  def lower_bound(self, nums: List[int], target: int) -&gt; int:\n      left, right = -1, len(nums)  # 开区间 (left, right)\n      while left + 1 &lt; right:  # 区间不为空\n          mid = (left + right) // 2\n          # 循环不变量：\n          # nums[left] &lt; target\n          # nums[right] &gt;= target\n          if nums[mid] &gt;= target:\n              right = mid  # 范围缩小到 (left, mid)\n          else:\n              left = mid  # 范围缩小到 (mid, right)\n      # 循环结束后 left+1 = right\n      # 此时 nums[left] &lt; target 而 nums[right] &gt;= target\n      # 所以 right 就是第一个 &gt;= target 的元素下标\n      return right\n</code></pre>\n+++</li>\n</ul>\n<p><code>LeetCode——</code>咒语和药水的成功对数（二分查找）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9zdWNjZXNzZnVsLXBhaXJzLW9mLXNwZWxscy1hbmQtcG90aW9ucy9kZXNjcmlwdGlvbi8=\">2300. 咒语和药水的成功对数 - 力扣（LeetCode）</span></p>\n<p>[二分答案]{.rainbow}：</p>\n<p><code>LeetCode——</code>使结果不超过阈值的最小除数（二分答案求最小入门题）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9maW5kLXRoZS1zbWFsbGVzdC1kaXZpc29yLWdpdmVuLWEtdGhyZXNob2xkLw==\">https://leetcode.cn/problems/find-the-smallest-divisor-given-a-threshold/</span> :fire:</p>\n<p>+++info Summary<br>（学习自灵茶山艾府）</p>\n<ul>\n<li>当a和b都是正整数时，向上取整可以转换为向下取整，公式如下：<script type=\"math/tex; mode=display\">\\lceil \\frac{a}{b} \\rceil = \\lfloor \\frac{a+b-1}{b} \\rfloor  = \\lfloor \\frac{a-1}{b} \\rfloor + 1</script></li>\n<li>只要某个数满足的表达式是单调的，我们就能对这个数进行二分查找。</li>\n<li>因为python的ceil向上取整函数计算出的是浮点数，会有精度误差，因此尽可能转换为向下取整函数floor。<br>+++</li>\n</ul>\n<p><code>LeetCode——</code>完成旅途的最少时间（二分答案）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9taW5pbXVtLXRpbWUtdG8tY29tcGxldGUtdHJpcHMv\">https://leetcode.cn/problems/minimum-time-to-complete-trips/</span> :fire:</p>\n<h1 id=\"栈-blue\"><a href=\"#栈-blue\" class=\"headerlink\" title=\"[栈]{.blue}\"></a>[栈]{.blue}</h1><p>[基础]{.rainbow}：</p>\n<p><code>LeetCode——</code>比较含退格的字符串（模拟栈）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9iYWNrc3BhY2Utc3RyaW5nLWNvbXBhcmUvZGVzY3JpcHRpb24v\">844. 比较含退格的字符串 - 力扣（LeetCode）</span></p>\n<p><code>LeetCode——</code>棒球比赛（模拟栈）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9iYXNlYmFsbC1nYW1lL3NvbHV0aW9ucy8=\">682. 棒球比赛 - 力扣（LeetCode）</span></p>\n<p><code>LeetCode——</code>用栈操作构建数组（无脑题）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9idWlsZC1hbi1hcnJheS13aXRoLXN0YWNrLW9wZXJhdGlvbnMvZGVzY3JpcHRpb24v\">1441. 用栈操作构建数组 - 力扣（LeetCode）</span></p>\n<p><code>LeetCode——</code>从字符串中移除星号（秒杀题）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9yZW1vdmluZy1zdGFycy1mcm9tLWEtc3RyaW5nLw==\">https://leetcode.cn/problems/removing-stars-from-a-string/</span> </p>\n<p><code>LeetCode——</code>设计浏览器历史记录（用于检验入门的题）（指针与栈结合）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9kZXNpZ24tYnJvd3Nlci1oaXN0b3J5Lw==\">1472. 设计浏览器历史记录 - 力扣（LeetCode）</span> </p>\n<p><code>LeetCode——</code>验证栈序列（双指针与栈结合）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy92YWxpZGF0ZS1zdGFjay1zZXF1ZW5jZXMvZGVzY3JpcHRpb24v\">946. 验证栈序列 - 力扣（LeetCode）</span> </p>\n<p><code>LeetCode——</code>有效的括号：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy92YWxpZC1wYXJlbnRoZXNlcy9kZXNjcmlwdGlvbi8/ZW52VHlwZT1zdHVkeS1wbGFuLXYyJmFtcDtlbnZJZD10b3AtMTAwLWxpa2Vk\">20. 有效的括号 - 力扣（LeetCode）</span> :fire:</p>\n<p><code>LeetCode——</code>计算字符串的镜像分数：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9maW5kLW1pcnJvci1zY29yZS1vZi1hLXN0cmluZy8=\">3412. 计算字符串的镜像分数 - 力扣（LeetCode）</span>:fire:</p>\n<p><code>LeetCode——</code>简化路径：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9zaW1wbGlmeS1wYXRoLw==\">https://leetcode.cn/problems/simplify-path/</span> :fire:</p>\n<h1 id=\"枚举-blue\"><a href=\"#枚举-blue\" class=\"headerlink\" title=\"[枚举]{.blue}\"></a>[枚举]{.blue}</h1><p>:::warning<br>同一道题目也会反复出现，比如下面的两数之和出现于前面的哈希表。<br>:::</p>\n<p><code>LeetCode——</code>两数之和（枚举右，维护左+哈希表）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy90d28tc3VtL2Rlc2NyaXB0aW9uLw==\">https://leetcode.cn/problems/two-sum/description/</span></p>\n<p>+++info Summary<br>（学习自灵茶山艾府）<br>枚举右，维护左<br>双变量问题如：$x+y==target$或者$x-y==target$。可以枚举右边的y，找是否有$x==target-y$满足。通常与哈希表结合。<br>+++</p>\n<p><code>LeetCode——</code>好数对的数目（枚举右，维护左+哈希表）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9udW1iZXItb2YtZ29vZC1wYWlycy9kZXNjcmlwdGlvbi8=\">https://leetcode.cn/problems/number-of-good-pairs/description/</span></p>\n<p><code>LeetCode——</code>可互换矩形的组数（枚举右，维护左+哈希表）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9udW1iZXItb2YtcGFpcnMtb2YtaW50ZXJjaGFuZ2VhYmxlLXJlY3RhbmdsZXMvZGVzY3JpcHRpb24v\">https://leetcode.cn/problems/number-of-pairs-of-interchangeable-rectangles/description/</span></p>\n<h1 id=\"前缀和-blue\"><a href=\"#前缀和-blue\" class=\"headerlink\" title=\"[前缀和]{.blue}\"></a>[前缀和]{.blue}</h1><p>[基础]{.rainbow}：<br><code>LeetCode——</code>区域和检索——数组不可变（前缀和入门模板题）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9yYW5nZS1zdW0tcXVlcnktaW1tdXRhYmxlLw==\">https://leetcode.cn/problems/range-sum-query-immutable/</span></p>\n<p>+++info Summary<br>前缀和我在一篇博文有详细介绍原理，于是不再总结。<br>这里注意到Python有一个accumulate函数，这个函数可以计算一个数组对应的前缀和数组。有一个initial参数，将其设为0，相当于在前缀和数组前增加一个0项：</p>\n<pre><code class=\"lang-python\">from itertools import accumulate\n\n# 创建一个列表\ndata = [1, 2, 3, 4, 5]\n\n# 使用accumulate计算前缀和\nresult1 = list(accumulate(data))\nresult2 = list(accumulate(data, initial=0))\n\n# 输出结果\nprint(result1) # 输出：[1, 3, 6, 10, 15]\nprint(result2) # 输出：[0, 1, 3, 6, 10, 15]\n</code></pre>\n<p>+++</p>\n<p><code>LeetCode——</code>变长子数组求和：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9zdW0tb2YtdmFyaWFibGUtbGVuZ3RoLXN1YmFycmF5cy8=\">https://leetcode.cn/problems/sum-of-variable-length-subarrays/</span></p>\n<p><code>LeetCode——</code>统计范围内的元音字符串数：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9jb3VudC12b3dlbC1zdHJpbmdzLWluLXJhbmdlcy8=\">https://leetcode.cn/problems/count-vowel-strings-in-ranges/</span> </p>\n<p><code>LeetCode——</code>特殊数组II（数组奇偶性与前缀和）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9zcGVjaWFsLWFycmF5LWlpLw==\">https://leetcode.cn/problems/special-array-ii/</span> :fire:</p>\n<p>[前缀和与哈希表]{.rainbow}：<br>+++info Summary<br>前缀和两次遍历示例写法：</p>\n<pre><code class=\"lang-python\">class Solution:\n    def subarraySum(self, nums: List[int], k: int) -&gt; int:\n        s = [0] * (len(nums) + 1)\n        for i, x in enumerate(nums):\n            s[i + 1] = s[i] + x\n\n        ans = 0\n        cnt = defaultdict(int)\n        for sj in s:\n            ans += cnt[sj - k]\n            cnt[sj] += 1\n        return ans\n</code></pre>\n<p>一遍遍历示例写法（由于遍历nums会从s[1]开始计算，所以要单独处理 s[0]=0）：</p>\n<pre><code class=\"lang-python\">class Solution:\n    def subarraySum(self, nums: List[int], k: int) -&gt; int:\n        ans = s = 0\n        cnt = defaultdict(int)\n        cnt[0] = 1  # s[0]=0 单独统计\n        for x in nums:\n            s += x\n            ans += cnt[s - k]\n            cnt[s] += 1\n        return ans\n</code></pre>\n<p>+++</p>\n<p><code>LeetCode——</code>和为K的子数组（启蒙题，值得反复品味）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9zdWJhcnJheS1zdW0tZXF1YWxzLWsvZGVzY3JpcHRpb24v\">https://leetcode.cn/problems/subarray-sum-equals-k/description/</span> </p>\n<p><code>LeetCode——</code>和相同的二元子数组：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9iaW5hcnktc3ViYXJyYXlzLXdpdGgtc3VtL2Rlc2NyaXB0aW9uLw==\">https://leetcode.cn/problems/binary-subarrays-with-sum/description/</span></p>\n<p><code>LeetCode——</code>和为奇数的子数组数目（区间奇偶与前缀和）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9udW1iZXItb2Ytc3ViLWFycmF5cy13aXRoLW9kZC1zdW0v\">https://leetcode.cn/problems/number-of-sub-arrays-with-odd-sum/</span> :fire:</p>\n<p>+++info Summary<br>前缀和奇偶性与数组区间和的关系：<br>如果两个前缀和的奇偶性不同（一个是偶数，一个是奇数），它们之间的子数组和一定是奇数。<br>+++</p>\n<p><code>LeetCode——</code>和可被K整除的子数组（余数哈希+前缀和）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9zdWJhcnJheS1zdW1zLWRpdmlzaWJsZS1ieS1rLw==\">https://leetcode.cn/problems/subarray-sums-divisible-by-k/</span></p>\n<p><code>LeetCode——</code>连续的子数组和（在前一道题基础上加入区间要求）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9jb250aW51b3VzLXN1YmFycmF5LXN1bS9kZXNjcmlwdGlvbi8=\">https://leetcode.cn/problems/continuous-subarray-sum/description/</span></p>\n<p><code>LeetCode——</code>连续数组（将不定和转为定和）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9jb250aWd1b3VzLWFycmF5L2Rlc2NyaXB0aW9uLw==\">https://leetcode.cn/problems/contiguous-array/description/</span> :fire:</p>\n<p>+++info Summary<br>本题可以用纯数学的方式避免用if-else判断数组值：<br>令前缀和为prev，nums[i]是数组值，如果要实现遇到nums[i]为1则prev+=1，而遇到nums[i]为-1则prev-=1，用：</p>\n<pre><code class=\"lang-python\">prev += nums[i] * 2 - 1\n</code></pre>\n<p>替代：</p>\n<pre><code class=\"lang-python\">if nums[i] == 1:\n    prev += 1\nelse:\n    prev -= 1\n</code></pre>\n<p>为什么会得到这个公式？我们重新思考一下，其实无外乎可以抽象为：prev += f(nums[i])，这样就能避免用if-else。那么这个函数f如何得到呢？这时候就要考虑nums[i]的值和目标f(nums[i])的值。发现nums[i]和f(nums[i])都可以分别抽象为一个向量，然后就变成了：</p>\n<script type=\"math/tex; mode=display\">\nf(\n\\left[\n\\begin{matrix}\n0    \\\\\n1    \\\\\n\\end{matrix}\n\\right]) = \n\\left[\n\\begin{matrix}\n-1    \\\\\n1    \\\\\n\\end{matrix}\n\\right]</script><p>一般f是一个线性变换，可以认为<script type=\"math/tex\">f=wx+b</script>，根据线性代数的知识，f也是一个向量，那么就有：</p>\n<script type=\"math/tex; mode=display\">\n\\left[\n\\begin{matrix}\n0    \\\\\n1    \\\\\n\\end{matrix}\n\\right] \\left[\n\\begin{matrix}\na    \\\\\nb    \\\\\n\\end{matrix}\n\\right] = \n\\left[\n\\begin{matrix}\n-1    \\\\\n1    \\\\\n\\end{matrix}\n\\right]</script><p>利用矩阵乘法，能够知道a=2，b=-1，即f=2x-1。<br>+++</p>\n<p><code>LeetCode——</code>统计美丽子数组数目：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9jb3VudC10aGUtbnVtYmVyLW9mLWJlYXV0aWZ1bC1zdWJhcnJheXMvZGVzY3JpcHRpb24v\">https://leetcode.cn/problems/count-the-number-of-beautiful-subarrays/description/</span> :fire:</p>\n<p>[距离和]{.rainbow}</p>\n<p><code>LeetCode——</code>有序数组中差绝对值之和：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9zdW0tb2YtYWJzb2x1dGUtZGlmZmVyZW5jZXMtaW4tYS1zb3J0ZWQtYXJyYXkv\">https://leetcode.cn/problems/sum-of-absolute-differences-in-a-sorted-array/</span> :fire:</p>\n<p>[前缀异或和]{.rainbow}</p>\n<p><code>LeetCode——</code>构建回文串检测：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9jYW4tbWFrZS1wYWxpbmRyb21lLWZyb20tc3Vic3RyaW5nLw==\">https://leetcode.cn/problems/can-make-palindrome-from-substring/</span> :fire:</p>\n<h1 id=\"网格图-blue\"><a href=\"#网格图-blue\" class=\"headerlink\" title=\"[网格图]{.blue}\"></a>[网格图]{.blue}</h1><p>[深度优先搜索DFS]{.rainbow}：<br><code>LeetCode——</code>岛屿数量（入门题，水平垂直方向）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9udW1iZXItb2YtaXNsYW5kcy9kZXNjcmlwdGlvbi8=\">https://leetcode.cn/problems/number-of-islands/description/</span></p>\n<p><code>LeetCode——</code>岛屿的最大面积：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9tYXgtYXJlYS1vZi1pc2xhbmQvZGVzY3JpcHRpb24v\">https://leetcode.cn/problems/max-area-of-island/description/</span></p>\n<p><code>LeetCode——</code>水域大小（加入对角线方向）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9wb25kLXNpemVzLWxjY2kvZGVzY3JpcHRpb24v\">https://leetcode.cn/problems/pond-sizes-lcci/description/</span></p>\n<p><code>LeetCode——</code>主题空间（边界不合法情况）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9ZZXNkUHcvZGVzY3JpcHRpb24v\">https://leetcode.cn/problems/YesdPw/description/</span> :fire:</p>\n<p><code>LeetCode——</code>岛屿的周长：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9pc2xhbmQtcGVyaW1ldGVyL2Rlc2NyaXB0aW9uLw==\">https://leetcode.cn/problems/island-perimeter/description/</span> </p>\n<h1 id=\"二叉树-blue\"><a href=\"#二叉树-blue\" class=\"headerlink\" title=\"[二叉树]{.blue}\"></a>[二叉树]{.blue}</h1><p>[遍历二叉树]{.rainbow}：<br><code>LeetCode——</code>二叉树的前序遍历：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9iaW5hcnktdHJlZS1wcmVvcmRlci10cmF2ZXJzYWwv\">https://leetcode.cn/problems/binary-tree-preorder-traversal/</span> </p>\n<p>+++info Summary<br>二叉树遍历的递归方式：</p>\n<pre><code class=\"lang-python\"># 假设二叉树类定义如：\n# class TreeNode:\n#     def __init__(self, val=0, left=None, right=None):\n#         self.val = val\n#         self.left = left\n#         self.right = right\n\n# 前序遍历\ndef preorderTraversal(self, root: Optional[TreeNode]) -&gt; List[int]:\n    ans = []\n    def dfs(root):\n        nonlocal ans\n        if not root:\n            return\n        ans.append(root.val)\n        dfs(root.left)\n        dfs(root.right)\n    dfs(root)\n    return ans\n\n# 中序遍历\ndef preorderTraversal(self, root: Optional[TreeNode]) -&gt; List[int]:\n    ans = []\n    def dfs(root):\n        nonlocal ans\n        if not root:\n            return\n        dfs(root.left)\n        ans.append(root.val)    # 只变化了这行代码的位置\n        dfs(root.right)\n    dfs(root)\n    return ans\n\n# 后序遍历\ndef preorderTraversal(self, root: Optional[TreeNode]) -&gt; List[int]:\n    ans = []\n    def dfs(root):\n        nonlocal ans\n        if not root:\n            return\n        dfs(root.left)\n        dfs(root.right)\n        ans.append(root.val)    # 只变化了这行代码的位置\n    dfs(root)\n    return ans\n</code></pre>\n<p>下面看借用栈实现迭代方式的二叉树前序遍历，学习自腐烂的橘子：<br><img data-src=\"../../assets/bindtree1.gif\" alt=\"image\"><br>整体思路是：</p>\n<ul>\n<li>初始化栈，将根节点入栈</li>\n<li>当栈不为空时，弹出栈顶元素</li>\n<li>如果栈顶元素右子树不为空，那么将右子树压入栈中</li>\n<li>如果栈顶元素左子树不为空，那么将左子树压入栈中<br>前序遍历的迭代方式代码为：<pre><code class=\"lang-python\">def preorderTraversal(self, root: Optional[TreeNode]) -&gt; List[int]:\n  if not root:\n      return []\n  stack, ans = [root], []\n  while stack:\n      node = stack.pop()\n      if node:\n          ans.append(node.val)\n          if node.right:\n              stack.append(node.right)\n          if node.left:\n              stack.append(node.left)\n  return ans\n</code></pre>\n为什么弹出栈顶节点后，先将右子树压入栈中？因为这是前序遍历，而且栈是先入后出，如果我们想要做到遍历“根-&gt;左子树-&gt;右子树”，那么就要先把右子树压入栈中，再压入左子树。这样后入的左子树就能先被弹出遍历。</li>\n</ul>\n<p>这是一种方便理解的思路，当然还有一种模板式的迭代前序遍历思路，也学习自腐烂的橘子：</p>\n<ul>\n<li>先将根节点cur和所有的左子树压入栈并加入结果中，直至cur为空。</li>\n<li>然后每弹出一个栈顶元素tmp，就到达它的右子树，再将这个节点当作cur，继续按照前面的步骤再来一遍，直至栈为空。</li>\n<li>代码如下：</li>\n</ul>\n<p><img data-src=\"../../assets/bindtree2.jpg\" alt=\"image\"></p>\n<pre><code class=\"lang-python\">def preorderTraversal(self, root: Optional[TreeNode]) -&gt; List[int]:\n    if not root:\n        return []\n    cur, stack, ans = root, [], []\n    while cur or stack:\n        while cur:\n            ans.append(cur.val)\n            stack.append(cur)\n            cur = cur.left\n        tmp = stack.pop()\n        cur = tmp.right\n    return ans\n</code></pre>\n<p>这样，首先确保根节点和左子树首先进入栈中，此时遍历到树底部，然后再一一从栈中取出节点，并到达右子树继续遍历。<br>+++</p>\n<p><code>LeetCode——</code>二叉树的中序遍历：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9iaW5hcnktdHJlZS1pbm9yZGVyLXRyYXZlcnNhbC9kZXNjcmlwdGlvbi8=\">https://leetcode.cn/problems/binary-tree-inorder-traversal/description/</span></p>\n<p>+++info Summary<br>二叉树的迭代版本中序遍历：</p>\n<pre><code class=\"lang-python\">def inorderTraversal(self, root: Optional[TreeNode]) -&gt; List[int]:\n    if not root:\n        return []\n    cur, stack, ans = root, [], []\n    while cur or stack:\n        while cur:\n            stack.append(cur)\n            cur = cur.left\n        tmp = stack.pop()\n        ans.append(tmp.val)    # 同样只移动了这行代码\n        cur = tmp.right\n    return ans\n</code></pre>\n<p>当内层while循环结束时，说明遍历到了二叉树最底层。<br>+++</p>\n<p><code>LeetCode——</code>二叉树的后序遍历：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9iaW5hcnktdHJlZS1wb3N0b3JkZXItdHJhdmVyc2FsL2Rlc2NyaXB0aW9uLw==\">https://leetcode.cn/problems/binary-tree-postorder-traversal/description/</span></p>\n<p>+++info Summary<br>二叉树的“后序遍历”，本质上是二叉树“从右侧开始的前序遍历”。<br>你可以尝试对一个二叉树进行前序遍历，然后反向所得到的列表结果，就是二叉树的后序遍历。按照这个思路，则有该代码：</p>\n<pre><code class=\"lang-python\">def postorderTraversal(self, root: Optional[TreeNode]) -&gt; List[int]:\n    if not root:\n        return []\n    cur, stack, ans = root, [], []\n    while cur or stack:\n        while cur:\n            ans.append(cur.val)\n            stack.append(cur)\n            cur = cur.right\n        tmp = stack.pop()\n        cur = tmp.left\n    return ans[::-1]\n</code></pre>\n<p>+++</p>\n<p><code>LeetCode——</code>叶子相似的树：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9sZWFmLXNpbWlsYXItdHJlZXMvZGVzY3JpcHRpb24v\">https://leetcode.cn/problems/leaf-similar-trees/description/</span> :fire:</p>\n<p>[自顶向下DFS]{.rainbow}：</p>\n<p>+++info Summary<br>自顶向下DFS通常是在[递]的过程中维护一个值。<br>+++</p>\n<p><code>LeetCode——</code>二叉树的最大深度（入门题）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9tYXhpbXVtLWRlcHRoLW9mLWJpbmFyeS10cmVlL2Rlc2NyaXB0aW9uLw==\">https://leetcode.cn/problems/maximum-depth-of-binary-tree/description/</span> </p>\n<p><code>LeetCode——</code>二叉树的最小深度（入门题）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9taW5pbXVtLWRlcHRoLW9mLWJpbmFyeS10cmVlL2Rlc2NyaXB0aW9uLw==\">https://leetcode.cn/problems/minimum-depth-of-binary-tree/description/</span> </p>\n<p><code>LeetCode——</code>路径总和：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9wYXRoLXN1bS9kZXNjcmlwdGlvbi8=\">https://leetcode.cn/problems/path-sum/description/</span> </p>\n<p><code>LeetCode——</code>求根节点到叶节点数字之和（向下传递累积值）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9zdW0tcm9vdC10by1sZWFmLW51bWJlcnMvZGVzY3JpcHRpb24v\">https://leetcode.cn/problems/sum-root-to-leaf-numbers/description/</span> </p>\n<p><code>LeetCode——</code>二叉树的右视图（灵活运用DFS）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9iaW5hcnktdHJlZS1yaWdodC1zaWRlLXZpZXcv\">https://leetcode.cn/problems/binary-tree-right-side-view/</span> </p>\n<p><code>LeetCode——</code>统计二叉树中好节点的数目（向下传递最大值）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9jb3VudC1nb29kLW5vZGVzLWluLWJpbmFyeS10cmVlLw==\">https://leetcode.cn/problems/count-good-nodes-in-binary-tree/</span></p>\n<p><code>LeetCode——</code>二叉树中的伪回文路径：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9wc2V1ZG8tcGFsaW5kcm9taWMtcGF0aHMtaW4tYS1iaW5hcnktdHJlZS8=\">https://leetcode.cn/problems/pseudo-palindromic-paths-in-a-binary-tree/</span> :fire:</p>\n<p>[自底向上DFS]{.rainbow}：</p>\n<p>+++info Summary<br>在[归]的过程中进行计算。<br>入门自底向上DFS很容易，但是掌握好还是稍有难度的。<br>+++</p>\n<p><code>LeetCode——</code>二叉树的最大深度：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9tYXhpbXVtLWRlcHRoLW9mLWJpbmFyeS10cmVlLw==\">https://leetcode.cn/problems/maximum-depth-of-binary-tree/</span> </p>\n<p><code>LeetCode——</code>二叉树的最小深度：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9taW5pbXVtLWRlcHRoLW9mLWJpbmFyeS10cmVlLw==\">https://leetcode.cn/problems/minimum-depth-of-binary-tree/</span> </p>\n<p><code>LeetCode——</code>单值二叉树（掌握自底向上DFS的条件判断）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy91bml2YWx1ZWQtYmluYXJ5LXRyZWUv\">https://leetcode.cn/problems/univalued-binary-tree/</span> :fire:</p>\n<p><code>LeetCode——</code>相同的树（先讨论根节点再讨论左右子树）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9zYW1lLXRyZWUv\">https://leetcode.cn/problems/same-tree/</span></p>\n<p><code>LeetCode——</code>对称二叉树：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9zeW1tZXRyaWMtdHJlZS8=\">https://leetcode.cn/problems/symmetric-tree/</span> </p>\n<p><code>LeetCode——</code>翻转等价二叉树：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9mbGlwLWVxdWl2YWxlbnQtYmluYXJ5LXRyZWVzLw==\">https://leetcode.cn/problems/flip-equivalent-binary-trees/</span> :fire:</p>\n<p><code>LeetCode——</code>找出克隆二叉树中的相同节点（在二叉树中递归寻找目标值）： <span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9maW5kLWEtY29ycmVzcG9uZGluZy1ub2RlLW9mLWEtYmluYXJ5LXRyZWUtaW4tYS1jbG9uZS1vZi10aGF0LXRyZWUv\">https://leetcode.cn/problems/find-a-corresponding-node-of-a-binary-tree-in-a-clone-of-that-tree/</span> </p>\n<p><code>LeetCode——</code>平衡二叉树（利用非法值快速回到递归入口）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9iYWxhbmNlZC1iaW5hcnktdHJlZS8=\">https://leetcode.cn/problems/balanced-binary-tree/</span> </p>\n<p><code>LeetCode——</code>翻转二叉树：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9pbnZlcnQtYmluYXJ5LXRyZWUv\">https://leetcode.cn/problems/invert-binary-tree/</span> :fire:</p>\n<p><code>LeetCode——</code>合并二叉树：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9tZXJnZS10d28tYmluYXJ5LXRyZWVzLw==\">https://leetcode.cn/problems/merge-two-binary-trees/</span> :fire:</p>\n<p><strong>自底向上 DFS：删点</strong>：</p>\n<p><code>LeetCode——</code>二叉树剪枝：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9iaW5hcnktdHJlZS1wcnVuaW5nLw==\">https://leetcode.cn/problems/binary-tree-pruning/</span> :fire:</p>\n<p><code>LeetCode——</code>删除给定值的叶子节点：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9kZWxldGUtbGVhdmVzLXdpdGgtYS1naXZlbi12YWx1ZS8=\">https://leetcode.cn/problems/delete-leaves-with-a-given-value/</span> :fire:</p>\n<p><code>LeetCode——</code>删点成林：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9kZWxldGUtbm9kZXMtYW5kLXJldHVybi1mb3Jlc3Qv\">https://leetcode.cn/problems/delete-nodes-and-return-forest/</span> :fire:</p>\n<p><strong>二叉搜索树</strong>：<br><code>LeetCode——</code>二叉搜索树中的搜索：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9zZWFyY2gtaW4tYS1iaW5hcnktc2VhcmNoLXRyZWUv\">https://leetcode.cn/problems/search-in-a-binary-search-tree/</span> :fire:</p>\n<p><strong>回溯</strong>：<br><code>LeetCode——</code>二叉树的所有路径：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9iaW5hcnktdHJlZS1wYXRocy9kZXNjcmlwdGlvbi8=\">https://leetcode.cn/problems/binary-tree-paths/description/</span> :fire:</p>\n<h1 id=\"贪心-blue\"><a href=\"#贪心-blue\" class=\"headerlink\" title=\"[贪心]{.blue}\"></a>[贪心]{.blue}</h1><p>[从最大/最小开始贪心]{.rainbow}：</p>\n<p><code>LeetCode——</code>重新分装苹果（排序+贪心，无脑题）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9hcHBsZS1yZWRpc3RyaWJ1dGlvbi1pbnRvLWJveGVzL2Rlc2NyaXB0aW9uLw==\">https://leetcode.cn/problems/apple-redistribution-into-boxes/description/</span> </p>\n<p><code>LeetCode——</code>装满石头的背包的最大数量（排序贪心，同无脑题）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9tYXhpbXVtLWJhZ3Mtd2l0aC1mdWxsLWNhcGFjaXR5LW9mLXJvY2tzLw==\">https://leetcode.cn/problems/maximum-bags-with-full-capacity-of-rocks/</span> </p>\n<p><code>LeetCode——</code>雪糕的最大数量（继续感受无脑贪心）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9tYXhpbXVtLWljZS1jcmVhbS1iYXJzLw==\">https://leetcode.cn/problems/maximum-ice-cream-bars/</span></p>\n<p><code>LeetCode——</code>K次取反后最大化的数组和（检验排序贪心的真正入门题）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9tYXhpbWl6ZS1zdW0tb2YtYXJyYXktYWZ0ZXItay1uZWdhdGlvbnMv\">https://leetcode.cn/problems/maximize-sum-of-array-after-k-negations/</span> :fire:</p>\n<p><code>LeetCode——</code>不同整数的最少数目（哈希+排序贪心）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9sZWFzdC1udW1iZXItb2YtdW5pcXVlLWludGVnZXJzLWFmdGVyLWstcmVtb3ZhbHMv\">https://leetcode.cn/problems/least-number-of-unique-integers-after-k-removals/</span> </p>\n<p>+++info Summary<br>小技巧——用collections.Counter统计一个序列或可迭代对象每个迭代对象的次数，基本用法：</p>\n<pre><code class=\"lang-python\">from collections import Counter\n\n# 使用字符串初始化Counter\ncounter = Counter(&quot;mississippi&quot;)\nprint(counter) # 输出: Counter(&#123;&#39;i&#39;: 4, &#39;s&#39;: 4, &#39;p&#39;: 2, &#39;m&#39;: 1&#125;)\n\n# 使用列表初始化Counter\ncounter = Counter(list(&quot;mississippi&quot;))\nprint(counter) # 输出: Counter(&#123;&#39;i&#39;: 4, &#39;s&#39;: 4, &#39;p&#39;: 2, &#39;m&#39;: 1&#125;)\n</code></pre>\n<p>+++</p>\n<p><code>LeetCode——</code>非递增顺序的最小子序列（依然是排序贪心）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9taW5pbXVtLXN1YnNlcXVlbmNlLWluLW5vbi1pbmNyZWFzaW5nLW9yZGVyLw==\">https://leetcode.cn/problems/minimum-subsequence-in-non-increasing-order/</span> </p>\n<p><code>LeetCode——</code>将数组分成最小总代价的子数组I（纯排序题）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9kaXZpZGUtYW4tYXJyYXktaW50by1zdWJhcnJheXMtd2l0aC1taW5pbXVtLWNvc3QtaS8=\">https://leetcode.cn/problems/divide-an-array-into-subarrays-with-minimum-cost-i/</span></p>\n<p><code>LeetCode——</code>数组大小减半：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9yZWR1Y2UtYXJyYXktc2l6ZS10by10aGUtaGFsZi8=\">https://leetcode.cn/problems/reduce-array-size-to-the-half/</span> :fire:</p>\n<p><code>LeetCode——</code>卡车上的最大单元数：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9tYXhpbXVtLXVuaXRzLW9uLWEtdHJ1Y2sv\">https://leetcode.cn/problems/maximum-units-on-a-truck/</span> :fire:</p>\n<p><code>LeetCode——</code>幸福值最大化的选择方案：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9tYXhpbWl6ZS1oYXBwaW5lc3Mtb2Ytc2VsZWN0ZWQtY2hpbGRyZW4v\">https://leetcode.cn/problems/maximize-happiness-of-selected-children/</span> :fire:</p>\n<p>[单序列配对]{.rainbow}</p>\n<p><code>LeetCode——</code>打折购买糖果的最小开销：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9taW5pbXVtLWNvc3Qtb2YtYnV5aW5nLWNhbmRpZXMtd2l0aC1kaXNjb3VudC8=\">https://leetcode.cn/problems/minimum-cost-of-buying-candies-with-discount/</span> :fire:</p>\n<p><code>LeetCode——</code>数组拆分：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9hcnJheS1wYXJ0aXRpb24v\">https://leetcode.cn/problems/array-partition/</span> :fire:</p>\n<p><code>LeetCode——</code>数组中最大数对和的最小值：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9taW5pbWl6ZS1tYXhpbXVtLXBhaXItc3VtLWluLWFycmF5Lw==\">https://leetcode.cn/problems/minimize-maximum-pair-sum-in-array/</span> :fire:</p>\n",
            "tags": [
                "计算机"
            ]
        },
        {
            "id": "https://yunhdan.github.io/cs/Javascript%E7%9F%A5%E8%AF%86%E6%A0%91/",
            "url": "https://yunhdan.github.io/cs/Javascript%E7%9F%A5%E8%AF%86%E6%A0%91/",
            "title": "Javascript知识树",
            "date_published": "2025-02-14T03:39:52.000Z",
            "content_html": "<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h1 id=\"基础\"><a href=\"#基础\" class=\"headerlink\" title=\"基础\"></a>基础</h1><h4 id=\"作用域：菜鸟教程\"><a href=\"#作用域：菜鸟教程\" class=\"headerlink\" title=\"作用域：菜鸟教程\"></a>作用域：<span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cucnVub29iLmNvbS9qcy9qcy1zY29wZS5odG1s\">菜鸟教程</span></h4><p>涵盖知识：<strong>局部作用域</strong>、<strong>全局变量</strong>、<strong>变量生命周期</strong></p>\n<blockquote>\n<p>需要特别关注：</p>\n<ul>\n<li>如果变量在函数内部没有声明（没有使用<code>var</code>关键字），则该变量为全局变量。</li>\n<li>所有全局变量属于<code>Window</code>对象。</li>\n</ul>\n</blockquote>\n<h4 id=\"函数：菜鸟教程\"><a href=\"#函数：菜鸟教程\" class=\"headerlink\" title=\"函数：菜鸟教程\"></a>函数：<span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cucnVub29iLmNvbS9qcy9qcy1mdW5jdGlvbnMuaHRtbA==\">菜鸟教程</span></h4><p>涵盖知识：<strong>函数的语法</strong>、<strong>参数传递</strong>、<strong>返回值</strong>、</p>\n<blockquote>\n<p>需要特别关注：</p>\n<ul>\n<li><code>return</code>可接返回值，返回值可有可无。</li>\n<li>在函数内部声明的变量（使用<code>var</code>）是局部变量。函数运行完毕，局部变量就会被删除。</li>\n</ul>\n</blockquote>\n<h4 id=\"数据类型：菜鸟教程\"><a href=\"#数据类型：菜鸟教程\" class=\"headerlink\" title=\"数据类型：菜鸟教程\"></a>数据类型：<span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cucnVub29iLmNvbS9qcy9qcy1kYXRhdHlwZXMuaHRtbA==\">菜鸟教程</span></h4><p>涵盖知识：<strong>数据类型的分类</strong>、<strong>动态类型的概念</strong>、<strong>字符串类型</strong>、<strong>数字类型</strong>、<strong>布尔类型</strong>、<strong>数组类型</strong>、<strong>对象类型</strong>、<strong><code>undefined</code>与<code>null</code></strong>、<strong><code>new</code>关键字声明变量类型</strong>。</p>\n<blockquote>\n<p>需要特别关注：</p>\n<ul>\n<li>可以用<code>typeof</code>查看变量的数据类型。</li>\n<li><code>Symbol</code>数据类型表示独一无二的值。</li>\n<li>对象的声明可以跨越多行，空格和折行无关紧要。</li>\n<li>对象的属性有两种寻址方式（获取方式）。</li>\n</ul>\n</blockquote>\n<h4 id=\"变量：菜鸟教程\"><a href=\"#变量：菜鸟教程\" class=\"headerlink\" title=\"变量：菜鸟教程\"></a>变量：<span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cucnVub29iLmNvbS9qcy9qcy12YXJpYWJsZXMuaHRtbA==\">菜鸟教程</span></h4><p>涵盖知识：<strong>声明变量</strong>、<strong>一条语句声明多个变量</strong>、<strong><code>let</code>与c<code>onst</code></strong>。</p>\n<blockquote>\n<p>需要特别注意：</p>\n<ul>\n<li><code>Javascript</code>的语句和变量都对大小写敏感。</li>\n<li>变量尽量以字母开头。</li>\n<li>如果重新声明<code>Javascript</code>变量，该变量的值不会丢失。</li>\n<li>声明变量时未指定值，则默认为<code>undefined</code>。</li>\n</ul>\n</blockquote>\n<h4 id=\"注释：菜鸟教程\"><a href=\"#注释：菜鸟教程\" class=\"headerlink\" title=\"注释：菜鸟教程\"></a>注释：<span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cucnVub29iLmNvbS9qcy9qcy1jb21tZW50cy5odG1s\">菜鸟教程</span></h4><p>涵盖知识：<strong>单行注释方法</strong>、<strong>多行注释方法</strong>、<strong>行末注释方法</strong>。</p>\n<h4 id=\"语句：菜鸟教程\"><a href=\"#语句：菜鸟教程\" class=\"headerlink\" title=\"语句：菜鸟教程\"></a>语句：<span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cucnVub29iLmNvbS9qcy9qcy1zdGF0ZW1lbnRzLmh0bWw=\">菜鸟教程</span></h4><p>涵盖知识：<strong>分号作用</strong>、<strong>关键字表</strong>、<strong>对代码行进行拆行</strong>。</p>\n<blockquote>\n<p>需要特别注意的：</p>\n<ul>\n<li>用分号来结束语句是可有可无的。</li>\n<li>空格用于增加可读性，<code>Javascript</code>会自动省略多余空格。</li>\n</ul>\n</blockquote>\n<h4 id=\"输出：菜鸟教程\"><a href=\"#输出：菜鸟教程\" class=\"headerlink\" title=\"输出：菜鸟教程\"></a>输出：<span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cucnVub29iLmNvbS9qcy9qcy1vdXRwdXQuaHRtbA==\">菜鸟教程</span></h4><p>涵盖知识：<strong><code>window.alert()</code></strong>、<strong><code>getElementById()</code></strong>、<strong><code>innerHTML</code></strong>、<strong><code>write()</code></strong>、<strong><code>console.log()</code></strong>。</p>\n<h4 id=\"用法：菜鸟教程\"><a href=\"#用法：菜鸟教程\" class=\"headerlink\" title=\"用法：菜鸟教程\"></a>用法：<span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cucnVub29iLmNvbS9qcy9qcy1ob3d0by5odG1s\">菜鸟教程</span></h4><p>涵盖知识：<strong><code>&lt;script&gt;</code>标签</strong>、<strong><code>Javascript</code>的三种插入方式</strong>。</p>\n",
            "tags": [
                "计算机"
            ]
        }
    ]
}