<!DOCTYPE html><html lang="Chinese" theme-mode="dark"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>EnlightenGAN论文阅读笔记 | Alex_Fall的博客</title><link rel="icon" type="image/x-icon" href="/favicon.ico"><link rel="preload" as="font" crossorigin="anonymous" href="/font/Bender.ttf"><link rel="preload" as="font" crossorigin="anonymous" href="/font/BenderLight.ttf"><link rel="preload" as="font" crossorigin="anonymous" href="/font/JetBrainsMono-Regular.woff2"><link rel="stylesheet" href="/css/arknights.css"><style>@font-face {
  font-family: Bender;
  src: local('Bender'), url("/font/Bender.ttf"), url("/font/Bender.otf");
}
@font-face {
  font-family: BenderLight;
  src: local('BenderLight'), url("/font/BenderLight.ttf");
}
@font-face {
  font-family: 'JetBrains Mono';
  src: local('JetBrains Mono'), url('/font/JetBrainsMono-Regular.woff2') format('woff2');
}
</style><script>var config = {"root":"/","search":{"preload":false,"activeHolder":"键入以继续","blurHolder":"数据检索","noResult":"无 $0 相关数据"},"code":{"codeInfo":"$0 - $1 行","copy":"复制"}}</script><link type="text/css" rel="stylesheet" href="/lib/encrypt/hbe.style.css"><link type="text/css" rel="stylesheet" href="//unpkg.com/lightgallery@2.7.1/css/lightgallery.css"><link type="text/css" rel="stylesheet" href="//unpkg.com/lightgallery@2.7.1/css/lg-zoom.css"><link type="text/css" rel="stylesheet" href="//unpkg.com/lightgallery@2.7.1/css/lg-thumbnail.css"><link type="text/css" rel="stylesheet" href="/lib/fontawesome/css/all.min.css"><script>if (window.localStorage.getItem('theme-mode') === 'light')
 document.documentElement.setAttribute('theme-mode', 'light')
if (window.localStorage.getItem('theme-mode') === 'dark')
 document.documentElement.setAttribute('theme-mode', 'dark')</script><style>:root {
 --dark-background: url('/img/bk6.jpg');
 --light-background: url('/img/bk4.jpg');
 --theme-encrypt-confirm: '确认'
}</style><script defer src="/js/arknights.js"></script><script defer src="/js/search.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script async src="//unpkg.com/valine/dist/Valine.min.js"></script><script defer type="module">import mermaid from '//unpkg.com/mermaid@10.5.0/dist/mermaid.esm.mjs';
window.mermaid = mermaid;
code.paintMermaid();
</script><script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.6.1/MathJax.js"></script><script>MathJax.Hub.Config({
  menuSettings: {
    zoom: "None"
  },
  showMathMenu: false,
  jax: ["input/TeX","output/CommonHTML"],
  extensions: ["tex2jax.js"],
  TeX: {
    extensions: ["AMSmath.js","AMSsymbols.js"],
    equationNumbers: {
      autoNumber: "AMS"
    }
  },
  tex2jax: {
    inlineMath: [["\\(", "\\)"]],
    displayMath: [["\\[", "\\]"]]
  }
});
</script><script async src="//unpkg.com/lightgallery@2.7.1/lightgallery.min.js"></script><script async src="//unpkg.com/lightgallery@2.7.1/plugins/zoom/lg-zoom.min.js"></script><script async src="//unpkg.com/lightgallery@2.7.1/plugins/thumbnail/lg-thumbnail.min.js"></script><script async src="/lib/encrypt/hbe.js"></script><script async src="/js/pjax.js"></script><script class="pjax-js">reset= () => {new Valine({
 el: '#valine'
 , appId: 'ZJpjFGsfKxOkMtnj4IZdHWwC-gzGzoHsz'
 , appKey: 'q5c1n2TgYhpAmFimYCwY4Ssz' , placeholder: '此条评论委托企鹅物流发送'
 , path: window.location.pathname
});document.querySelector('.lg-container')?.remove()
lightGallery(document.getElementById('post-bg'), {
  plugins: [lgZoom,lgThumbnail],
  selector: '.item-img'})}</script><script>window.addEventListener("load",() => {pjax = new Pjax({
 cacheBust: false,
 selectors: ['title','article','#aside-block','.pjax-js','data-pjax','.busuanzi'],
 switches: {'article': Pjax.switches.sideBySide},
 switchesOptions: {
   'article': {
     classNames: {
       remove: "pjax-out",
       add: "pjax-in"
     }
   }
 }
});
document.addEventListener("pjax:complete", reset);document.addEventListener('pjax:success', _ => bszCaller.fetch(
 "//busuanzi.ibruce.info/busuanzi?jsonpCallback=BusuanziCallback", a => {
  bszTag.texts(a),
  bszTag.shows()
}));reset()})</script><meta name="generator" content="Hexo 6.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head><body><div class="loading" style="opacity: 0;"><div class="loadingBar left"></div><div class="loadingBar right"></div></div><main><header class="closed"><div class="navBtn"><i class="navBtnIcon"><span class="navBtnIconBar"></span><span class="navBtnIconBar"></span><span class="navBtnIconBar"></span></i></div><nav><div class="navItem" id="search-header"><span class="navItemTitle"><input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="数据检索" spellcheck="false" maxlength="50" type="text" id="search-input"></span></div><div class="navItem" id="search-holder"></div><div class="search-popup" tabindex="0"><div id="search-result"></div></div><ol class="navContent"><li class="navItem"><a class="navBlock" href="/"><span class="navItemTitle">大厅</span></a></li><li class="navItem" matchdata="categories,tags"><a class="navBlock" href="/archives/"><span class="navItemTitle">档案室</span></a></li><li class="navItem"><a class="navBlock" href="/%E7%AE%80%E5%8E%86/"><span class="navItemTitle">人事简历</span></a></li><li class="navItem"><a class="navBlock" href="/music/"><span class="navItemTitle">塞壬唱片</span></a></li></ol></nav></header><article><div id="post-bg"><div id="post-title"><h1>EnlightenGAN论文阅读笔记</h1><div id="post-info"><span>文章发布时间: <div class="control"><time datetime="2024-02-23T16:00:00.000Z" id="date"> 2024-02-24</time></div></span><br><span>最后更新时间: <div class="control"><time datetime="2024-02-24T16:08:25.063Z" id="updated"> 2024-02-25</time></div></span><br><span>文章总字数: <div class="control">2.1k</div></span><br><span id="busuanzi_container_page_pv">页面浏览: <span class="control" id="busuanzi_value_page_pv">加载中...</span></span></div></div><hr><div id="post-content"><h1 id="EnlightenGAN论文阅读笔记"><a href="#EnlightenGAN论文阅读笔记" class="headerlink" title="EnlightenGAN论文阅读笔记"></a>EnlightenGAN论文阅读笔记</h1><blockquote>
<p>论文是2019年IEEE的EnlightenGAN: Deep Light Enhancement without Paired Supervision.这篇论文是低光增强领域无监督学习的开山之作。</p>
<p>论文链接如下：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1906.06972.pdf">arxiv.org/pdf/1906.06972.pdf</a></p>
</blockquote>
<p>[TOC]</p>
<h2 id="出发点"><a href="#出发点" class="headerlink" title="出发点"></a>出发点</h2><h3 id="出发点1：从监督学习的缺点入手。"><a href="#出发点1：从监督学习的缺点入手。" class="headerlink" title="出发点1：从监督学习的缺点入手。"></a><strong>出发点1</strong>：从监督学习的缺点入手。</h3><blockquote>
<p>it is very difficult or even impractical to simultaneously capture corrupted and ground truth images of the same visual scene.</p>
</blockquote>
<p>​    指出在低光增强领域，监督学习的第一个不足之处——在同一个场景下同时获得亮度正常的图片和低光图片是很难而且不现实的。</p>
<blockquote>
<p>synthesizing corrupted images from clean images could sometimes help, but such synthesized results are usually not photo-realistic enough.</p>
</blockquote>
<p>​    监督学习有时候会使用合成的低光图片进行训练，也就是拍摄正常光照的图片后，经过模糊、加噪音等，合成低光图片，作为一对数据进行训练。作者指出这种方式并不足够真实。算是第二个不足之处。</p>
<p>​    我的理解是低光图片的合成是人为控制的，因此合成的低光程度不一样得到的效果也不同。</p>
<blockquote>
<p>specifically for the low-light enhancement problem, there may be no unique or well-defined high-light ground truth given a low-light image.</p>
</blockquote>
<p>​    作者指出低光领域监督学习的第三个不足之处，在于低光图片的ground-truth并不唯一。某个场景晚上的图片，它的ground-truth可以是该场景在白天的任何时候。也就是说，为一张低光图片配对一个绝对的正常光照图片是没有必要的。</p>
<h3 id="出发点2：从拍摄所得低光图片的特点入手。"><a href="#出发点2：从拍摄所得低光图片的特点入手。" class="headerlink" title="出发点2：从拍摄所得低光图片的特点入手。"></a><strong>出发点2</strong>：从拍摄所得低光图片的特点入手。</h3><blockquote>
<p>Taking into account the above issues, our overarching goal is to enhance a low-light photo with spatially varying light conditions and over/under-exposure artifacts, while the paired training data is unavailable.</p>
</blockquote>
<p>​    作者认为，低光图片的增强需要考虑到图片光照条件的空间变化。也就是说，一张图片拍摄后，不同地方的光照不同，那么进行低光增强时，不同位置的增强的程度也不同，如果一张图片给所有地方增强程度相同，就有可能出现过度曝光而失真的情况。</p>
<h2 id="创新点"><a href="#创新点" class="headerlink" title="创新点"></a>创新点</h2><h3 id="创新点1：在低光增强中第一次引入双重判别器。"><a href="#创新点1：在低光增强中第一次引入双重判别器。" class="headerlink" title="创新点1：在低光增强中第一次引入双重判别器。"></a><strong>创新点1</strong>：在低光增强中第一次引入双重判别器。</h3><blockquote>
<p>We first propose a dual- discriminator to balance global and local low-light enhancement.</p>
</blockquote>
<h3 id="创新点2：引入一个自特征保留损失。"><a href="#创新点2：引入一个自特征保留损失。" class="headerlink" title="创新点2：引入一个自特征保留损失。"></a><strong>创新点2</strong>：引入一个自特征保留损失。</h3><blockquote>
<p>Further, owing to the absence of ground-truth supervision, a self-regularized perceptual loss is proposed to constrain the feature distance between the low-light input image and its enhanced version, which is subsequently adopted both locally and globally together with the adversarial loss for training EnlightenGAN.</p>
</blockquote>
<h3 id="创新点3：开发原始低光输入的Attention-Map引导生成。"><a href="#创新点3：开发原始低光输入的Attention-Map引导生成。" class="headerlink" title="创新点3：开发原始低光输入的Attention Map引导生成。"></a><strong>创新点3</strong>：开发原始低光输入的Attention Map引导生成。</h3><blockquote>
<p>We also propose to exploit the illumination information of the low-light input as a self-regularized attentional map in each level of deep features to regularize the unsupervised learning.</p>
</blockquote>
<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p class='item-img' data-src='/2024/02/24/EnlightenGAN%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E7%AC%94%E8%AE%B0/屏幕截图 2023-12-12 203151.png'><img src="/2024/02/24/EnlightenGAN%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E7%AC%94%E8%AE%B0/屏幕截图 2023-12-12 203151.png" alt="iamge"></p>
<p>​    <strong>框架</strong>：作者使用U-Net作为Generator，在其中对每层加入了注意力map以引导生成。使用了两个Discriminator，均为PatchGAN设计。一个是Global Discriminator，一个是Local Discriminator。</p>
<blockquote>
<p>By extracting multi-level features from different depth layers, U-Net pre- serves rich texture information and synthesizes high quality images using multi-scale context information.</p>
<p>PatchGAN是一种设计，最早出现于CircleGAN模型的判别器中。它是原来GAN的一种延申与升级。PatchGAN与感受野有关，具体在<a target="_blank" rel="noopener" href="https://blog.csdn.net/landing_guy_/article/details/121214808">Patch GAN的理解_patchgan判别器-CSDN博客</a>这篇博客中有详述。</p>
</blockquote>
<p>​    <strong>Generator详解</strong>：输入的低光RGB图片取出其照明通道I，然后归一化为[0， 1]，然后利用1-I（元素差异）作为该层的attention map。然后这层的低光RGB图片经过卷积与最大值汇聚得到下一层的RGB图片。然后此RGB图片取出其照明通道I，这个照明通道同样地归一化为[0， 1]，利用1-I（元素差异）作为该层的attention map。这一层的RGB图像继续通过卷积与最大值汇聚得到下一层的RGB图片，以此类推。直到RGB图片只经过卷积得到最后一层的RGB图片，然后经过该层的attention map（元素乘法），然后通过上采样层Upsampling Layer和卷积层得到上一层的RGB图片，以此类推，最顶层的RGB图片经过上采样与卷积，与最顶层的attention map元素乘法，再与残差连接的原始低光图片相加，得到增强光亮后的输出。</p>
<p>​    值得一提的是，模型中的Upsampling Layer并非简单的一个转置卷积，而是一个双线性上采样层，以减轻伪影。</p>
<p>​    <strong>Discriminator详解</strong>：我的理解是，Global Discriminator对整张输出图片和原始低光图片判别，Local Discriminator在正常低光图片和增强图片分别随机地裁剪局部块进行判别。两个Discriminator都进行判断输出图片来自真实图片还是增强图片。</p>
<h2 id="设计及其损失"><a href="#设计及其损失" class="headerlink" title="设计及其损失"></a>设计及其损失</h2><h3 id="1-双重判别器（Dual-Discriminator）及其损失"><a href="#1-双重判别器（Dual-Discriminator）及其损失" class="headerlink" title="1.双重判别器（Dual Discriminator）及其损失"></a>1.双重判别器（Dual Discriminator）及其损失</h3><blockquote>
<p>we observe that an image-level vanilla discriminator often fails on spatially-varying light images; if the input image has some local area that needs to be enhanced differently from other parts, e.g., a small bright region in an overall dark background, the global image discriminator alone is often unable to provide the desired adaptivity.</p>
</blockquote>
<p>​    拍摄的图片的光照是存在空间变化的，如果有一些部位需要特别地增强，而有一些部位相对于其他部位又很亮，那么单纯用一个全局的判别器总是做的很失败。所以作者引入全局-局部判别器，以解决局部过度增强以及局部增强不足的情况。</p>
<p>​    对于全局判别器的损失，首先基于Relativistic discriminator的损失结构，标准的判别器损失为：</p>
<script type="math/tex; mode=display">
D_{Ra}(x_r, x_f) = \sigma(C(x_r) - \mathbb{R}_{x_f \sim \mathbb{P}_{fake}}[C(x_f)]),\tag{1}</script><script type="math/tex; mode=display">
D_{Ra}(x_f, x_r) = \sigma(C(x_f) - \mathbb{E}_{x_r \sim \mathbb{P}_{real}}[C(x_r)]),\tag{2}</script><p>然后将<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.292ex" height="1ex" role="img" focusable="false" viewbox="0 -431 571 442"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D70E" d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"/></g></g></g></svg></mjx-container>换为LSGAN中的损失函数，最后得到全局判别器D和全局生成器G的损失：</p>
<script type="math/tex; mode=display">
L^{Global}_D = \mathbb{E}_{x_r \sim \mathbb{P}_{real}}[(D_{Ra}(x_r, x_f) - 1)^2] + \mathbb{E}_{x_f \sim \mathbb{P}_{fake}}[D_{Ra}(x_f, x_r)^2],\tag{3}</script><script type="math/tex; mode=display">
L_G^{Global} = \mathbb{E}_{x_f \sim \mathbb{P}_{fake}}[(D_{Ra}(x_f, x_r) - 1)^2] + \mathbb{E}_{x_r \sim \mathbb{P}_{real}}[D_{Ra}(x_r, x_f)^2],\tag{4}</script><p>​    对于局部判别器的损失，直接引用LSGAN的对抗性损失：</p>
<script type="math/tex; mode=display">
L_D^{Local} = \mathbb{E}_{x_r \sim \mathbb{P}_{real-patches}}[(D(x_r) - 1)^2] + \mathbb{E}_{x_f \sim \mathbb{P}_{fake-patches}}[(D(x_f) - 0)^2],\tag{5}</script><script type="math/tex; mode=display">
L_G^{Local} = \mathbb{E}_{x_r \sim \mathbb{P}_{fake-patches}}[(D(x_f) - 1)^2],\tag{6}</script><h3 id="2-自特征保留损失（Self-Feature-Preserving-Loss）"><a href="#2-自特征保留损失（Self-Feature-Preserving-Loss）" class="headerlink" title="2.自特征保留损失（Self Feature Preserving Loss）"></a>2.自特征保留损失（Self Feature Preserving Loss）</h3><blockquote>
<p>感知损失：Johnson等人提出感知损失，通常的做法是通过预训练的VGG去抽取输出图片和真实标签图片的特征，然后限制这两组特征的距离。</p>
</blockquote>
<p>​    基于感知损失，作者提供了无监督学习版的感知损失：并非限制输出与真实标签特征的距离，而是限制输入与输出特征的距离。</p>
<blockquote>
<p>In our unpaired setting, we propose to instead constrain the VGG-feature distance between the input low-light and its enhanced normal-light output.</p>
</blockquote>
<p>损失函数如下：</p>
<script type="math/tex; mode=display">
L_{SFP}(I^L) = \frac{1}{W_{i, j}H{i, j}} \sum_{x = 1}^{W_{i, j}}{\sum_{y = 1}^{H_{i, j}}{(\phi _{i, j}(I^L) - \phi_{i, j}(G(I^L)))^2}}, \tag{7}</script><p>符号描述如下：</p>
<blockquote>
<p>where <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="3.045ex" height="2.297ex" role="img" focusable="false" viewbox="0 -1015.1 1346.1 1015.1"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mstyle" transform="scale(1.2)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D43C" d="M43 1Q26 1 26 10Q26 12 29 24Q34 43 39 45Q42 46 54 46H60Q120 46 136 53Q137 53 138 54Q143 56 149 77T198 273Q210 318 216 344Q286 624 286 626Q284 630 284 631Q274 637 213 637H193Q184 643 189 662Q193 677 195 680T209 683H213Q285 681 359 681Q481 681 487 683H497Q504 676 504 672T501 655T494 639Q491 637 471 637Q440 637 407 634Q393 631 388 623Q381 609 337 432Q326 385 315 341Q245 65 245 59Q245 52 255 50T307 46H339Q345 38 345 37T342 19Q338 6 332 0H316Q279 2 179 2Q143 2 113 2T65 2T43 1Z"/></g><g data-mml-node="mi" transform="translate(590.2,363) scale(0.707)"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"/></g></g></g></g></g></g></svg></mjx-container>denotes the input low-light image and <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.679ex;" xmlns="http://www.w3.org/2000/svg" width="7.292ex" height="2.975ex" role="img" focusable="false" viewbox="0 -1015.1 3222.9 1315.1"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mstyle" transform="scale(1.2)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D43A" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q492 659 471 656T418 643T357 615T294 567T236 496T189 394T158 260Q156 242 156 221Q156 173 170 136T206 79T256 45T308 28T353 24Q407 24 452 47T514 106Q517 114 529 161T541 214Q541 222 528 224T468 227H431Q425 233 425 235T427 254Q431 267 437 273H454Q494 271 594 271Q634 271 659 271T695 272T707 272Q721 272 721 263Q721 261 719 249Q714 230 709 228Q706 227 694 227Q674 227 653 224Q646 221 643 215T629 164Q620 131 614 108Q589 6 586 3Q584 1 581 1Q571 1 553 21T530 52Q530 53 528 52T522 47Q448 -22 322 -22Q201 -22 126 55T50 252Z"/></g><g data-mml-node="mo" transform="translate(786,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="msup" transform="translate(1175,0)"><g data-mml-node="mi"><path data-c="1D43C" d="M43 1Q26 1 26 10Q26 12 29 24Q34 43 39 45Q42 46 54 46H60Q120 46 136 53Q137 53 138 54Q143 56 149 77T198 273Q210 318 216 344Q286 624 286 626Q284 630 284 631Q274 637 213 637H193Q184 643 189 662Q193 677 195 680T209 683H213Q285 681 359 681Q481 681 487 683H497Q504 676 504 672T501 655T494 639Q491 637 471 637Q440 637 407 634Q393 631 388 623Q381 609 337 432Q326 385 315 341Q245 65 245 59Q245 52 255 50T307 46H339Q345 38 345 37T342 19Q338 6 332 0H316Q279 2 179 2Q143 2 113 2T65 2T43 1Z"/></g><g data-mml-node="mi" transform="translate(590.2,363) scale(0.707)"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"/></g></g><g data-mml-node="mo" transform="translate(2296.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></g></g></svg></mjx-container> denotes the generator’s enhanced output. <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.666ex;" xmlns="http://www.w3.org/2000/svg" width="3.192ex" height="2.236ex" role="img" focusable="false" viewbox="0 -694 1410.9 988.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D719" d="M409 688Q413 694 421 694H429H442Q448 688 448 686Q448 679 418 563Q411 535 404 504T392 458L388 442Q388 441 397 441T429 435T477 418Q521 397 550 357T579 260T548 151T471 65T374 11T279 -10H275L251 -105Q245 -128 238 -160Q230 -192 227 -198T215 -205H209Q189 -205 189 -198Q189 -193 211 -103L234 -11Q234 -10 226 -10Q221 -10 206 -8T161 6T107 36T62 89T43 171Q43 231 76 284T157 370T254 422T342 441Q347 441 348 445L378 567Q409 686 409 688ZM122 150Q122 116 134 91T167 53T203 35T237 27H244L337 404Q333 404 326 403T297 395T255 379T211 350T170 304Q152 276 137 237Q122 191 122 150ZM500 282Q500 320 484 347T444 385T405 400T381 404H378L332 217L284 29Q284 27 285 27Q293 27 317 33T357 47Q400 66 431 100T475 170T494 234T500 282Z"/></g><g data-mml-node="TeXAtom" transform="translate(629,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="mi" transform="translate(623,0)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"/></g></g></g></g></g></svg></mjx-container>denotes the feature map extracted from a VGG-16 model pre-trained on ImageNet. i represents its i-th max pooling, and j represents its j-th convolutional layer after i-th max pooling layer. <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.799ex;" xmlns="http://www.w3.org/2000/svg" width="4.775ex" height="2.653ex" role="img" focusable="false" viewbox="0 -819.6 2110.6 1172.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mstyle" transform="scale(1.2)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"/></g><g data-mml-node="TeXAtom" transform="translate(977,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="mi" transform="translate(623,0)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"/></g></g></g></g></g></g></g></svg></mjx-container>and <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.799ex;" xmlns="http://www.w3.org/2000/svg" width="4.468ex" height="2.653ex" role="img" focusable="false" viewbox="0 -819.6 1975 1172.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mstyle" transform="scale(1.2)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"/></g><g data-mml-node="TeXAtom" transform="translate(864,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="mi" transform="translate(623,0)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"/></g></g></g></g></g></g></g></svg></mjx-container> are the dimensions of the extracted feature maps. By default we choose i = 5, j = 1.</p>
</blockquote>
<p>下面这段话说明，自特征保留损失作用于全局判别器，也作用于局部判别器：</p>
<blockquote>
<p>For our local discriminator, the cropped local patches from input and output images are also regularized by a similarly defined self feature preserving loss.</p>
</blockquote>
<h3 id="总损失"><a href="#总损失" class="headerlink" title="总损失"></a>总损失</h3><script type="math/tex; mode=display">
Loss = L_{SFP}^{Global} + L_{SFP}^{Local} + L_G^{Global} + L_G^{Local}, \tag{8}</script><h3 id="自正则Attention-Map"><a href="#自正则Attention-Map" class="headerlink" title="自正则Attention Map"></a>自正则Attention Map</h3><p>在前面的模型中，已经解释了Attention Map的机理：</p>
<blockquote>
<p>We take the illumination channel I of the input RGB image, normalize it to [0,1], and then use 1−I (element-wise difference) as our self-regularized attention map. We then resize the attention map to fit each feature map and multiply it with all intermediate feature maps as well as the output image.</p>
</blockquote>
<p>每一层的Attention Map要Resize为与该层RGB特征图片形状一致，这样才能进行元素乘法。</p>
<p>作者指出，Attention Map也是一个自正则化的手段，而且是该工作中非常关键的一步。</p>
<blockquote>
<p>本文毕</p>
</blockquote>
<div id="paginator"></div></div><div id="post-footer"><div id="pages"><div class="footer-link" style="width: 50%;text-align:right;border-right:1px #fe2 solid"><a href="/2024/02/24/Retinexformer%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E7%AC%94%E8%AE%B0/">← 下一篇 Retinexformer论文精读笔记</a></div><div class="footer-link" style="width: 50%;right:1px;border-left:1px #fe2 solid"><a href="/2024/02/23/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/">注意力机制 上一篇 →</a></div></div></div><details id="reward"><summary>打赏</summary><div><span>支付宝 | Alipay</span><br><img src="/img/Alipay.png"></div><div><span>微信 | Wechat</span><br><img src="/img/WeChat.png"></div></details><div id="comments"><div id="valine"></div></div></div><div class="bottom-btn"><div><a class="i-top" id="to-top" onClick="scrolls.scrolltop();" title="回到顶部" style="opacity: 0; display: none;">∧ </a><a class="i-index" id="to-index" href="#toc-div" title="文章目录">≡</a><a class="i-color" id="color-mode" onClick="colorMode.change()" title="切换主题"></a></div></div></article><aside><div id="about"><a href="/" id="logo"><img src="https://ak.hypergryph.com/assets/index/images/ak/pc/faction/1.png" alt="Logo"></a><h1 id="Dr"><a href="/">Dr.Alex_Fall</a></h1><div id="description"><p>这是一个舟厨的博客</p></div><div id="social-links"><a class="social" target="_blank" rel="noopener" href="https://github.com/yunhdan"><i class="fab fa-github" alt="GitHub"></i></a><a class="social" target="_blank" rel="noopener" href="https://blog.csdn.net/m0_73599738?type=blog"><i class="fa-solid fa-c" alt="CSDN"></i></a></div></div><div id="aside-block"><div id="toc-div"><h1>目录</h1><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#EnlightenGAN%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0"><span class="toc-number">1.</span> <span class="toc-text">EnlightenGAN论文阅读笔记</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%87%BA%E5%8F%91%E7%82%B9"><span class="toc-number">1.1.</span> <span class="toc-text">出发点</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%87%BA%E5%8F%91%E7%82%B91%EF%BC%9A%E4%BB%8E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%BC%BA%E7%82%B9%E5%85%A5%E6%89%8B%E3%80%82"><span class="toc-number">1.1.1.</span> <span class="toc-text">出发点1：从监督学习的缺点入手。</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%87%BA%E5%8F%91%E7%82%B92%EF%BC%9A%E4%BB%8E%E6%8B%8D%E6%91%84%E6%89%80%E5%BE%97%E4%BD%8E%E5%85%89%E5%9B%BE%E7%89%87%E7%9A%84%E7%89%B9%E7%82%B9%E5%85%A5%E6%89%8B%E3%80%82"><span class="toc-number">1.1.2.</span> <span class="toc-text">出发点2：从拍摄所得低光图片的特点入手。</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E6%96%B0%E7%82%B9"><span class="toc-number">1.2.</span> <span class="toc-text">创新点</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E6%96%B0%E7%82%B91%EF%BC%9A%E5%9C%A8%E4%BD%8E%E5%85%89%E5%A2%9E%E5%BC%BA%E4%B8%AD%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%BC%95%E5%85%A5%E5%8F%8C%E9%87%8D%E5%88%A4%E5%88%AB%E5%99%A8%E3%80%82"><span class="toc-number">1.2.1.</span> <span class="toc-text">创新点1：在低光增强中第一次引入双重判别器。</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E6%96%B0%E7%82%B92%EF%BC%9A%E5%BC%95%E5%85%A5%E4%B8%80%E4%B8%AA%E8%87%AA%E7%89%B9%E5%BE%81%E4%BF%9D%E7%95%99%E6%8D%9F%E5%A4%B1%E3%80%82"><span class="toc-number">1.2.2.</span> <span class="toc-text">创新点2：引入一个自特征保留损失。</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E6%96%B0%E7%82%B93%EF%BC%9A%E5%BC%80%E5%8F%91%E5%8E%9F%E5%A7%8B%E4%BD%8E%E5%85%89%E8%BE%93%E5%85%A5%E7%9A%84Attention-Map%E5%BC%95%E5%AF%BC%E7%94%9F%E6%88%90%E3%80%82"><span class="toc-number">1.2.3.</span> <span class="toc-text">创新点3：开发原始低光输入的Attention Map引导生成。</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.3.</span> <span class="toc-text">模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%BE%E8%AE%A1%E5%8F%8A%E5%85%B6%E6%8D%9F%E5%A4%B1"><span class="toc-number">1.4.</span> <span class="toc-text">设计及其损失</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E5%8F%8C%E9%87%8D%E5%88%A4%E5%88%AB%E5%99%A8%EF%BC%88Dual-Discriminator%EF%BC%89%E5%8F%8A%E5%85%B6%E6%8D%9F%E5%A4%B1"><span class="toc-number">1.4.1.</span> <span class="toc-text">1.双重判别器（Dual Discriminator）及其损失</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E8%87%AA%E7%89%B9%E5%BE%81%E4%BF%9D%E7%95%99%E6%8D%9F%E5%A4%B1%EF%BC%88Self-Feature-Preserving-Loss%EF%BC%89"><span class="toc-number">1.4.2.</span> <span class="toc-text">2.自特征保留损失（Self Feature Preserving Loss）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E6%8D%9F%E5%A4%B1"><span class="toc-number">1.4.3.</span> <span class="toc-text">总损失</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E6%AD%A3%E5%88%99Attention-Map"><span class="toc-number">1.4.4.</span> <span class="toc-text">自正则Attention Map</span></a></li></ol></li></ol></li></ol></div></div><footer><nobr>构建自 <a target="_blank" rel="noopener" href="http://hexo.io">Hexo</a></nobr><wbr><nobr> 使用主题 <a target="_blank" rel="noopener" href="https://github.com/Yue-plus/hexo-theme-arknights">Arknights</a></nobr><wbr><nobr> 主题作者 <a target="_blank" rel="noopener" href="https://github.com/Yue-plus">Yue_plus</a></nobr></footer></aside></main><canvas id="canvas-dust"></canvas></body></html>