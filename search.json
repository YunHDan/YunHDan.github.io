[{"title":"PairLIE论文阅读笔记","url":"/2024/02/22/PairLIE%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/","content":"\n\nPairLIE论文阅读笔记\n论文为2023CVPR的Learning a Simple Low-light Image Enhancer from Paired Low-light Instances.论文链接如下：\nopenaccess.thecvf.com/content/CVPR2023/papers/Fu_Learning_a_Simple_Low-Light_Image_Enhancer_From_Paired_Low-Light_Instances_CVPR_2023_paper.pdf\n\n[TOC]\n出发点\n1.However, collecting high-quality reference maps in real-world scenarios is time-consuming and expensive.\n\n出发点1：在低光照领域，从现实世界中获取高质量的参考照片进行监督学习，既费时又困难，成本昂贵。\n因为获得低光环境的照片是容易的，而此低光照片对应的亮度较大的参考图片是难得的。\n\n2.To tackle the issues of limited information in a single low-light image and the poor adaptability of handcrafted priors, we propose to leverage paired low-light instances to train the LIE network.\nAdditionally, twice-exposure images provide useful information for solving the LIE task. As a result, our solution can reduce the demand for handcrafted priors and improve the adaptability of the network.\n\n出发点2：为了解决手动设置的先验的低适应性，减少手动设置先验的需求，同时提升模型对陌生环境的适应性。\n创新点\nThe core insight of our approach is to sufficiently exploit priors from paired low-light images.\nThose low-light image pairs share the same scene content but different illumination. Mathematically, Retinex decomposition with low-light image pairs can be expressed as:\n\n\n创新点1：作者利用两张低光图片进行训练，以充分提取低光图片的信息。\n\ninstead of directly imposing the Retinex decomposition on original low-light images, we adopt a simple self-supervised mechanism to remove inappropriate features and implement the Retinex decomposition on the optimized image.\n\n创新点2：作者基于Retinex理论，但是并不循旧地直接运用Retinex的分解。作者采用一个简单的自监督机制以实现不合理特征的去除（通常是一些噪音）以及更好地实现Retinex理论。\n模型\n将两张同一场景不同曝光的低光图片送入训练中，图片I1与I2先经过P-Net去除噪音，得到i1与i2，然后利用L-Net与R-Net分解为照度L1与反射R1（对应有L2与R2）。\n在测试，只需要输入一张低光照图片I，经过P-Net的噪音去除，得到i，然后用L-Net与R-Net分解为照度和反射，然后对照度L进行增强，操作为g(L)，把增强结果与反射R进行元素乘法，得到增强后的图片Enhanced Image。\n设计及其损失\nNote that, this paper does not focus on designing modernistic network structures. L-Net and R-Net are very similar and simple,\n\n1.模型使用的L-Net与R-Net十分简单。整体架构只是单纯的卷积神经网络。\n\nApart from L-Net and R-Net, we introduce P-Net to remove inappropriate features from the original image. Specifically, the structure of the P-Net is identical to the R-Net.\n\n2,P-Net被设计用于去除不合理特征。 \n\nNote that the projection loss needs to cooperate with the other constraints to avoid a trivial solution.i,e.,i1 = I1.\n\n3.Projection Loss：最大程度限制去除不合理特征后的i1和原始低光图片I1的区别。\n这个损失需要避免一个特例，即降噪后图片与原图相同，即未降噪。\n\nSince sensor noise hidden in dark regions will be amplified when the contrast is improved.\nIn our method, the sensor noise can be implicitly removed by Eq. 1.\n\n4.Reflection Loss：通常用传感或摄影设备拍摄低光场景照片会携带一定的设备噪音，这个损失最大限度保证两张图片的反射是相同的，减少传感或摄影设备的影响，这是因为图片场景的内容相同。\n这个损失是确保反射的一致性。\n\nis applied to ensure a reasonable decomposition.\n is to guide the decomposition.\nSpecifically, the initialized illumination L0 is calculated via the maximum of the R, G, and B channels：\n\n5.Retinex Loss：Retinex损失是为了限制分解组块L-Net和R-Net以满足Retinex的理论要求。\n\n本文毕\n\n","categories":["科研"],"tags":["low-light image enhancement","笔记"]},{"title":"test","url":"/2024/02/22/test/","content":"本文件用来测试一下latex。\nnpm安装的package如下：\nhexo-browsersync@0.3.0├── hexo-deployer-git@4.0.0├── hexo-filter-mathjax@0.9.0├── hexo-generator-archive@2.0.0├── hexo-generator-category@2.0.0├── hexo-generator-index@3.0.0├── hexo-generator-tag@2.0.0├── hexo-renderer-ejs@2.0.0├── hexo-renderer-kramed@0.1.4├── hexo-renderer-pug@3.0.0├── hexo-renderer-stylus@2.1.0├── hexo-server@3.0.0└── hexo@6.3.0\n 这个成功了，具体操作是公式与$之间增加空格\n  $前后都有空格，结果一样\na \\in b 加入`，直接不显示公式\n$ a \\in b $ ， 加入`和$前后都有空格，显示高亮的源代码\n ， 在$前加入\\转义，一样正常显示\n开头加入mathjax:true有关键性影响\n，测试发现，公式与$之间无空格也没问题。\n测试行间公式，$$会导致前文大量内容被注释。\n\n\n发现要使用行间公式，必须对$进行转义。\n，有空格，无花括号\n，无空格，无花括号\n，无空格，有花括号\n，有空格，有花括号\n在本地网站没有任何问题。在实际blog发现也正常。\n现在测试一下其他重要的特性。\nimport torchprint(torch.cuda.is_available())\n$ hexo clean\n所以至此latex渲染已经基本完成部署。\n参考博客：\nhttp://t.csdnimg.cn/Y1Lv8\n","categories":["乱的"],"tags":["杂的"]},{"title":"注意力机制","url":"/2024/02/23/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/","content":"注意力机制[TOC]\n\n所学的自注意力机制的知识由李沐老师教授，感恩沐神！\n\n注意力提示双组件与查询、键、值​    注意力提示双组件分为非自主性提示与自主性提示。非自主性提示可以理解为环境中物体的突出性带来的提示。比如，一杯红咖啡摆在一堆报纸中间，咖啡是红色的，这种突出性的提示就是环境带来的非自主性提示。与之相反的是自主性提示，当我们喝完咖啡，会兴奋，兴奋起来开始阅读报纸。也就是说人们受主观意愿推动去集中于看报纸，这就是自主性提示，这个自主性提示来自人们自己。\n\n\n​    在非自主性提示中，分为两种，一种是感官的输入，称为值，通常是一些感官输入，比如咖啡的颜色，书本的字体颜色。另一种是非意志的线索，称为键，比如书本就是一个键，咖啡就是一个键，都是客观存在的非意志的线索。\n自主性提示，通常被称为查询，它们是一些意志线索，是人的主观意愿。\n​    注意力机制做的是什么事情？一般来说，环境中的非意志线索——键会与感官输入——值，一一对应。这时，主观的意志因素——查询通过注意力汇聚（也称注意力池化）选择其中一个值，然后输出。这样就做到了所谓的集中注意力。\n\n非参数注意力汇聚： 核回归​    在注意力机制的早期，或者说是注意力机制未出现时，人们通过用平均汇聚（平均池化）处理问题，效果比较差。后来，和提出了 核回归。公式如下：\n\n​    给定查询x，通过这个f函数，可以对每个键值对附上权重，权重最大的值，即为注意力集中处。其中，xi是每个键，是每个值，K是核，本质上是一个函数，后面我们再举例核的选取。这里我们再注意一下分母，分母的也是每个键，只是为了保证公式内符号不冲突，两次遍历一个用i一个用j，实际代表的意义是一样的。\n当K是高斯核时，即K公式为：\n代入核回归公式，有：\n这样，当选取的是高斯核，其实注意力机制与有关了。\n​    在 核回归下，如果一个键xi越是接近给定的查询x，那么分配给这个键对应值的注意力权重就越大，也就获得了更多的注意力。\n这是早期的注意力机制，使用的 还只是一个非参数的注意力汇聚方法。\n带参数的注意力汇聚​    前面提到， 实际上是一个非参数的注意力汇聚方法，我们当然可以自己加上一个可学习的参数到该注意力汇聚中，得到一个带参数的注意力汇聚。\n具体操作，只需要在下面的查询x和键xi之间的距离乘以可学习参数w即可：\n通过训练，即可学习到较合适的w，得到较合适的注意力汇聚函数。\n注意力评分函数注意力汇聚函数f，可以被写成下面的加权和：\n\n​    其中，q是查询query，k是键key，v是值value，m是键值对的个数，α是一个用q和k，通过特定的函数计算出来的注意力权重，公式如下：\n\n​    其中a就是我们的注意力评分函数。注意力评分函数a，通过查询q和键k，输出一个结果，然后将结果送入到中，计算出注意力权重，本质上是键对应的值的概率分布，概率小，选择该键值对的机会就小，对应得到更少的注意力。\n​    特别地，高斯核函数的注意力评分函数是指数exp里的部分，即(-1 / 2)(x - xi)²。注意力评分函数不是固定的，可以自己选择注意力评分函数，然后把这个函数的输出结果输入到中计算出注意力权重。\n宏观来看，注意力机制可以用下图描述，它实现了上面的公式。\n\n加性注意力一般来说，当查询和键是不同长度的矢量时，可以使用加性注意力作为评分函数。\n比如，q是长为q的向量，k是长度为k的向量，k与q不相同。这时，加性注意力的评分函数为：\n​    其中，可学习的参数是、和，是h×q的矩阵，是h×k的矩阵，是h×1的向量。这个公式保证了最后得到的注意力权重是一个实数。\n​    观察式子，不难发现，其实注意力评分函数是通过将query和key链接起来然后传入感知机中实现的，使用作为激活函数，而且无偏置项。h是隐藏单元数，是一个超参数。\n看代码实现会促进理解：\nclass AdditiveAttention(nn.Module):    \"\"\"加性注意力\"\"\"    def __init__(self, key_size, query_size, num_hiddens, dropout, **kwargs):        super(AdditiveAttention, self).__init__(**kwargs)        self.W_k = nn.Linear(key_size, num_hiddens, bias=False)        self.W_q = nn.Linear(query_size, num_hiddens, bias=False)        self.w_v = nn.Linear(num_hiddens, 1, bias=False)        self.dropout = nn.Dropout(dropout)    def forward(self, queries, keys, values, valid_lens):        queries, keys = self.W_q(queries), self.W_k(keys)        # 在维度扩展后，        # queries的形状：(batch_size，查询的个数，1，num_hidden)        # key的形状：(batch_size，1，“键－值”对的个数，num_hiddens)        # 使用广播方式进行求和        features = queries.unsqueeze(2) + keys.unsqueeze(1)        features = torch.tanh(features)        # self.w_v仅有一个输出，因此从形状中移除最后那个维度。        # scores的形状：(batch_size，查询的个数，“键-值”对的个数)        scores = self.w_v(features).squeeze(-1)        self.attention_weights = masked_softmax(scores, valid_lens)        # values的形状：(batch_size，“键－值”对的个数，值的维度)        return torch.bmm(self.dropout(self.attention_weights), values)\nMasked-：掩蔽操作前面的代码中，可以注意到有个叫做masked-操作，这里简单记录一下。\n​    操作是用于输出一个概率分布，在注意力机制中，可以作为注意力的权重。在数据中，某些文本序列有时被填充了没有意义的特殊词元，为了只将有意义的词元注入到注意力汇聚中获得注意力权重，可以指定一个有效词元长度，使得计算时，超出有效长度的部分被过滤掉，这就是Masked-。\n超出有效长度的部分归为0，达到被掩蔽的作用。\n缩放点积注意力当查询和键长度相同，可以用缩放点积注意力评分函数。\n查询、键和值的缩放点积注意力是：\n\n​    里的内容就是缩放点积注意力评分函数。其中，d是查询和键的长度，n是小批量，m是键值对数，v是值的长度。\n代码实现如下，促进理解：\nclass DotProductAttention(nn.Module):    \"\"\"缩放点积注意力\"\"\"    def __init__(self, dropout, **kwargs):        super(DotProductAttention, self).__init__(**kwargs)        self.dropout = nn.Dropout(dropout)    # queries的形状：(batch_size，查询的个数，d)    # keys的形状：(batch_size，“键－值”对的个数，d)    # values的形状：(batch_size，“键－值”对的个数，值的维度)    # valid_lens的形状:(batch_size，)或者(batch_size，查询的个数)    def forward(self, queries, keys, values, valid_lens=None):        d = queries.shape[-1]        # 设置transpose_b=True为了交换keys的最后两个维度        scores = torch.bmm(queries, keys.transpose(1,2)) / math.sqrt(d)        self.attention_weights = masked_softmax(scores, valid_lens)        return torch.bmm(self.dropout(self.attention_weights), values)\n自注意力​    给定由词元组成的序列，，……，，其中任意(1  i  n)，n是序列长度，d是词元特征。自注意力输出为一个长度相同的序列，，……，，其中：每一个都是对应词元的注意力权重。\n​    也就是说，自注意力其实就是把查询变成每个键自己，通过某种运算来直接计算得到句子在编码过程中每个位置上的注意力权重；然后再以权重和的形式来计算得到整个句子的隐含向量表示。\n\n位置编码绝对位置信息​    在处理词元序列时，循环神经网络是逐个的重复地处理词元的， 而自注意力则因为并行计算而放弃了顺序操作。 为了使用序列的顺序信息，通过在输入表示中添加位置编码，来注入绝对的或相对的位置信息。\n​    位置编码信息可以通过学习得到，也可以通过直接固定得到。这里介绍Transformer使用的基于正弦函数和余弦函数的固定位置编码方法。\n​    假设输入是batch中一个序列中，n个词元的d维嵌入表示。位置编码使用相同形状的位置嵌入矩阵，输出。位置嵌入矩阵第i行的第列和第列的元素由如下公式计算：\n\n​    为了让基于正弦函数和余弦函数地固定位置编码方法更加直观，下面的图片展示了相应的效果，每一行是词元所在的位置，四条不同颜色的曲线代表不同维度下的正弦函数。可以看到，选定一个，不同对应的三角函数值是不一样的，也就是说同一个词元，不同维度特征得到的位置编码不同，不同词元之间的位置编码也不尽相同。\n\n相对位置信息​    上述所作的编码，是绝对位置编码。如果已知任何确定的位置偏移，位置处的位置编码可以线性投影到位置处的位置编码。\n令，那么任何一对都可以投影到，具体公式可见：很明显，这是通过上面的2×2投影矩阵做到的，这个矩阵不依赖于任何位置的索引。\n多头注意力​    自注意力机制的缺陷就是：模型在对当前位置的信息进行编码时，会过度的将注意力集中于自身的位置，因此作者提出了通过多头注意力机制来解决这一问题。\n\n在实践中，当给定相同的查询、键和值的集合时， 我们希望模型可以基于相同的注意力机制学习到不同的行为， 然后将不同的行为作为知识组合起来， 捕获序列内各种范围的依赖关系 （例如，短距离依赖和长距离依赖关系）。 因此，允许注意力机制组合使用查询、键和值的不同 子空间表示（ ）可能是有益的。\n为此，与其只使用单独一个注意力汇聚， 我们可以用独立学习得到的ℎ组不同的 线性投影（ ）来变换查询、键和值。 然后，这ℎ组变换后的查询、键和值将并行地送到注意力汇聚中。 最后，将这ℎ个注意力汇聚的输出拼接在一起， 并且通过另一个可以学习的线性投影进行变换， 以产生最终输出。 这种设计被称为多头注意力（ ） 对于ℎ个注意力汇聚输出，每一个注意力汇聚都被称作一个头（）。 展示了使用全连接层来实现可学习的线性变换的多头注意力。\n\n\n给定查询、键和值，每个注意力头(i = 1, 2, …, h)的计算方法为：​    其中，可学习的参数为、和以及注意力汇聚函数。可以是缩放点积注意力，也可以是加性注意力。\n多头注意力的输出是多个头经过连结，再经过一个线性转换（全连接层）的结果，输出为：其中，是可学习参数。\n​    在实现的过程中，通常用缩放点积注意力作为每一个注意力头。令，这时候就可以实现个头的并行计算。我们来看代码如何实现，以促进理解。在实现中，就是参数。\nclass MultiHeadAttention(nn.Module):    \"\"\"多头注意力\"\"\"    def __init__(self, key_size, query_size, value_size, num_hiddens,                 num_heads, dropout, bias=False, **kwargs):        #   num_hiddens是词元特征数        super(MultiHeadAttention, self).__init__(**kwargs)        self.num_heads = num_heads        self.attention = d2l.DotProductAttention(dropout)        self.W_q = nn.Linear(query_size, num_hiddens, bias=bias)        #   把查询的特征数转换为词元特征数        self.W_k = nn.Linear(key_size, num_hiddens, bias=bias)        #   把键的特征数转换为词元特征数        self.W_v = nn.Linear(value_size, num_hiddens, bias=bias)        #   把值的特征数转换为词元特征数        self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias)        #   把多个头的特征数转换为输出特征数    def forward(self, queries, keys, values, valid_lens):        # queries，keys，values的形状:        # (batch_size，查询或者“键－值”对的个数，num_hiddens)        # valid_lens　的形状:        # (batch_size，)或(batch_size，查询的个数)        # 经过变换后，输出的queries，keys，values　的形状:        # (batch_size*num_heads，查询或者“键－值”对的个数，        # num_hiddens/num_heads)        queries = transpose_qkv(self.W_q(queries), self.num_heads)        keys = transpose_qkv(self.W_k(keys), self.num_heads)        values = transpose_qkv(self.W_v(values), self.num_heads)        if valid_lens is not None:            # 在轴0，将第一项（标量或者矢量）复制num_heads次，            # 然后如此复制第二项，然后诸如此类。            valid_lens = torch.repeat_interleave(                valid_lens, repeats=self.num_heads, dim=0)        # output的形状:(batch_size*num_heads，查询的个数，        # num_hiddens/num_heads)        output = self.attention(queries, keys, values, valid_lens)        # output_concat的形状:(batch_size，查询的个数，num_hiddens)        output_concat = transpose_output(output, self.num_heads)        return self.W_o(output_concat)\n​    多个头的并行计算是通过下面这两个转置函数实现的。transpose_output函数反转了transpose_qkv函数的操作。\ndef transpose_qkv(X, num_heads):    \"\"\"为了多注意力头的并行计算而变换形状\"\"\"    # 输入X的形状:(batch_size，查询或者“键－值”对的个数，num_hiddens)    # 输出X的形状:(batch_size，查询或者“键－值”对的个数，num_heads，    # num_hiddens/num_heads)    X = X.reshape(X.shape[0], X.shape[1], num_heads, -1)    # 输出X的形状:(batch_size，num_heads，查询或者“键－值”对的个数,    # num_hiddens/num_heads)    X = X.permute(0, 2, 1, 3)    # 最终输出的形状:(batch_size*num_heads,查询或者“键－值”对的个数,    # num_hiddens/num_heads)    return X.reshape(-1, X.shape[2], X.shape[3])def transpose_output(X, num_heads):    \"\"\"逆转transpose_qkv函数的操作\"\"\"    X = X.reshape(-1, num_heads, X.shape[1], X.shape[2])    X = X.permute(0, 2, 1, 3)    return X.reshape(X.shape[0], X.shape[1], -1)\n​    很容易看到，经过多头注意力后的输出形状并未改变，与查询、键和值的形状相同，均为(batch_size，查询或者“键－值”对的个数，)。\nTransformer​    Transformer完全基于注意力机制，没有任何卷积层或循环神经网络层。\n​    尽管Transformer最初是应用于在文本数据上的序列到序列学习，但现在已经推广到各种现代的深度学习中，例如语言、视觉、语音和强化学习领域。\n模型整体架构如下图：\n​    Transformer是由基于自注意力模块叠加而成的编码器和解码器组成的。源序列和目标序列的嵌入将加上位置编码，再分别输入打编码器和解码器中。\n​    从宏观角度来看，Transformer的编码器由多个相同的块叠加而成，共有n层。每个块都有两个子层（后续用表示子层）。第一个子层是多头自注意力汇聚。第二个子层是基于位置的前馈网络。\n​    更具体来说，编码器的自注意力的查询、键和值都是来自前一个编码器块的输出。每个子层都采用了残差连接和层规范化。\n​    输入序列的每一个词元经过编码器的一层后的输出也是一个维的向量。\n​    Transformer的解码器也是由多个相同的块叠加而成的，并且块中使用了残差连接和层规范化。然而，除了编码器中描述的两个子层之外，解码器还在这两个子层之间插入第三个子层，称为编码器-解码器注意力层。在这个层中，查询来自前一个解码器层的输出，而键和值来自整个编码器的输出。\n​    解码器解码的每个词元都只能考虑该词元之前的所有词元，而不能考虑此词元往后的词元，这称为掩蔽注意力，这可以确保预测仅依赖于已生成的输出词元。\n基于位置的前馈网络​    本质上，基于位置的前馈网络是在对一个序列的所有词元使用同一个多层感知机进行变换。\n​    在下面的实现中，输入X的形状（批量大小，时间步数或序列长度，ffn_num_input）将被一个两层的感知机转换成形状为（批量大小，时间步数或序列长度，ffn_num_outputs）的输出张量。\nclass PositionWiseFFN(nn.Module):    #   \"\"\"基于位置的前馈网络\"\"\"    def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs,                 **kwargs):        super(PositionWiseFFN, self).__init__(**kwargs)        self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)        self.relu = nn.ReLU()        self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs)    def forward(self, X):        return self.dense2(self.relu(self.dense1(X)))\n​    这里插述一下关于nn.Linear的操作。\n​    众所周知，nn.Linear就是一个单层感知机，参数是(input, output)，通常这个参数就是Linear的权重矩阵。\n​    对于一个二维的输入，input是输入特征的维度，也就是矩阵的最后一维。对一张图片来说（形状为h*w*c），特征维是通道c。output是该层感知机的隐藏单元数。单层感知机的操作就是把二维输入最后一维的维数转换为隐藏单元数，而其他的维度都视为样本数。比如输入矩阵形状为(x，y)，单层感知机的隐藏单元数为z，也就是说Linear的参数设置为(y，z)。于是，经过Linear的处理，结果为(x，z)。\n​    对于一个三维或者三维以上的输入，同理，它只将最后一维视为特征维进行变换，而其他维统一视作样本数。比如，一个形状为(batch_size，valid_lens，size)的输入矩阵，Linear首先会把它看成(batch_size*valid_lens，size)的二维矩阵，然后再进行变换，变换为(batch_size*valid_lens，num_hiddens)。\n​    可以观察到，Linear不会变化样本维，只会变化特征维。俗称，改变张量的最里层维度的尺寸。\n残差连接和层规范化​    残差连接被认为是深度网络的必备技术，提出者是何凯明，其论文出处：\n​    Deep Residual Learning for Image Recognition (thecvf.com)    \n​    层规范化同批量规范化一样，都是正则化的重要手段，但是在自然语言处理中，层规范化的效果要优于批量规范化，层规范化和批量规范化的区别在这篇博客中解释地非常到位，在此不再赘述：\n​    BatchNorm和LayerNorm——通俗易懂的理解-CSDN博客\n代码实现如下：\nclass AddNorm(nn.Module):    \"\"\"残差连接后进行层规范化\"\"\"    def __init__(self, normalized_shape, dropout, **kwargs):        super(AddNorm, self).__init__(**kwargs)        self.dropout = nn.Dropout(dropout)        #   随机丢弃数据的一些特征，但总体形状不变        self.ln = nn.LayerNorm(normalized_shape)        #   LayerNorm的输入参数是[句子长度，每个单词的特征维度数]，很明显LayerNorm不改变输入样本的特征    def forward(self, X, Y):        return self.ln(self.dropout(Y) + X)\n编码器有了前面的铺垫，下面可以来实现编码器中的一个块：\nclass EncoderBlock(nn.Module):    #   \"\"\"Transformer编码器块\"\"\"    #   这只是编码器的一个块    # Transformer编码器中的任何块都不会改变其输入的形状    def __init__(self, key_size, query_size, value_size, num_hiddens,                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,                 dropout, use_bias=False, **kwargs):        super(EncoderBlock, self).__init__(**kwargs)        self.attention = d2l.MultiHeadAttention(            key_size, query_size, value_size, num_hiddens, num_heads, dropout,            use_bias)        self.addnorm1 = AddNorm(norm_shape, dropout)        self.ffn = PositionWiseFFN(            ffn_num_input, ffn_num_hiddens, num_hiddens)        self.addnorm2 = AddNorm(norm_shape, dropout)    def forward(self, X, valid_lens):        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))        return self.addnorm2(Y, self.ffn(Y))\n测试一下：\nX = torch.ones((2, 100, 24))valid_lens = torch.tensor([3, 2])encoder_blk = EncoderBlock(24, 24, 24, 24, [100, 24], 24, 48, 8, 0.5)encoder_blk.eval()print(encoder_blk(X, valid_lens).shape)\n结果：\ntorch.Size([2, 100, 24])\n然后，可以根据这个编码器的基本块，堆叠n个，实现最后的编码器：\nclass TransformerEncoder(d2l.Encoder):    # \"\"\"Transformer编码器\"\"\"    def __init__(self, vocab_size, key_size, query_size, value_size,                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,                 num_heads, num_layers, dropout, use_bias=False, **kwargs):        super(TransformerEncoder, self).__init__(**kwargs)        self.num_hiddens = num_hiddens        self.embedding = nn.Embedding(vocab_size, num_hiddens)        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)        self.blks = nn.Sequential()        for i in range(num_layers):            self.blks.add_module(\"block\" + str(i),                                 EncoderBlock(key_size, query_size, value_size, num_hiddens,                                              norm_shape, ffn_num_input, ffn_num_hiddens,                                              num_heads, dropout, use_bias))    def forward(self, X, valid_lens, *args):        # 因为位置编码值在-1和1之间，        # 因此嵌入值乘以嵌入维度的平方根进行缩放，        # 然后再与位置编码相加。        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))        for i, blk in enumerate(self.blks):            X = blk(X, valid_lens)            self.attention_weights[i] = blk.attention.attention.attention_weights        return X\n​    测试一下：\nencoder = TransformerEncoder(    200, 24, 24, 24, 24, [100, 24], 24, 48, 8, 2, 0.5)encoder.eval()valid_lens = torch.tensor([3, 2])print(encoder(torch.ones((2, 100), dtype=torch.long), valid_lens).shape)\n​    结果：\ntorch.Size([2, 100, 24])\n解码器​    Transformer解码器也是由多个相同的块组成。每个块包含三个子层：解码器自注意力、“编码器-解码器”注意力和基于位置的前馈网络。这些子层也都被残差连接和层规范化围绕。\n​    前面说到了掩蔽多头解码器的自注意力层（第一个子层）中，查询、键和值都来自上一个解码器的输出。我们知道，在序列到序列模型中，训练阶段的输出序列所有位置（时间步）的词元都是已知的；然而，在预测阶段，其输出序列的词元是逐个生成的。因此，为了在解码器中保留这个自回归属性，其掩蔽多头解码器的自注意力层设置了参数dec_valid_lens，以便任何查询，都只会与解码器中所有已经生成词元的位置（即直到该查询位置为止）进行注意力计算。\n​    下面搭建一个解码器块：\nclass DecoderBlock(nn.Module):    # \"\"\"解码器中第i个块\"\"\"    def __init__(self, key_size, query_size, value_size, num_hiddens,                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,                 dropout, i, **kwargs):        super(DecoderBlock, self).__init__(**kwargs)        self.i = i        self.attention1 = d2l.MultiHeadAttention(            key_size, query_size, value_size, num_hiddens, num_heads, dropout)        self.addnorm1 = AddNorm(norm_shape, dropout)        self.attention2 = d2l.MultiHeadAttention(            key_size, query_size, value_size, num_hiddens, num_heads, dropout)        self.addnorm2 = AddNorm(norm_shape, dropout)        self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens,                                   num_hiddens)        self.addnorm3 = AddNorm(norm_shape, dropout)    def forward(self, X, state):        enc_outputs, enc_valid_lens = state[0], state[1]        # 训练阶段，输出序列的所有词元都在同一时间处理，        # 因此state[2][self.i]初始化为None。        # 预测阶段，输出序列是通过词元一个接着一个解码的，        # 因此state[2][self.i]包含着直到当前时间步第i个块解码的输出表示，也就是上一个解码器块的输出        # 第i个解码器块位于第i个时间步        if state[2][self.i] is None:            key_values = X        else:            key_values = torch.cat((state[2][self.i], X), axis=1)        state[2][self.i] = key_values        if self.training:            batch_size, num_steps, _ = X.shape            # dec_valid_lens的开头:(batch_size,num_steps),            # 其中每一行是[1,2,...,num_steps]            # 从这里可以知道，valid_lens是所有词元数            dec_valid_lens = torch.arange(                1, num_steps + 1, device=X.device).repeat(batch_size, 1)            # 产生一个(batch_size, num_steps)形状的dec_valid_lens        else:            dec_valid_lens = None        # 自注意力        X2 = self.attention1(X, key_values, key_values, dec_valid_lens)        Y = self.addnorm1(X, X2)        # 编码器－解码器注意力。        # enc_outputs的开头:(batch_size,num_steps,num_hiddens)        Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens)        Z = self.addnorm2(Y, Y2)        return self.addnorm3(Z, self.ffn(Z)), state\n测试一下：\ndecoder_blk = DecoderBlock(24, 24, 24, 24, [100, 24], 24, 48, 8, 0.5, 0)decoder_blk.eval()X = torch.ones((2, 100, 24))state = [encoder_blk(X, valid_lens), valid_lens, [None]]print(decoder_blk(X, state)[0].shape)\n结果：\ntorch.Size([2, 100, 24])\n现在，我们可以构建由num_layers个块组成的完整的解码器。最后通过一个全连接层计算所有vocab_size个可能的输出词元的预测值。\nclass TransformerDecoder(d2l.AttentionDecoder):    def __init__(self, vocab_size, key_size, query_size, value_size,                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,                 num_heads, num_layers, dropout, **kwargs):        super(TransformerDecoder, self).__init__(**kwargs)        self.num_hiddens = num_hiddens        self.num_layers = num_layers        self.embedding = nn.Embedding(vocab_size, num_hiddens)        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)        self.blks = nn.Sequential()        for i in range(num_layers):            self.blks.add_module(\"block\" + str(i),                                 DecoderBlock(key_size, query_size, value_size, num_hiddens,                                              norm_shape, ffn_num_input, ffn_num_hiddens,                                              num_heads, dropout, i))        self.dense = nn.Linear(num_hiddens, vocab_size)        # 用Linear转化为词元特征维度，输出。    def init_state(self, enc_outputs, enc_valid_lens, *args):        return [enc_outputs, enc_valid_lens, [None] * self.num_layers]    def forward(self, X, state):        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))        self._attention_weights = [[None] * len(self.blks) for _ in range(2)]        for i, blk in enumerate(self.blks):            X, state = blk(X, state)        return self.dense(X), state\n训练与预测\n提示：需要安装下载d2l包。导入torch和torch.nn是必要的。\n\n下面训练Transformer。\n\n注：BLEU分数是机器翻译的评价标准，在0~1区间内，越接近1，越准确。\n\nnum_hiddens, num_layers, dropout, batch_size, num_steps = 32, 2, 0.1, 64, 10lr, num_epochs, device = 0.005, 200, d2l.try_gpu()ffn_num_input, ffn_num_hiddens, num_heads = 32, 64, 4key_size, query_size, value_size = 32, 32, 32norm_shape = [32]train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)encoder = TransformerEncoder(    len(src_vocab), key_size, query_size, value_size, num_hiddens,    norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,    num_layers, dropout)decoder = TransformerDecoder(    len(tgt_vocab), key_size, query_size, value_size, num_hiddens,    norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,    num_layers, dropout)net = d2l.EncoderDecoder(encoder, decoder)d2l.train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)\n训练结果：\nloss 0.032, 5679.3 tokens/sec on cuda:0&lt;Figure size 350x250 with 1 Axes&gt;\n训练结束后，将一些英语句子翻译成法语，并且计算它们的BLEU分数。\nengs = ['go .', \"i lost .\", 'he\\'s calm .', 'i\\'m home .']fras = ['va !', 'j\\'ai perdu .', 'il est calme .', 'je suis chez moi .']for eng, fra in zip(engs, fras):    translation, dec_attention_weight_seq = d2l.predict_seq2seq(        net, eng, src_vocab, tgt_vocab, num_steps, device, True)    print(f'{eng} =&gt; {translation}, ',          f'bleu {d2l.bleu(translation, fra, k=2):.3f}')\n结果：\ngo . =&gt; va !,  bleu 1.000i lost . =&gt; j'ai perdu .,  bleu 1.000he's calm . =&gt; il est calme .,  bleu 1.000i'm home . =&gt; je suis chez moi .,  bleu 1.000\n\n本文毕。\n\n","categories":["科研"],"tags":["深度学习"]},{"title":"Retinexformer论文精读笔记","url":"/2024/02/24/Retinexformer%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E7%AC%94%E8%AE%B0/","content":"Retinexformer论文精读笔记\n论文为2023年ICCV的Retinexformer: One-stage Retinex-based Transformer for Low-light Image Enhancement。论文链接：browse.arxiv.org/pdf/2303.06705.pdf，代码链接：caiyuanhao1998/Retinexformer: “Retinexformer: One-stage Retinex-based Transformer for Low-light Image Enhancement” (ICCV 2023) (github.com)\n这篇论文在不少数据集上刷了SOTA。\n\n[TOC]\n出发点\nIn addition, these CNN-based methods show limitations in capturing long-range dependencies and non-local self- similarity, which are critical for image restoration.\n\n​    出发点1：低光增强领域很多深度学习方法都应用CNN，因此缺少图片的长程依赖关系，作者于是想要在低光处理中运用Transformer以解决这个问题，实现效果的提升。这其实也并不稀罕，也已经出现过许多基于Transformer的模型，或者hybrid版本的模型。\n\nThese CNNs are first trained independently and then connected together to be finetuned end-to-end. The training process is tedious and time-consuming.\n\n​    出发点2：作者看到很多基于Retinex的深度学习方法依赖于多个pipeline，无法实施真正的一步处理，于是作者想要通过一步处理以实现低光增强。\n\nThe computational complexity is quadratic to the in- put spatial size. This computational cost may be unaffordable. Due to this limitation, some CNN-Transformer hybrid algorithms like SNR-Net only employ a single global Transformer layer at the lowest spatial resolution of a U-shaped CNN. Thus, the potential of Transformer for low- light image enhancement still remains under-explored.\n\n​    出发点3：最近的基于注意力的低光增强方法，并不纯粹地应用Transformer，而是CNN与Transformer的混合体，这主要源于Transformer的计算复杂度是序列的平方。这就是为什么SNR-Net只在最低分辨率应用一次全局Transformer，这就限制了Transformer的威力，作者想要解决这个问题。这个是我认为的最核心的一个出发点，作者应该就是得到SNR-Net的启发并进行思考而做的研究。\n模型\n​    下面是冗杂的流程详述及论文许多没提到的实现细节。\n​    先说整体。整个模型框架分为两个部分，一部分是ORF，一部分是IGT。ORF对应于(a)图左侧(i)部分，用于光照估计illumination Estimator。IGT对应于(a)图右侧(ii)部分，用于去噪Corruption Restorer。上图还有两个部分，即(b)与(c)。(c)是IG-MSA，是光照引导的多头注意力。(c)含在(b)当中，(b)是IGAB，是一个Transformer的Encoder架构。\n​    首先一张低光图片输入ORF，同时计算其光照先验Lp，两者做一个concatenation送入一个1×1卷积层，一个9×9的卷积层。9×9的卷积层输出提亮后的图片特征Light-up Feature，然后一方面将这个Light-up Feature做保存以后面使用，另一方面继续经过一个1×1的卷积层处理，变换通道数为3，输出为用于提亮的Lit-up Map，即论文中的。然后输入I与元素乘积，得到提亮后的图片。\n​    IGT类似于U-Net。下采样阶段的第一步，经过卷积层（Embedding）处理为可供Transformer处理的图片序列（详见ViT论文）。第二步，经过一个IGAB处理。在代码中，IGAB接收两个参数，一个是图片序列，一个是光照特征引导Light-up Feature，这两个参数的形状需要保持一致，除此之外，用了一个列表存储不同形状图片序列对应的Light-up Feature，在这一步，直接存储Light-up Feature，同时还用了一个列表存储不同形状的图片序列，这都是为了满足后面IG-MSA的处理。第三步，经过卷积层，图片序列特征由C变成2C，在代码中，作者也将Light-up Feature喂给，使得Light-up Feature形状与图片序列保持一致，特征也为2C，然后存储到前面提到的列表中，作者在论文中没有提到，我也是看代码才进一步认识的。后面是类似的，经过两个IGAB，一个卷积层最后得到特征数为4C的图片序列。\n​    IGT的底层是两个IGAB，输入的图片序列特征数是4C，经过处理图片序列特征数不变。\n​    上采样阶段。首先经过卷积层，输出特征不变为4C。第二步，将之前下采样阶段用列表存储的不同形状的图片序列拿出来，把与现在的图片序列A形状匹配的图片序列B，两者进行concatenation，得到特征为2C的图片序列。第三步，经过卷积层处理，输出特征数不变的图片序列，大小为2C。第四步，经过2个IGAB处理，处理细节是：将之前下采样阶段用列表存储的不同形状图片序列对应的Light-up Feature拿出来，把形状与现在图片序列相同的一个，共同作为IGAB的两个参数，输出特征数不变，为2C。后面是类似的。最后，经过Embedding层映射为3维的图片，然后与经残差连接而来的原始提亮图片相加，得到最后处理完的去噪图片。\n​    IGAB类似Transformer的Encoder，由两个LayerNorm，一个前馈网络FFN和核心组件IG-MSA组成，输入与输出的形状一样，即IGAB同样地不改变图片序列的形状。IG-MSA接收两个参数，一个是输入的图片序列，一个是光照特征引导Light-up Feature。两者维度一致，假设为H×W×C。首先两者都reshape为HW×C。送入多头注意力，处理为query，key，value。value单独拿出来与前面的Light-up Feature元素乘积为A，此即为光照引导的注意力。query与key进行矩阵乘法，得到C×C的矩阵B，然后与A做矩阵乘法，送入全连接层进行映射，转换为H×W×C形状的输出，最后加入一个位置信息Position Encoding即可。\n创新点\nWe formulate a one-stage Retinex-based low-light enhancement framework, ORF, that enjoys an easy one- stage training process and models the corruptions well.\n\n​    创新点1：从模型框架图可以知道，作者基于Retinex理论，但是并不在模型中直接将图片I分解为反射量R与光照L，这个是一方面。另一方面，从模型图可以看到，模型分为两部分，ORF(i)与IGT(ii)，ORF的输出结果为light-up image与light-up feature，这两个都将继续作为IGT的输入，ORF与IGT是一个整体，实现了end-to-end，one-stage的效果。这跟我以往看到的双网络架构（一个用于R，一个用于L）确实不一样。\n​    创新点2：Retinex理论将图片分解为R与L，即。作者引入两个扰动项与。分别代表着反射与照度两方面的噪音与伪影，而以往的方法只考虑一个，而没有同时考虑两个。这里而且。 R表示一个干净的图片。\n​    作者在模型中也没有直接分解图片为R与L，而是直接估计，然后将低光图片I与元素乘积，得到含伪影与噪音的提亮图片：\n这里是因为才有了上面的等式。然后提亮后的图片就成为了：这里代表提亮后的图片，即模型图的lit-up image，而且则可以认为是全部的扰动项，也就是噪声。所以IGT对上面的C进行去除，得到干净图片R。\n​    创新点3：提亮后图片的特征往往包含许多纹理信息，这可以应用到去噪中：\n\nRegions with better lighting conditions can provide semantic contextual representations to help enhance the dark regions. Thus, we use the light-up feature Fluen- coding illumination information and interactions of regions with different lighting conditions to direct the computation of self-attention. \n\n所以，作者将light-up feature作为Transformer中Encoder的引导，就有了照度引导的Transformer。这感觉是有点低光增强色彩的Transformer。\n​    创新点4：在IGT部分可以看到，作者将Transformer的Encoder应用到 U-Net的每一层中，U-Net不仅仅只有CNN。这对于目前的我来说应该算是比较新颖的。\n​    创新点5：作者指出自己的模型计算复杂度是序列长度的线性倍，不是传统Transformer的平方倍。\n​    按照他的说法以及矩阵乘法计算复杂度的公式，确实是序列长度的线性倍。这里用的注意力评分函数与缩放点积注意力评分函数不太一样。具体而言，缩放点积注意力是：而作者使用的评分函数是：这个细微的差别，导致了所计算出来的时间复杂度是序列（HW）的线性倍，即：而原始的缩放点积注意力下的Transformer时间复杂度是序列HW的平方。这就是他说计算复杂度是序列的线性倍的原因。\n\n我没有了解过作者使用的这个注意力评分函数，他在代码里面也是使用这个没见过的评分函数，其与缩放点积注意力的差别只在两个矩阵做乘法的顺序，但是得出的结果是与缩放点积注意力有很大差别的。\n如果，那么传统缩放点积注意力，而作者的注意力是，在实际中，HW是序列长度，人为设计的d是缩放因子，往往d远小于HW，最终计算出来的结果就避免了序列的平方倍。\n作者代码也是使用这个注意力函数，这么看来也是故意为之，论文公式也是用的，然而也没有提供相关文献索引。鉴于自己知识浅薄，尚未知道原因，大佬可补充。\n\n​    于是这个模型能够在U-Net的每一层中使用，而非只在最低分辨率用。这就提高了计算速度。\n总结​    这篇2023年低光增强的sota文，令Transformer与U-Net有机结合，使得提亮后图片的伪影与噪声得到很好的去除。通过直接估计照度而非直接分解低光图片为照度L与反射R，实现one-stage的增强。其为Transformer引入了光照引导，利用其纹理特征提升去噪能力，提高了Transformer在低光增强的应用效率。\n​    个人在创新点5关于其计算复杂度呈序列线性倍留了一个疑惑，欢迎大佬交流。\n​    这篇文章，新颖度，有效性，问题大小满分都为100分的话，个人认为，新颖度：10分，有效性：100分，问题大小：10分，所以价值可以认为是10000分，这在低光增强这样的low-level视觉已经是相当大的贡献。\n","categories":["科研"],"tags":["low-light image enhancement","笔记"]},{"title":"EnlightenGAN论文阅读笔记","url":"/2024/02/24/EnlightenGAN%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E7%AC%94%E8%AE%B0/","content":"EnlightenGAN论文阅读笔记\n论文是2019年IEEE的EnlightenGAN: Deep Light Enhancement without Paired Supervision.这篇论文是低光增强领域无监督学习的开山之作。\n论文链接如下：arxiv.org/pdf/1906.06972.pdf\n\n[TOC]\n出发点出发点1：从监督学习的缺点入手。\nit is very difficult or even impractical to simultaneously capture corrupted and ground truth images of the same visual scene.\n\n​    指出在低光增强领域，监督学习的第一个不足之处——在同一个场景下同时获得亮度正常的图片和低光图片是很难而且不现实的。\n\nsynthesizing corrupted images from clean images could sometimes help, but such synthesized results are usually not photo-realistic enough.\n\n​    监督学习有时候会使用合成的低光图片进行训练，也就是拍摄正常光照的图片后，经过模糊、加噪音等，合成低光图片，作为一对数据进行训练。作者指出这种方式并不足够真实。算是第二个不足之处。\n​    我的理解是低光图片的合成是人为控制的，因此合成的低光程度不一样得到的效果也不同。\n\nspecifically for the low-light enhancement problem, there may be no unique or well-defined high-light ground truth given a low-light image.\n\n​    作者指出低光领域监督学习的第三个不足之处，在于低光图片的ground-truth并不唯一。某个场景晚上的图片，它的ground-truth可以是该场景在白天的任何时候。也就是说，为一张低光图片配对一个绝对的正常光照图片是没有必要的。\n出发点2：从拍摄所得低光图片的特点入手。\nTaking into account the above issues, our overarching goal is to enhance a low-light photo with spatially varying light conditions and over/under-exposure artifacts, while the paired training data is unavailable.\n\n​    作者认为，低光图片的增强需要考虑到图片光照条件的空间变化。也就是说，一张图片拍摄后，不同地方的光照不同，那么进行低光增强时，不同位置的增强的程度也不同，如果一张图片给所有地方增强程度相同，就有可能出现过度曝光而失真的情况。\n创新点创新点1：在低光增强中第一次引入双重判别器。\nWe first propose a dual- discriminator to balance global and local low-light enhancement.\n\n创新点2：引入一个自特征保留损失。\nFurther, owing to the absence of ground-truth supervision, a self-regularized perceptual loss is proposed to constrain the feature distance between the low-light input image and its enhanced version, which is subsequently adopted both locally and globally together with the adversarial loss for training EnlightenGAN.\n\n创新点3：开发原始低光输入的Attention Map引导生成。\nWe also propose to exploit the illumination information of the low-light input as a self-regularized attentional map in each level of deep features to regularize the unsupervised learning.\n\n模型\n​    框架：作者使用U-Net作为Generator，在其中对每层加入了注意力map以引导生成。使用了两个Discriminator，均为PatchGAN设计。一个是Global Discriminator，一个是Local Discriminator。\n\nBy extracting multi-level features from different depth layers, U-Net pre- serves rich texture information and synthesizes high quality images using multi-scale context information.\nPatchGAN是一种设计，最早出现于CircleGAN模型的判别器中。它是原来GAN的一种延申与升级。PatchGAN与感受野有关，具体在Patch GAN的理解_patchgan判别器-CSDN博客这篇博客中有详述。\n\n​    Generator详解：输入的低光RGB图片取出其照明通道I，然后归一化为[0， 1]，然后利用1-I（元素差异）作为该层的attention map。然后这层的低光RGB图片经过卷积与最大值汇聚得到下一层的RGB图片。然后此RGB图片取出其照明通道I，这个照明通道同样地归一化为[0， 1]，利用1-I（元素差异）作为该层的attention map。这一层的RGB图像继续通过卷积与最大值汇聚得到下一层的RGB图片，以此类推。直到RGB图片只经过卷积得到最后一层的RGB图片，然后经过该层的attention map（元素乘法），然后通过上采样层Upsampling Layer和卷积层得到上一层的RGB图片，以此类推，最顶层的RGB图片经过上采样与卷积，与最顶层的attention map元素乘法，再与残差连接的原始低光图片相加，得到增强光亮后的输出。\n​    值得一提的是，模型中的Upsampling Layer并非简单的一个转置卷积，而是一个双线性上采样层，以减轻伪影。\n​    Discriminator详解：我的理解是，Global Discriminator对整张输出图片和原始低光图片判别，Local Discriminator在正常低光图片和增强图片分别随机地裁剪局部块进行判别。两个Discriminator都进行判断输出图片来自真实图片还是增强图片。\n设计及其损失1.双重判别器（Dual Discriminator）及其损失\nwe observe that an image-level vanilla discriminator often fails on spatially-varying light images; if the input image has some local area that needs to be enhanced differently from other parts, e.g., a small bright region in an overall dark background, the global image discriminator alone is often unable to provide the desired adaptivity.\n\n​    拍摄的图片的光照是存在空间变化的，如果有一些部位需要特别地增强，而有一些部位相对于其他部位又很亮，那么单纯用一个全局的判别器总是做的很失败。所以作者引入全局-局部判别器，以解决局部过度增强以及局部增强不足的情况。\n​    对于全局判别器的损失，首先基于Relativistic discriminator的损失结构，标准的判别器损失为：\n\nD_{Ra}(x_r, x_f) = \\sigma(C(x_r) - \\mathbb{R}_{x_f \\sim \\mathbb{P}_{fake}}[C(x_f)]),\\tag{1}\nD_{Ra}(x_f, x_r) = \\sigma(C(x_f) - \\mathbb{E}_{x_r \\sim \\mathbb{P}_{real}}[C(x_r)]),\\tag{2}然后将换为LSGAN中的损失函数，最后得到全局判别器D和全局生成器G的损失：\n\nL^{Global}_D = \\mathbb{E}_{x_r \\sim \\mathbb{P}_{real}}[(D_{Ra}(x_r, x_f) - 1)^2] + \\mathbb{E}_{x_f \\sim \\mathbb{P}_{fake}}[D_{Ra}(x_f, x_r)^2],\\tag{3}\nL_G^{Global} = \\mathbb{E}_{x_f \\sim \\mathbb{P}_{fake}}[(D_{Ra}(x_f, x_r) - 1)^2] + \\mathbb{E}_{x_r \\sim \\mathbb{P}_{real}}[D_{Ra}(x_r, x_f)^2],\\tag{4}​    对于局部判别器的损失，直接引用LSGAN的对抗性损失：\n\nL_D^{Local} = \\mathbb{E}_{x_r \\sim \\mathbb{P}_{real-patches}}[(D(x_r) - 1)^2] + \\mathbb{E}_{x_f \\sim \\mathbb{P}_{fake-patches}}[(D(x_f) - 0)^2],\\tag{5}\nL_G^{Local} = \\mathbb{E}_{x_r \\sim \\mathbb{P}_{fake-patches}}[(D(x_f) - 1)^2],\\tag{6}2.自特征保留损失（Self Feature Preserving Loss）\n感知损失：Johnson等人提出感知损失，通常的做法是通过预训练的VGG去抽取输出图片和真实标签图片的特征，然后限制这两组特征的距离。\n\n​    基于感知损失，作者提供了无监督学习版的感知损失：并非限制输出与真实标签特征的距离，而是限制输入与输出特征的距离。\n\nIn our unpaired setting, we propose to instead constrain the VGG-feature distance between the input low-light and its enhanced normal-light output.\n\n损失函数如下：\n\nL_{SFP}(I^L) = \\frac{1}{W_{i, j}H{i, j}} \\sum_{x = 1}^{W_{i, j}}{\\sum_{y = 1}^{H_{i, j}}{(\\phi _{i, j}(I^L) - \\phi_{i, j}(G(I^L)))^2}}, \\tag{7}符号描述如下：\n\nwhere denotes the input low-light image and  denotes the generator’s enhanced output. denotes the feature map extracted from a VGG-16 model pre-trained on ImageNet. i represents its i-th max pooling, and j represents its j-th convolutional layer after i-th max pooling layer. and  are the dimensions of the extracted feature maps. By default we choose i = 5, j = 1.\n\n下面这段话说明，自特征保留损失作用于全局判别器，也作用于局部判别器：\n\nFor our local discriminator, the cropped local patches from input and output images are also regularized by a similarly defined self feature preserving loss.\n\n总损失\nLoss = L_{SFP}^{Global} + L_{SFP}^{Local} + L_G^{Global} + L_G^{Local}, \\tag{8}自正则Attention Map在前面的模型中，已经解释了Attention Map的机理：\n\nWe take the illumination channel I of the input RGB image, normalize it to [0,1], and then use 1−I (element-wise difference) as our self-regularized attention map. We then resize the attention map to fit each feature map and multiply it with all intermediate feature maps as well as the output image.\n\n每一层的Attention Map要Resize为与该层RGB特征图片形状一致，这样才能进行元素乘法。\n作者指出，Attention Map也是一个自正则化的手段，而且是该工作中非常关键的一步。\n\n本文毕\n\n","categories":["科研"],"tags":["low-light image enhancement","笔记"]},{"title":"大二下整体规划","url":"/2024/02/27/%E5%A4%A7%E4%BA%8C%E4%B8%8B%E6%95%B4%E4%BD%93%E8%A7%84%E5%88%92/","content":"\n      \n        c5eb05fb20e6d4861fb9830397f1764bc005d05d90fc0bdc35239fb2c74af9b7b3781cabc1187ebcae56577e2198c200bb3af778f06346bb55ba04200c8320c4fe3a0c9689525bb7bcdec8d656a564e3f81b18baba9bd756677f6ac384ec025f8321542a530a5a8565e2a7d921178192a12e8c0b0aaf493be4e469588f87333d3da28bc453ecf265ae5f741447015adc6a77ae152121a860342a92b1d9385ec9f2064c944c8809cdb4a458230e12de3449516299777e3ac02014f73953f71d4947173db1ef07e94600f1abe15ccccfccba77613f7359c602b584f93f2b71e0a4e6b1744c92c0fbed9949d3777b451b2d256f3c9e8531e2b087ac716d6468202b24601b5541faa36d177421d3793e35e67fe560674e3988e26299c997480ce88cb5329edbdec9ab4f840e949b8e2d9269\n      \n      \n        \n          \n          \n            请输入与 Rhodes Island™ 取得弱神经连接时的口令：\n          \n        \n        \n      \n    \n    ","categories":["规划"],"tags":["规划"]}]