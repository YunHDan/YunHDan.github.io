<!-- build time:Mon Jun 23 2025 12:47:23 GMT+0800 (中国标准时间) --><!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#FFF"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/ico" sizes="32x32" href="/images/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="alternate" type="application/rss+xml" title="枯萎的花将在另一彼岸悄然绽放" href="https://yunhdan.github.io/rss.xml"><link rel="alternate" type="application/atom+xml" title="枯萎的花将在另一彼岸悄然绽放" href="https://yunhdan.github.io/atom.xml"><link rel="alternate" type="application/json" title="枯萎的花将在另一彼岸悄然绽放" href="https://yunhdan.github.io/feed.json"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CInconsolata:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/css/app.css?v=0.2.5"><link rel="canonical" href="https://yunhdan.github.io/research/EnlightenGAN%E8%AE%BA%E6%96%87%E8%AF%A6%E8%A7%A3/"><title>EnlightenGAN论文详解 - 学术 | Runfar's Zone = 枯萎的花将在另一彼岸悄然绽放</title><meta name="generator" content="Hexo 6.3.0"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">EnlightenGAN论文详解</h1><div class="meta"><span class="item" title="创建时间：2024-02-24 00:00:00"><span class="icon"><i class="ic i-calendar"></i> </span><span class="text">发表于</span> <time itemprop="dateCreated datePublished" datetime="2024-02-24T00:00:00+08:00">2024-02-24</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span class="text">本文字数</span> <span>5.3k</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span class="text">阅读时长</span> <span>5 分钟</span></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="切换导航栏"><span class="line"></span> <span class="line"></span> <span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">Runfar's Zone</a></li></ul><ul class="right"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div id="imgs" class="pjax"><ul><li class="item" data-background-image="https://s2.loli.net/2025/05/18/TFaWz4Eix8QuHZj.jpg"></li><li class="item" data-background-image="https://s2.loli.net/2024/12/04/6buchRFIEO4qjsa.jpg"></li><li class="item" data-background-image="https://s2.loli.net/2024/12/04/5zWRn6vaogI3yKM.png"></li><li class="item" data-background-image="https://s2.loli.net/2024/12/04/kMOUAgCQ12iVlmB.png"></li><li class="item" data-background-image="https://s2.loli.net/2024/12/07/WK8pgGa4EM5nBNX.jpg"></li><li class="item" data-background-image="https://s2.loli.net/2024/12/04/C5clUonWQVD8sKO.jpg"></li></ul></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"/></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"/><use xlink:href="#gentle-wave" x="48" y="3"/><use xlink:href="#gentle-wave" x="48" y="5"/><use xlink:href="#gentle-wave" x="48" y="7"/></g></svg></div><main><div class="inner"><div id="main" class="pjax"><div class="article wrap"><div class="breadcrumb" itemscope itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i> <span><a href="/">首页</a></span><i class="ic i-angle-right"></i> <span class="current" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/research/" itemprop="item" rel="index" title="分类于 学术"><span itemprop="name">学术</span></a><meta itemprop="position" content="1"></span></div><article itemscope itemtype="http://schema.org/Article" class="post block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://yunhdan.github.io/research/EnlightenGAN%E8%AE%BA%E6%96%87%E8%AF%A6%E8%A7%A3/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/me.jpg"><meta itemprop="name" content="Runhua Deng"><meta itemprop="description" content=", 计算机视觉 & 图像恢复"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="枯萎的花将在另一彼岸悄然绽放"></span><div class="body md" itemprop="articleBody"><h1 id="EnlightenGAN论文详解"><a href="#EnlightenGAN论文详解" class="headerlink" title="EnlightenGAN论文详解"></a>EnlightenGAN论文详解</h1><blockquote><p>论文是2019年IEEE的EnlightenGAN: Deep Light Enhancement without Paired Supervision.这篇论文是低光增强领域无监督学习的开山之作。</p><p>论文链接如下：<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE5MDYuMDY5NzIucGRm">arxiv.org/pdf/1906.06972.pdf</span></p></blockquote><h2 id="出发点"><a href="#出发点" class="headerlink" title="出发点"></a>出发点</h2><h3 id="出发点1：从监督学习的缺点入手。"><a href="#出发点1：从监督学习的缺点入手。" class="headerlink" title="出发点1：从监督学习的缺点入手。"></a><strong>出发点1</strong>：从监督学习的缺点入手。</h3><blockquote><p>it is very difficult or even impractical to simultaneously capture corrupted and ground truth images of the same visual scene.</p></blockquote><p>​ 指出在低光增强领域，监督学习的第一个不足之处——在同一个场景下同时获得亮度正常的图片和低光图片是很难而且不现实的。</p><blockquote><p>synthesizing corrupted images from clean images could sometimes help, but such synthesized results are usually not photo-realistic enough.</p></blockquote><p>​ 监督学习有时候会使用合成的低光图片进行训练，也就是拍摄正常光照的图片后，经过模糊、加噪音等，合成低光图片，作为一对数据进行训练。作者指出这种方式并不足够真实。算是第二个不足之处。</p><p>​ 我的理解是低光图片的合成是人为控制的，因此合成的低光程度不一样得到的效果也不同。</p><blockquote><p>specifically for the low-light enhancement problem, there may be no unique or well-defined high-light ground truth given a low-light image.</p></blockquote><p>​ 作者指出低光领域监督学习的第三个不足之处，在于低光图片的ground-truth并不唯一。某个场景晚上的图片，它的ground-truth可以是该场景在白天的任何时候。也就是说，为一张低光图片配对一个绝对的正常光照图片是没有必要的。</p><h3 id="出发点2：从拍摄所得低光图片的特点入手。"><a href="#出发点2：从拍摄所得低光图片的特点入手。" class="headerlink" title="出发点2：从拍摄所得低光图片的特点入手。"></a><strong>出发点2</strong>：从拍摄所得低光图片的特点入手。</h3><blockquote><p>Taking into account the above issues, our overarching goal is to enhance a low-light photo with spatially varying light conditions and over/under-exposure artifacts, while the paired training data is unavailable.</p></blockquote><p>​ 作者认为，低光图片的增强需要考虑到图片光照条件的空间变化。也就是说，一张图片拍摄后，不同地方的光照不同，那么进行低光增强时，不同位置的增强的程度也不同，如果一张图片给所有地方增强程度相同，就有可能出现过度曝光而失真的情况。</p><h2 id="创新点"><a href="#创新点" class="headerlink" title="创新点"></a>创新点</h2><h3 id="创新点1：在低光增强中第一次引入双重判别器。"><a href="#创新点1：在低光增强中第一次引入双重判别器。" class="headerlink" title="创新点1：在低光增强中第一次引入双重判别器。"></a><strong>创新点1</strong>：在低光增强中第一次引入双重判别器。</h3><blockquote><p>We first propose a dual- discriminator to balance global and local low-light enhancement.</p></blockquote><h3 id="创新点2：引入一个自特征保留损失。"><a href="#创新点2：引入一个自特征保留损失。" class="headerlink" title="创新点2：引入一个自特征保留损失。"></a><strong>创新点2</strong>：引入一个自特征保留损失。</h3><blockquote><p>Further, owing to the absence of ground-truth supervision, a self-regularized perceptual loss is proposed to constrain the feature distance between the low-light input image and its enhanced version, which is subsequently adopted both locally and globally together with the adversarial loss for training EnlightenGAN.</p></blockquote><h3 id="创新点3：开发原始低光输入的Attention-Map引导生成。"><a href="#创新点3：开发原始低光输入的Attention-Map引导生成。" class="headerlink" title="创新点3：开发原始低光输入的Attention Map引导生成。"></a><strong>创新点3</strong>：开发原始低光输入的Attention Map引导生成。</h3><blockquote><p>We also propose to exploit the illumination information of the low-light input as a self-regularized attentional map in each level of deep features to regularize the unsupervised learning.</p></blockquote><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p><img data-src="../../assets/enlightengan.png" alt="iamge"></p><p>​ <strong>框架</strong>：作者使用U-Net作为Generator，在其中对每层加入了注意力map以引导生成。使用了两个Discriminator，均为PatchGAN设计。一个是Global Discriminator，一个是Local Discriminator。</p><blockquote><p>By extracting multi-level features from different depth layers, U-Net pre- serves rich texture information and synthesizes high quality images using multi-scale context information.</p><p>PatchGAN是一种设计，最早出现于CircleGAN模型的判别器中。它是原来GAN的一种延申与升级。PatchGAN与感受野有关，具体在<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xhbmRpbmdfZ3V5Xy9hcnRpY2xlL2RldGFpbHMvMTIxMjE0ODA4">Patch GAN的理解_patchgan判别器-CSDN博客</span>这篇博客中有详述。</p></blockquote><p>​ <strong>Generator详解</strong>：输入的低光RGB图片取出其照明通道I，然后归一化为[0， 1]，然后利用1-I（元素差异）作为该层的attention map。然后这层的低光RGB图片经过卷积与最大值汇聚得到下一层的RGB图片。然后此RGB图片取出其照明通道I，这个照明通道同样地归一化为[0， 1]，利用1-I（元素差异）作为该层的attention map。这一层的RGB图像继续通过卷积与最大值汇聚得到下一层的RGB图片，以此类推。直到RGB图片只经过卷积得到最后一层的RGB图片，然后经过该层的attention map（元素乘法），然后通过上采样层Upsampling Layer和卷积层得到上一层的RGB图片，以此类推，最顶层的RGB图片经过上采样与卷积，与最顶层的attention map元素乘法，再与残差连接的原始低光图片相加，得到增强光亮后的输出。</p><p>​ 值得一提的是，模型中的Upsampling Layer并非简单的一个转置卷积，而是一个双线性上采样层，以减轻伪影。</p><p>​ <strong>Discriminator详解</strong>：我的理解是，Global Discriminator对整张输出图片和原始低光图片判别，Local Discriminator在正常低光图片和增强图片分别随机地裁剪局部块进行判别。两个Discriminator都进行判断输出图片来自真实图片还是增强图片。</p><h2 id="设计及其损失"><a href="#设计及其损失" class="headerlink" title="设计及其损失"></a>设计及其损失</h2><h3 id="1-双重判别器（Dual-Discriminator）及其损失"><a href="#1-双重判别器（Dual-Discriminator）及其损失" class="headerlink" title="1.双重判别器（Dual Discriminator）及其损失"></a>1.双重判别器（Dual Discriminator）及其损失</h3><blockquote><p>we observe that an image-level vanilla discriminator often fails on spatially-varying light images; if the input image has some local area that needs to be enhanced differently from other parts, e.g., a small bright region in an overall dark background, the global image discriminator alone is often unable to provide the desired adaptivity.</p></blockquote><p>​ 拍摄的图片的光照是存在空间变化的，如果有一些部位需要特别地增强，而有一些部位相对于其他部位又很亮，那么单纯用一个全局的判别器总是做的很失败。所以作者引入全局-局部判别器，以解决局部过度增强以及局部增强不足的情况。</p><p>​ 对于全局判别器的损失，首先基于Relativistic discriminator的损失结构，标准的判别器损失为：</p><script type="math/tex;mode=display">D_{Ra}(x_r, x_f) = \sigma(C(x_r) - \mathbb{R}_{x_f \sim \mathbb{P}_{fake}}[C(x_f)]),    \\
D_{Ra}(x_f, x_r) = \sigma(C(x_f) - \mathbb{E}_{x_r \sim \mathbb{P}_{real}}[C(x_r)]),\tag{1}</script><p>然后将$\sigma$换为LSGAN中的损失函数，最后得到全局判别器D和全局生成器G的损失：</p><script type="math/tex;mode=display">L^{Global}_D = \mathbb{E}_{x_r \sim \mathbb{P}_{real}}[(D_{Ra}(x_r, x_f) - 1)^2] + \mathbb{E}_{x_f \sim \mathbb{P}_{fake}}[D_{Ra}(x_f, x_r)^2],    \\
L_G^{Global} = \mathbb{E}_{x_f \sim \mathbb{P}_{fake}}[(D_{Ra}(x_f, x_r) - 1)^2] + \mathbb{E}_{x_r \sim \mathbb{P}_{real}}[D_{Ra}(x_r, x_f)^2],\tag{2}</script><p>​ 对于局部判别器的损失，直接引用LSGAN的对抗性损失：</p><script type="math/tex;mode=display">L_D^{Local} = \mathbb{E}_{x_r \sim \mathbb{P}_{real-patches}}[(D(x_r) - 1)^2] + \mathbb{E}_{x_f \sim \mathbb{P}_{fake-patches}}[(D(x_f) - 0)^2],    \\
L_G^{Local} = \mathbb{E}_{x_r \sim \mathbb{P}_{fake-patches}}[(D(x_f) - 1)^2],\tag{3}</script><h3 id="2-自特征保留损失（Self-Feature-Preserving-Loss）"><a href="#2-自特征保留损失（Self-Feature-Preserving-Loss）" class="headerlink" title="2.自特征保留损失（Self Feature Preserving Loss）"></a>2.自特征保留损失（Self Feature Preserving Loss）</h3><blockquote><p>感知损失：Johnson等人提出感知损失，通常的做法是通过预训练的VGG去抽取输出图片和真实标签图片的特征，然后限制这两组特征的距离。</p></blockquote><p>​ 基于感知损失，作者提供了无监督学习版的感知损失：并非限制输出与真实标签特征的距离，而是限制输入与输出特征的距离。</p><blockquote><p>In our unpaired setting, we propose to instead constrain the VGG-feature distance between the input low-light and its enhanced normal-light output.</p></blockquote><p>损失函数如下：</p><script type="math/tex;mode=display">L_{SFP}(I^L) = \frac{1}{W_{i, j}H{i, j}} \sum_{x = 1}^{W_{i, j}}{\sum_{y = 1}^{H_{i, j}}{(\phi _{i, j}(I^L) - \phi_{i, j}(G(I^L)))^2}}, \tag{7}</script><p>符号描述如下：</p><blockquote><p>where $\large{I^L}$denotes the input low-light image and $\large{G(I^L)}$ denotes the generator’s enhanced output. $\phi_{i, j}$denotes the feature map extracted from a VGG-16 model pre-trained on ImageNet. i represents its i-th max pooling, and j represents its j-th convolutional layer after i-th max pooling layer. $\large{W_{i,j}}$and $\large{H_{i,j}}$ are the dimensions of the extracted feature maps. By default we choose i = 5, j = 1.</p></blockquote><p>下面这段话说明，自特征保留损失作用于全局判别器，也作用于局部判别器：</p><blockquote><p>For our local discriminator, the cropped local patches from input and output images are also regularized by a similarly defined self feature preserving loss.</p></blockquote><h3 id="总损失"><a href="#总损失" class="headerlink" title="总损失"></a>总损失</h3><script type="math/tex;mode=display">Loss = L_{SFP}^{Global} + L_{SFP}^{Local} + L_G^{Global} + L_G^{Local}, \tag{8}</script><h3 id="自正则Attention-Map"><a href="#自正则Attention-Map" class="headerlink" title="自正则Attention Map"></a>自正则Attention Map</h3><p>在前面的模型中，已经解释了Attention Map的机理：</p><blockquote><p>We take the illumination channel I of the input RGB image, normalize it to [0,1], and then use 1−I (element-wise difference) as our self-regularized attention map. We then resize the attention map to fit each feature map and multiply it with all intermediate feature maps as well as the output image.</p></blockquote><p>每一层的Attention Map要Resize为与该层RGB特征图片形状一致，这样才能进行元素乘法。</p><p>作者指出，Attention Map也是一个自正则化的手段，而且是该工作中非常关键的一步。</p><blockquote><p>本文毕</p></blockquote></div><footer><div class="meta"><span class="item"><span class="icon"><i class="ic i-calendar-check"></i> </span><span class="text">更新于</span> <time title="修改时间：2024-12-04 20:45:44" itemprop="dateModified" datetime="2024-12-04T20:45:44+08:00">2024-12-04</time> </span><span id="research/EnlightenGAN论文详解/" class="item leancloud_visitors" data-flag-title="EnlightenGAN论文详解" title="阅读次数"><span class="icon"><i class="ic i-eye"></i> </span><span class="text">阅读次数</span> <span class="leancloud-visitors-count"></span> <span class="text">次</span></span></div><div class="reward"><button><i class="ic i-heartbeat"></i> 赞赏</button><p>请我喝[茶]~(￣▽￣)~*</p><div id="qr"><div><img data-src="/images/wechatpay.jpg" alt="Runhua Deng 微信支付"><p>微信支付</p></div><div><img data-src="/images/alipay.png" alt="Runhua Deng alipay"><p>alipay</p></div><div><img data-src="/images/paypal.png" alt="Runhua Deng paypal"><p>paypal</p></div></div></div><div id="copyright"><ul><li class="author"><strong>本文作者： </strong>Runhua Deng <i class="ic i-at"><em>@</em></i>枯萎的花将在另一彼岸悄然绽放</li><li class="link"><strong>本文链接：</strong> <a href="https://yunhdan.github.io/research/EnlightenGAN%E8%AE%BA%E6%96%87%E8%AF%A6%E8%A7%A3/" title="EnlightenGAN论文详解">https://yunhdan.github.io/research/EnlightenGAN论文详解/</a></li><li class="license"><strong>版权声明： </strong>本站所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</span> 许可协议。转载请注明出处！</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/research/Retinexformer%E8%AE%BA%E6%96%87%E8%AF%A6%E8%A7%A3/" itemprop="url" rel="prev" data-background-image="https:&#x2F;&#x2F;s2.loli.net&#x2F;2024&#x2F;12&#x2F;07&#x2F;Qzbi1Wys9Kxg5qL.jpg" title="Retinexformer论文详解"><span class="type">上一篇</span> <span class="category"><i class="ic i-flag"></i> 学术</span><h3>Retinexformer论文详解</h3></a></div><div class="item right"><a href="/%E5%85%AC%E5%91%8A/" itemprop="url" rel="next" data-background-image="https:&#x2F;&#x2F;s2.loli.net&#x2F;2025&#x2F;05&#x2F;18&#x2F;hdkJKAHiDeOgMtN.jpg" title="公告"><span class="type">下一篇</span> <span class="category"><i class="ic i-flag"></i></span><h3>公告</h3></a></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="文章目录"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#EnlightenGAN%E8%AE%BA%E6%96%87%E8%AF%A6%E8%A7%A3"><span class="toc-number">1.</span> <span class="toc-text">EnlightenGAN论文详解</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%87%BA%E5%8F%91%E7%82%B9"><span class="toc-number">1.1.</span> <span class="toc-text">出发点</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%87%BA%E5%8F%91%E7%82%B91%EF%BC%9A%E4%BB%8E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%BC%BA%E7%82%B9%E5%85%A5%E6%89%8B%E3%80%82"><span class="toc-number">1.1.1.</span> <span class="toc-text">出发点1：从监督学习的缺点入手。</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%87%BA%E5%8F%91%E7%82%B92%EF%BC%9A%E4%BB%8E%E6%8B%8D%E6%91%84%E6%89%80%E5%BE%97%E4%BD%8E%E5%85%89%E5%9B%BE%E7%89%87%E7%9A%84%E7%89%B9%E7%82%B9%E5%85%A5%E6%89%8B%E3%80%82"><span class="toc-number">1.1.2.</span> <span class="toc-text">出发点2：从拍摄所得低光图片的特点入手。</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E6%96%B0%E7%82%B9"><span class="toc-number">1.2.</span> <span class="toc-text">创新点</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E6%96%B0%E7%82%B91%EF%BC%9A%E5%9C%A8%E4%BD%8E%E5%85%89%E5%A2%9E%E5%BC%BA%E4%B8%AD%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%BC%95%E5%85%A5%E5%8F%8C%E9%87%8D%E5%88%A4%E5%88%AB%E5%99%A8%E3%80%82"><span class="toc-number">1.2.1.</span> <span class="toc-text">创新点1：在低光增强中第一次引入双重判别器。</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E6%96%B0%E7%82%B92%EF%BC%9A%E5%BC%95%E5%85%A5%E4%B8%80%E4%B8%AA%E8%87%AA%E7%89%B9%E5%BE%81%E4%BF%9D%E7%95%99%E6%8D%9F%E5%A4%B1%E3%80%82"><span class="toc-number">1.2.2.</span> <span class="toc-text">创新点2：引入一个自特征保留损失。</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E6%96%B0%E7%82%B93%EF%BC%9A%E5%BC%80%E5%8F%91%E5%8E%9F%E5%A7%8B%E4%BD%8E%E5%85%89%E8%BE%93%E5%85%A5%E7%9A%84Attention-Map%E5%BC%95%E5%AF%BC%E7%94%9F%E6%88%90%E3%80%82"><span class="toc-number">1.2.3.</span> <span class="toc-text">创新点3：开发原始低光输入的Attention Map引导生成。</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.3.</span> <span class="toc-text">模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%BE%E8%AE%A1%E5%8F%8A%E5%85%B6%E6%8D%9F%E5%A4%B1"><span class="toc-number">1.4.</span> <span class="toc-text">设计及其损失</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E5%8F%8C%E9%87%8D%E5%88%A4%E5%88%AB%E5%99%A8%EF%BC%88Dual-Discriminator%EF%BC%89%E5%8F%8A%E5%85%B6%E6%8D%9F%E5%A4%B1"><span class="toc-number">1.4.1.</span> <span class="toc-text">1.双重判别器（Dual Discriminator）及其损失</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E8%87%AA%E7%89%B9%E5%BE%81%E4%BF%9D%E7%95%99%E6%8D%9F%E5%A4%B1%EF%BC%88Self-Feature-Preserving-Loss%EF%BC%89"><span class="toc-number">1.4.2.</span> <span class="toc-text">2.自特征保留损失（Self Feature Preserving Loss）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E6%8D%9F%E5%A4%B1"><span class="toc-number">1.4.3.</span> <span class="toc-text">总损失</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E6%AD%A3%E5%88%99Attention-Map"><span class="toc-number">1.4.4.</span> <span class="toc-text">自正则Attention Map</span></a></li></ol></li></ol></li></ol></div><div class="related panel pjax" data-title="系列文章"><ul><li><a href="/research/PairLIE%E8%AE%BA%E6%96%87%E8%AF%A6%E8%A7%A3/" rel="bookmark" title="PairLIE论文详解">PairLIE论文详解</a></li><li class="active"><a href="/research/EnlightenGAN%E8%AE%BA%E6%96%87%E8%AF%A6%E8%A7%A3/" rel="bookmark" title="EnlightenGAN论文详解">EnlightenGAN论文详解</a></li><li><a href="/research/Retinexformer%E8%AE%BA%E6%96%87%E8%AF%A6%E8%A7%A3/" rel="bookmark" title="Retinexformer论文详解">Retinexformer论文详解</a></li><li><a href="/research/Mamba%E4%B8%B2%E7%83%A7/" rel="bookmark" title="Mamba串烧">Mamba串烧</a></li><li><a href="/research/Low%20Level%20Vision%E8%AE%BA%E6%96%87%E7%B2%BE%E7%82%BC/" rel="bookmark" title="Low Level Vision论文精炼">Low Level Vision论文精炼</a></li><li><a href="/research/%E5%85%B7%E8%BA%AB%E6%99%BA%E8%83%BD%E5%9F%BA%E7%A1%80%E6%8A%80%E6%9C%AF/" rel="bookmark" title="具身智能基础技术">具身智能基础技术</a></li><li><a href="/research/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%AE%BA%E6%96%87%E7%B2%BE%E7%82%BC/" rel="bookmark" title="多模态论文精炼">多模态论文精炼</a></li></ul></div><div class="overview panel" data-title="站点概览"><div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="image" itemprop="image" alt="Runhua Deng" data-src="/images/me.jpg"><p class="name" itemprop="name">Runhua Deng</p><div class="description" itemprop="description">计算机视觉 & 图像恢复</div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">39</span> <span class="name">文章</span></a></div><div class="item categories"><a href="/categories/"><span class="count">7</span> <span class="name">分类</span></a></div></nav><div class="social"><span class="exturl item github" data-url="aHR0cHM6Ly9naXRodWIuY29tL1l1bkhEYW4=" title="https:&#x2F;&#x2F;github.com&#x2F;YunHDan"><i class="ic i-github"></i></span> <span class="exturl item music" data-url="aHR0cHM6Ly9tdXNpYy4xNjMuY29tLyMvdXNlci9ob21lP2lkPTIxMjYzMjE4OTI=" title="https:&#x2F;&#x2F;music.163.com&#x2F;#&#x2F;user&#x2F;home?id&#x3D;2126321892"><i class="ic i-cloud-music"></i></span> <a href="/alex2312666252@gmail.com" title="alex2312666252@gmail.com" class="item email"><i class="ic i-envelope"></i></a> <span class="exturl item csdn" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L20wXzczNTk5NzM4P3NwbT0xMDAwLjIxMTUuMzAwMS41MzQz" title="https:&#x2F;&#x2F;blog.csdn.net&#x2F;m0_73599738?spm&#x3D;1000.2115.3001.5343"><i class="ic i-link"></i></span></div><ul class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>首页</a></li><li class="item dropdown"><a href="/about/" rel="section"><i class="ic i-user"></i>关于</a><ul class="submenu"><li class="item"><a href="/about/me/" rel="section"><i class="ic i-file"></i>简历</a></li><li class="item"><a href="/about/learn_me/" rel="section"><i class="ic i-smile"></i>了解我</a></li></ul></li><li class="item dropdown"><a href="javascript:void(0);"><i class="ic i-feather"></i>文章</a><ul class="submenu"><li class="item"><a href="/archives/" rel="section"><i class="ic i-list-alt"></i>归档</a></li><li class="item"><a href="/categories/" rel="section"><i class="ic i-th"></i>分类</a></li><li class="item"><a href="/tags/" rel="section"><i class="ic i-tags"></i>标签</a></li></ul></li><li class="item"><a href="/friends/" rel="section"><i class="ic i-heart"></i>友達</a></li></ul></div></div></div><ul id="quick"><li class="prev pjax"><a href="/research/Retinexformer%E8%AE%BA%E6%96%87%E8%AF%A6%E8%A7%A3/" rel="prev" title="上一篇"><i class="ic i-chevron-left"></i></a></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/%E5%85%AC%E5%91%8A/" rel="next" title="下一篇"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>随机文章</h2><ul><li class="item"><div class="breadcrumb"><a href="/categories/cs/" title="分类于 计算机">计算机</a></div><span><a href="/cs/Conda%E3%80%81Pip%E3%80%81Cuda/" title="Conda、Pip、Cuda">Conda、Pip、Cuda</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/cs/" title="分类于 计算机">计算机</a></div><span><a href="/cs/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86%E7%90%86%E8%AE%BA/" title="编译原理理论">编译原理理论</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/baoyan/" title="分类于 保研">保研</a></div><span><a href="/baoyan/Low-level-Vision-Group/" title="Low-level-Vision-Group">Low-level-Vision-Group</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/research/" title="分类于 学术">学术</a></div><span><a href="/research/%E5%85%B7%E8%BA%AB%E6%99%BA%E8%83%BD%E5%9F%BA%E7%A1%80%E6%8A%80%E6%9C%AF/" title="具身智能基础技术">具身智能基础技术</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 人工智能">人工智能</a></div><span><a href="/ai/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/" title="深度学习理论">深度学习理论</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/cs/" title="分类于 计算机">计算机</a></div><span><a href="/cs/Vue/" title="Vue">Vue</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/baoyan/" title="分类于 保研">保研</a></div><span><a href="/baoyan/%E4%BF%9D%E7%A0%94%E4%BF%A1%E6%81%AF%E6%B1%87%E6%80%BB/" title="保研信息汇总">保研信息汇总</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/cs/" title="分类于 计算机">计算机</a></div><span><a href="/cs/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/" title="边缘计算">边缘计算</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/cs/" title="分类于 计算机">计算机</a></div><span><a href="/cs/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%90%86%E8%AE%BA2/" title="数据库理论2">数据库理论2</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/research/" title="分类于 学术">学术</a></div><span><a href="/research/Retinexformer%E8%AE%BA%E6%96%87%E8%AF%A6%E8%A7%A3/" title="Retinexformer论文详解">Retinexformer论文详解</a></span></li></ul></div><div><h2>最新评论</h2><ul class="leancloud-recent-comment"></ul></div></div><div class="status"><div class="copyright">&copy; 2023 – <span itemprop="copyrightYear">2025</span> <span class="with-love"><i class="ic i-sakura rotate"></i> </span><span class="author" itemprop="copyrightHolder">Runhua Deng @ Runfar's Zone</span></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-chart-area"></i> </span><span title="站点总字数">194k 字</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="ic i-coffee"></i> </span><span title="站点阅读时长">2:57</span></div><div class="powered-by">基于 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL={path:"research/EnlightenGAN论文详解/",favicon:{show:"（●´3｀●）やれやれだぜ",hide:"(´Д｀)大変だ！"},search:{placeholder:"文章搜索",empty:"关于 「 ${query} 」，什么也没搜到",stats:"${time} ms 内找到 ${hits} 条结果"},valine:!0,copy_tex:!0,katex:!0,fancybox:!0,copyright:'复制成功，转载请遵守 <i class="ic i-creative-commons"></i>BY-NC-SA 协议。',ignores:[function(e){return e.includes("#")},function(e){return new RegExp(LOCAL.path+"$").test(e)}]}</script><script src="https://cdn.polyfill.io/v2/polyfill.js"></script><script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script><script src="/js/app.js?v=0.2.5"></script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({log:!1,pluginJsPath:"lib/",pluginModelPath:"assets/",pluginRootPath:"live2dw/",tagMode:!1})</script></body></html><!-- rebuild by hrmmi -->