<!-- build time:Sun Aug 17 2025 19:29:41 GMT+0800 (中国标准时间) --><!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#FFF"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/ico" sizes="32x32" href="/images/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="alternate" type="application/rss+xml" title="枯萎的花将在另一彼岸悄然绽放" href="https://yunhdan.github.io/rss.xml"><link rel="alternate" type="application/atom+xml" title="枯萎的花将在另一彼岸悄然绽放" href="https://yunhdan.github.io/atom.xml"><link rel="alternate" type="application/json" title="枯萎的花将在另一彼岸悄然绽放" href="https://yunhdan.github.io/feed.json"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CInconsolata:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/css/app.css?v=0.2.5"><link rel="canonical" href="https://yunhdan.github.io/research/Mamba%E4%B8%B2%E7%83%A7/"><title>Mamba串烧 - 学术 | Runfar's Zone = 枯萎的花将在另一彼岸悄然绽放</title><meta name="generator" content="Hexo 6.3.0"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">Mamba串烧</h1><div class="meta"><span class="item" title="创建时间：2024-03-07 00:00:00"><span class="icon"><i class="ic i-calendar"></i> </span><span class="text">发表于</span> <time itemprop="dateCreated datePublished" datetime="2024-03-07T00:00:00+08:00">2024-03-07</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span class="text">本文字数</span> <span>9.4k</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span class="text">阅读时长</span> <span>9 分钟</span></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="切换导航栏"><span class="line"></span> <span class="line"></span> <span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">Runfar's Zone</a></li></ul><ul class="right"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div id="imgs" class="pjax"><ul><li class="item" data-background-image="https://s2.loli.net/2024/12/07/KLgcFPSTG2MfUNz.jpg"></li><li class="item" data-background-image="https://s2.loli.net/2024/12/04/CwaK19SUG28Jskz.jpg"></li><li class="item" data-background-image="https://s2.loli.net/2025/05/18/9av4d6YnlDSHCcN.jpg"></li><li class="item" data-background-image="https://s2.loli.net/2024/12/07/2bjuywCtcRGKhNH.jpg"></li><li class="item" data-background-image="https://s2.loli.net/2024/12/04/YlBRLtmPTfDx9dz.jpg"></li><li class="item" data-background-image="https://s2.loli.net/2024/12/07/JwyO74o96vDn1sW.jpg"></li></ul></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"/></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"/><use xlink:href="#gentle-wave" x="48" y="3"/><use xlink:href="#gentle-wave" x="48" y="5"/><use xlink:href="#gentle-wave" x="48" y="7"/></g></svg></div><main><div class="inner"><div id="main" class="pjax"><div class="article wrap"><div class="breadcrumb" itemscope itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i> <span><a href="/">首页</a></span><i class="ic i-angle-right"></i> <span class="current" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/research/" itemprop="item" rel="index" title="分类于 学术"><span itemprop="name">学术</span></a><meta itemprop="position" content="1"></span></div><article itemscope itemtype="http://schema.org/Article" class="post block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://yunhdan.github.io/research/Mamba%E4%B8%B2%E7%83%A7/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/me.jpg"><meta itemprop="name" content="Runhua Deng"><meta itemprop="description" content=", 计算机视觉 & 图像恢复"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="枯萎的花将在另一彼岸悄然绽放"></span><div class="body md" itemprop="articleBody"><h1 id="mamba串烧"><a class="anchor" href="#mamba串烧">#</a> Mamba 串烧</h1><h2 id="mamba-linear-time-sequence-modeling-with-selective-state-spaces"><a class="anchor" href="#mamba-linear-time-sequence-modeling-with-selective-state-spaces">#</a> Mamba: Linear-Time Sequence Modeling with Selective State Spaces</h2><blockquote><p>本文核心内容与思想改编自该博文中的核心与精华：<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3ZfSlVMWV92L2FydGljbGUvZGV0YWlscy8xMzQ5MjMzMDE=">一文通透想颠覆 Transformer 的 Mamba：从 SSM、HiPPO、S4 到 Mamba_mamba 模型 - CSDN 博客</span>。</p></blockquote><h3 id="状态空间与状态空间模型ssm"><a class="anchor" href="#状态空间与状态空间模型ssm">#</a> 状态空间与状态空间模型 SSM</h3><p>想象一下我们正在穿过一个迷宫，图中每个小框代表迷宫中的一个位置，并附有某个隐式的信息，例如你距离出口有多远。</p><p><img data-src="../../assets/6fb7fc4aa942446ba27a3005c57a144a.png" alt="image"></p><p>​	而上述迷宫可以简化建模为一个 “状态空间表示 state space representation”，每一个小框都含有：你当前所在的位置 (当前状态 current state)、下一步可以去哪里 (未来可能的状态 possible future states)，以及哪些变化会将你带到下一个状态 (向右或向左)。而描述状态的变量 (在我们的示例中为 X 和 Y 坐标以及到出口的距离) 可以表示为 “<em>状态向量 state vectors</em>”。</p><p>一般 SSMs 包括以下组成：</p><ul><li>输入序列 x (t)，比如在迷宫中向左和向下移动。</li><li>潜在状态表示 h (t)，比如距离出口距离和 x/y 坐标。</li><li>输出序列 y (t)，比如再次向左移动以更快到达出口。</li></ul><p>然而，它不使用<strong>离散序列</strong> (如向左移动一次)，而是将<strong>连续序列作为输入</strong>并预测输出序列。</p><p><img data-src="../../assets/97c0cc93f809480cb60b8fe4aad9c2fa.png" alt="image"></p><p>​	SSM 假设系统 (例如在 3D 空间中移动的物体) 可以通过两个方程从其在时间 t 时的状态进行预测「当然，其实下面第一个方程表示成这样可能更好：$$h (t) = Ah (t-1) Bx (t)$$，不然容易引发歧义」。</p><p><img data-src="../../assets/901700cea9474984823b4b83c6f959db.png" alt="image"></p><p>通过求解这些方程，可以根据观察到的数据：输入序列和先前状态，去预测系统的未来状态。</p><h3 id="ssm关键的两个方程状态方程和输出方程"><a class="anchor" href="#ssm关键的两个方程状态方程和输出方程">#</a> SSM 关键的两个方程：状态方程和输出方程</h3><p>总之，SSM 的关键是找到：状态表示 (state representation)$$h (t)$$，以便根据输入序列预测输出序列。</p><p><img data-src="../../assets/39569d2a2e534effbbaac00f3cde7495.png" alt="image"></p><p>而这两个方程也是状态空间模型的核心，且矩阵 <em>A</em>、 <em>B</em>、 <em>C</em>、 <em>D</em> 都是可以学习的参数。</p><p>​	第一个方程：状态方程，矩阵 B 与输入 x (t) 相乘之后，再加上矩阵 A 与<strong>前一个状态</strong> h (t) 相乘的结果<br>换言之，B 矩阵影响输入 x (t)，A 矩阵影响前一个状态 h (t)，而 h (t) 指的是任何给定时间 t 的潜在状态表示 (latent state representation)，而 x (t) 指的是某个输入「** 当然，还是上面那句话，表示成这样更好：$$h (t) = Ah (t-1) Bx (t)$$ **」。</p><p><img data-src="../../assets/2575220139ef4f90a6f880b33d0dd1d4.png" alt="image"></p><p>第二个方程：输出方程，描述了状态如何转换为输出 (通过矩阵 C)，以及输入如何影响输出 (通过矩阵 D)</p><p><img data-src="../../assets/41a7acc5701b47ab86b96d0f856fcab5.png" alt="image"></p><h3 id="建立对ssm中两个核心方程的统一视角"><a class="anchor" href="#建立对ssm中两个核心方程的统一视角">#</a> 建立对 SSM 中两个核心方程的统一视角</h3><p>最终，我们可以通过下图统一这两个方程：</p><p><img data-src="../../assets/db4ed47d5b9745dcae1e0bb2cfbdceb5.png" alt="image"></p><p>为了进一步加深对该图的理解，我们一步一步拆解下</p><ol><li>假设我们有一些输入信号 x (t)，该信号首先乘以矩阵 B。</li></ol><p><img data-src="../../assets/81314632f8134ed3a82f10ca39ddc3e2.png" alt="image"></p><p>2. 上面第一步的结果，加上：上一个状态与矩阵 A 相乘 (<em>矩阵 A 描述了所有内部状态如何连接</em>) 的结果，用来更新状态 state。</p><p><img data-src="../../assets/0f9d625d583c440da274a4cc03a89671.png" alt="image"></p><p>3. 然后，使用矩阵<em> C</em> 来将状态转换为输出。</p><p><img data-src="../../assets/7f0b831f99254a6683c03cf64eb60759.png" alt="image"></p><p>4. 最后，再利用<em>矩阵 D</em> 提供从输入到输出的直接信号，这通常也称为跳跃连接 skip-connection。</p><p><img data-src="../../assets/70e360256ad14a9fb4f2023e6d64fa91.png" alt="image"></p><p>5. 由于<em>矩阵 D</em> 类似于跳跃连接，因此在没有跳跃连接的情况下，SSM 通常被视为如下：</p><p><img data-src="../../assets/ef80819b80084f2f87c8a11f5ff12d77.png" alt="image"></p><p>回到我们的简化视角，现在可以关注只矩阵<em> A</em>、<em>B</em>、<em>C</em> 构建的 SSM 核心。</p><p><img data-src="../../assets/1d9dc55c6ff14811a274ee386dee3b9b.png" alt="image"></p><p>​	总之，这两个方程共同旨在根据观测数据预测系统的状态，且考虑到输入一般都是连续的，因此 SSM 的主要表示是连续时间表示 (continuous-time representation)。</p><h3 id="从ssm到s4的升级"><a class="anchor" href="#从ssm到s4的升级">#</a> 从 SSM 到 S4 的升级</h3><p>用了三步：离散化 SSM、循环 / 卷积表示、基于 HiPPO 处理长序列。</p><h4 id="离散数据的连续化基于零阶保持技术做连续化并采样"><a class="anchor" href="#离散数据的连续化基于零阶保持技术做连续化并采样">#</a> 离散数据的连续化：基于零阶保持技术做连续化并采样</h4><p>​	由于除了连续的输入之外，还会通常碰到离散的输入 (如文本序列)，因此如果模型也能处理离散化数据则再好不过。怎么做到呢？好在 可以利用<em>零阶保持技术 (Zero-order hold technique)</em>。</p><p><img data-src="../../assets/c604c15affe145d38c916371a776ca2b.png" alt="image"></p><ol><li>首先，每次收到离散信号时，我们都会保留其值，直到收到新的离散信号，如此操作导致的结果就是<strong>创建了 SSM 可以使用的连续信号</strong>。</li><li>保持该值的时间由一个新的可学习参数表示，称为<em>步长 (size)——$$\Delta$$</em> ，它代表输入的阶段性保持 (resolution)。</li><li>有了连续的输入信号后，便可以生成连续的输出，并且仅根据输入的时间步长对值进行采样。</li></ol><p><img data-src="../../assets/98c3bea39d5d47f295b5646a44a4449f.png" alt="image"></p><p>这些采样值就是我们的离散输出，且 可以按如下方式做零阶保持</p><p><img data-src="../../assets/9a82ed6712cc46a39f1e17cad09589cc.png" alt="image"></p><p>​	它们共同使我们能够从连续 SSM 转变为离散 SSM，使得不再是<strong>函数到函数 x (t) → y (t)</strong>，而是<strong>序列到序列<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">x_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.58056em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.33610799999999996em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span> → <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">y_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.33610799999999996em"><span style="top:-2.5500000000000003em;margin-left:-.03588em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span></strong>，所以你看到离散化的 SSM 时，不再带参数 t 了</p><p><img data-src="../../assets/d3296186fdea4bb3bbb2942917197348.png" alt="image"></p><p>​	这里，矩阵 A 和 B 现在表示模型的离散参数 (且这里 使用 而不是 来表示离散时间步长)。<br>​	注意：我们在保存时，仍然保存 矩阵 A 的连续形式 (而非离散化版本)，只是在训练过程中，连续表示被离散化 (During training, the continuous representation is discretized)。</p><h4 id="循环结构表示the-recurrent-representation"><a class="anchor" href="#循环结构表示the-recurrent-representation">#</a> 循环结构表示 The Recurrent Representation</h4><p>总之，离散 SSM 允许 可以用离散时间步长重新表述问题。</p><p><img data-src="../../assets/8b7a641e4a90413d990826f2542fad3b.png" alt="image"></p><p>在每个时间步，都会涉及到隐藏状态的更新 (比如 $$h_k$$* 取决于<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><msub><mi>x</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">Bx_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.83333em;vertical-align:-.15em"></span><span class="mord mathnormal" style="margin-right:.05017em">B</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.33610799999999996em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span> 和 $$Ah_{k-1}$$ 的共同作用结果，* 然后通过 $$Ch_k$$ 预测输出 $$y_k$$)。</p><p><img data-src="../../assets/6c0ecd1bc379447cb910a4cc2f7bad8b.png" alt="image"></p><blockquote><p>为方便大家理解其中的细节，我再展开一下 $$y_2$$</p><p><img data-src="../../assets/eq.png" alt="image"></p></blockquote><p>有没有眼前一亮？如此，便可以 RNN 的结构来处理。</p><p><img data-src="../../assets/c08033ddcdc84549b8627a82bb9c3272.png" alt="image"></p><p>然后 可以这样展开 (其中，$$h_k$$<em> 始终是 $$Bx_k$$ 和 $$Ah_{k-1}$$ 的共同作用之下更新的</em>)。</p><p><img data-src="../../assets/44e0cb2cb0ff403e860204c6df15c69f.png" alt="image"></p><h4 id="卷积结构表示the-convolution-representation"><a class="anchor" href="#卷积结构表示the-convolution-representation">#</a> 卷积结构表示 The Convolution Representation</h4><p>在经典的图像识别任务中，我们用过滤器 (* 即卷积核 kernels)* 来导出聚合特征，而 SSM 也可以表示成卷积的形式。</p><p><img data-src="../../assets/cee443116b6a40958c3d3dd00759ecd4.png" alt="image"></p><p>由于我们处理的是文本而不是图像，因此我们需要一维视角</p><p><img data-src="../../assets/4acf72cd781f4f148e88b53c1b4ff990.png" alt="image"></p><p>而用来表示这个 “过滤器” 的内核源自 SSM 公式。</p><p><img data-src="../../assets/c6316218cd2f46098071c9e4b80801b3.png" alt="image"></p><p>1. 与卷积一样，我们可以使用 SSM 内核来检查每组 token 并计算输出</p><p><img data-src="../../assets/86971cbb83d14505a34a992b63753e6f.png" alt="image"></p><p>2. 内核将移动一次以执行下一步的计算。</p><p><img data-src="../../assets/a14afc54921a49ffbe88d2aab2b0a7e6.png" alt="image"></p><p>3. 最后一步，我们可以看到内核的完整效果：</p><p><img data-src="../../assets/3eb104e1035841f2b4150ac09ce03c27.png" alt="image"></p><p>至于上图中的 $$y_2$$ 是咋计算得到的，别忘了我上面推导出来的。</p><p><img data-src="../../assets/eq.png" alt="image"></p><p>​	总结一下，将 SSM 表示为卷积的一个主要好处是它可以像卷积神经网络 CNN 一样进行并行训练。然而，由于内核大小固定，它们的推理不如 RNN 那样快速。</p><p><img data-src="../../assets/12d4519c5a4a45cca4f3b00b059ec7bb.png" alt="image"></p><p>最终，SSM 可以视为从输入信号到输出信号的参数化映射。</p><p>1.SSMs 可以当做是 RNN 与 CNN 的结合「These models can be interpreted as acombination of recurrent neural networks (RNNs) and convolutional neural networks (CNNs)」，即推理用 RNN，训练用 CNN。</p><p><img data-src="../../assets/9421d487c8094867ba5382e75a8c2254.png" alt="image"></p><p>2. 总之，这类模型可以非常高效地计算为递归或卷积，在序列长度上具有线性或近线性缩放 (This class of models can be computed very efficiently as either arecurrence or convolution, with linear or near-linear scaling in sequence length。</p><h4 id="矩阵a的问题与解决策略hippo"><a class="anchor" href="#矩阵a的问题与解决策略hippo">#</a> 矩阵 A 的问题与解决策略 ——HiPPO</h4><p>​	我们之前在循环表示中看到的那样，矩阵 A 捕获先前 previous 状态的信息来构建新状态 (<em>$$h_k = A h_{k-1} + B x_k$$，当 k = 5 时，则有 $$h_5 = A h_{4} + B x_5$$</em>)</p><p><img data-src="../../assets/aac3331c3cd04b0286d2be468873d6b8.png" alt="image"></p><p>其实，某种意义上，算是矩阵 A 产生了隐藏状态 (matrix A produces the hidden state)。</p><p><img data-src="../../assets/d34915d21c464dc3bbe9c1a7b76aec83.png" alt="image"></p><p>​	由于矩阵 A 只记住之前的几个 token 和捕获迄今为止看到的每个 token 之间的区别，特别是在循环表示的上下文中，因为它只回顾以前的状态。</p><p>那么我们怎样才能以保留比较长的 memory 的方式创建矩阵 A 呢？</p><p>​	答案是可以使用 Hungry Hungry Hippo ((High-order Polynomial Projection Operator，简称 H3)，HiPPO 尝试将当前看到的所有输入信号压缩为系数向量 (HiPPO attempts to compress all input signals it has seen thus far into a vector of coefficients)。</p><p><img data-src="../../assets/a94ac9c07a37484ba904caabf50aa88a.png" alt="image"></p><p>​	它使用矩阵 A 构建一个 “可以很好地捕获最近的 token 并衰减旧的 token” 状态表示 (<em>to build a state representation that captures recent tokens well and decays older tokens</em>)，其公式可以表示如下：</p><p><img data-src="../../assets/703af009f166446d9e7a5bd451bafe70.png" alt="image"></p><p>具体表示可以如下图所示：</p><p><img data-src="../../assets/b9da94c9074f4ea19d769ac667e35236.png" alt="image"></p><p>​	正由于 HiPPO 矩阵可以产生一个隐藏状态来记住其历史 (从数学上讲，它是通过跟踪 Legendre polynomial 的系数来实现的，这使得它能够逼近所有以前的历史)，使得在被应用于循环表示和卷积表示中时，可以处理远程依赖性</p><p>​	如此，S4 的定义就出来了：序列的结构化状态空间 ——Structured State Space for Sequences，一类可以有效处理长序列的 SSM。</p><p><img data-src="../../assets/61db745199ed43ed985aad17b3d5b5ce.png" alt="image"></p><h3 id="ssm的问题矩阵参数固定不变无法针对输入做针对性推理"><a class="anchor" href="#ssm的问题矩阵参数固定不变无法针对输入做针对性推理">#</a> SSM 的问题：矩阵参数固定不变，无法针对输入做针对性推理</h3><p>首先，Linear Time Invariance (LTI) 规定 SSM 中的 A、B、C 始终是固定不变的参数。这意味着：</p><p>1. 对于 SSM 生成的每个 token，矩阵 A 、B、C 都是相同的 (regardless of what sequence you give the SSM, the values of A,B,and C remain the same. We have a static representation that is not content-aware)<br>2. 使得 SSM 无法针对输入做针对性的推理「since it treats each token equally as a result of the fixed A, B, and C matrices. This is a problem as we want the SSM to reason about the input (prompt)」</p><p>此外，如下图所示，无论输入 x 是什么，矩阵 B 都保持完全相同，因此与 x 无关。</p><p><img data-src="../../assets/5b0c8ec1c046453499ced2e2bafde6a5.png" alt="image"></p><p>同样，无论输入如何，A 和 C 也保持固定。</p><p><img data-src="../../assets/8878eb69248d4d1c830a60609846eb0d.png" alt="image"></p><h3 id="mamba组成结构与原理简析"><a class="anchor" href="#mamba组成结构与原理简析">#</a> Mamba 组成结构与原理简析</h3><p>​	简言之，Mamba 是一种状态空间模型 (SSM)，建立在更现代的适用于深度学习的结构化 SSM (简称 S6) 基础上，与经典架构 RNN 有相似之处。</p><h4 id="mamba-有选择处理信息-硬件感知算法-更简单ssm架构"><a class="anchor" href="#mamba-有选择处理信息-硬件感知算法-更简单ssm架构">#</a> Mamba = 有选择处理信息 + 硬件感知算法 + 更简单 SSM 架构</h4><p>与先前的研究相比， <code>Mamba</code> 主要有三点创新：</p><p><strong>1. 对输入信息有选择性处理 <code>(Selection Mechanism)</code></strong></p><p>相比 <code>SSM</code> 压缩所有历史记录 (当然， <code>transformer</code> 则是不压缩所有历史记录)， <code>mamba</code> 设计了一个简单的选择机制，通过 “参数化</p><p><code>SSM</code> 的输入”，以便关注或忽略特定的输入。这样一来，模型能够过滤掉与问题无关的信息，并且可以长期记住与问题相关的信息。</p><p><strong>2. 硬件感知的算法 <code>(Hardware-aware Algorithm)</code></strong></p><p>该算法采用 “并行扫描算法” 而非 “卷积” 来进行模型的循环计算，但为了减少 <code>GPU</code> 内存层次结构中不同级别之间的 <code>IO</code> 访问，</p><p>没有具体化扩展状态。</p><p>当然，这点也是受到了 <code>S5(Simplified State Space Layers for Sequence Modeling)</code> 的启发。</p><p><strong>3. 更简单的架构</strong></p><p>将 <code>SSM</code> 架构的设计与 <code>transformer</code> 的 <code>MLP</code> 块合并为一个块 <code>(combining the design of prior SSM architectures with the MLP block of Transformers into a single block)</code> ，来简化过去的深度序列模型架构，从而得到一个包含 <code>selective state space</code> 的架构设计。</p><h4 id="选择性状态空间模型从s4到s6"><a class="anchor" href="#选择性状态空间模型从s4到s6">#</a> 选择性状态空间模型：从 <code>S4</code> 到 <code>S6</code></h4><p>​	作者认为，序列建模的一个基础问题是把上下文压缩成更小的状态 ( <code>We argue that a fundamental problem of sequence modeling is compressing context into a smaller state</code> )，从这个角度来看：</p><ul><li><p>注意力机制虽然有效果但效率不算很高，毕竟其需要显式地存储整个上下文 ( <code>storing the entire context</code> ，也就是 <code>KV</code> 缓存)，直接导致训练和推理消耗算力大。好比， <code>Transformer</code> 就像人类每写一个字之前，都把前面的所有字 + 输入都复习一遍，所以写的慢。</p></li><li><p><code>RNN</code> 的推理和训练效率高，但性能容易受到对上下文压缩程度的限制。</p></li></ul><blockquote><p><code>On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training. However, their effectiveness is limited by how well this state has compressed the context。</code></p></blockquote><p>好比， <code>RNN</code> 每次只参考前面固定的字数 (仔细体会这句话： <code>When generating the output, the RNN only needs to consider the previous hidden state and current input. It prevents recalculating all previous hidden states which is what a Transformer would do</code> )，写的快是快，但容易忘掉更前面的内容。</p><ul><li>而 <code>SMM</code> 的问题在于其中的矩阵 A B C 始终是不变的，无法针对不同的输入针对性的推理，详见上文</li></ul><p><img data-src="../../assets/5b0c8ec1c046453499ced2e2bafde6a5.png" alt="image"></p><ul><li>最终， <code>Mamba</code> 的解决办法是，让模型对信息有选择性处理，可以关注或忽略<strong>特定</strong>的内容，即使状态大小固定也能压缩上下文。好比， <code>Mamba</code> 每次参考前面所有内容的一个概括，越往后写对前面内容概括得越狠，丢掉细节、保留大意。</li></ul><p>总之，序列模型的效率与效果的权衡点在于它们对状态的压缩程度：</p><ul><li><p>高效的模型必须有一个小的状态 (比如 <code>RNN</code> 或 <code>S4</code> )</p></li><li><p>而有效的模型必须有一个包含来自上下文的所有必要信息的状态 (比如 <code>transformer</code> )</p></li></ul><p>而 <code>mamba</code> 为了兼顾效率和效果，选择性的关注必须关注的、过滤掉可以忽略的：</p><p><img data-src="../../assets/6d9309b4a9f241089edb08f6df5acec5.png" alt="image"></p><p>为方便大家理解，再进一步阐述 <code>mamba</code> 与其前身结构化空间模型 <code>S4</code> 的优势。</p><p>首先，在其前身 <code>S4</code> 中，其有 4 个参数 **(∆, A, B, C)**。</p><p><img data-src="../../assets/2159452e3e7148aab51e4d978bc44a97.png" alt="image"></p><p>且它们都是固定的，不随输入变化 (即与输入无关)，这些参数控制了以下两个阶段：<br><img data-src="../../assets/572c09fc3f6147178af7163c41287af9.png" alt="image"></p><ul><li><p><strong>第一阶段 ( <code>1a 1b</code> )</strong>，通常采用固定公式 <code>A = 𝑓𝐴(∆, A)和B = 𝑓𝐵(∆, A, B)</code> ，将 “连续参数” <code>(∆,A,B)</code> 转化为 “离散参数” <code>(A,B)</code> ，其中 <code>(𝑓𝐴, 𝑓𝐵)</code> 称为离散化规则，且可以使用多种规则来实现这一转换。</p><blockquote><p><code>The first stage transforms the “continuous parameters” (∆, A, B) to “discrete parameters” (A, B) through fixed formulas A = 𝑓𝐴(∆, A) and B = 𝑓𝐵(∆, A, B), where the pair (𝑓𝐴, 𝑓𝐵) is called a discretization rule.</code></p></blockquote><p>例如下述方程中定义的零阶保持 ( <code>ZOH</code> )：</p><blockquote><p><code>Various rules can be used such as the zero-order hold (ZOH) defined in equation (4).</code></p></blockquote><p>![image](../../assets/eq (1).png)</p></li><li><p><strong>第二阶段 ( <code>2a 2b</code> ，和 <code>3a 3b</code> )</strong>，在参数由 <code>(∆，A, B, C)</code> 变换为 <code>(A, B, C)</code> 后，模型可以用两种方式计算，即线性递归 (2) 或全局卷积 (3)。</p><blockquote><p><code>After the parameters have been transformed from (∆, A, B, C) ↦ (A, B, C), the model can be computed in two ways, either as a linear recurrence (2) or a global convolution (3)。</code></p></blockquote></li></ul><p>如之前所说的</p><blockquote><p>模型通常使用卷积模式 (3) 可以进行高效的并行化训练「 其中整个输入序列提前看到，为何可以做高效的并行化呢，因为该模式能够绕过状态计算，并实现仅包含 (B, L, D) 的卷积核 (3a)，即 <code>Thus the more efficient convolution mode wasintroduced which could bypass the state computation and materializes a convolution kernel (3a) of only (𝙱, 𝙻, 𝙳)」</code> 。</p></blockquote><blockquote><p>并切换到循环模式 (2) 以高效的自回归推理 (其中输入每次只看到一个时间步)<br><code>the model uses the convolutional mode (3) for efficient parallelizable training (where the whole input sequence is seen ahead of time), and switched into recurrent mode (2) for efficient autoregressive inference (wheret he inputs are seen one timestep at a time).</code></p></blockquote><blockquote><p>下面，再分析下各个变量的含义</p><ul><li>\Delta$$ 一个标量，类似遗忘门 即 <code>data dependent</code> 的 Δ 跟 <code>RNN</code> 的 <code>forget gate</code> 的功能类似 <code>(step size Δ that represents the resolution of the input discretization of SSMs is the principled foundation of heuristic gating mechanisms.)</code><br>B，起到的作用类似于：进 RNN 的 <code>memory</code><br>C，起到的作用类似于：取 RNN 的 <code>memory</code></li></ul><p>咋理解？我拿出上文第二部分的这个图 一摆，就一目了然了。</p><p><img data-src="../../assets/aac3331c3cd04b0286d2be468873d6b8.png" alt="image"></p><p>所以有人说， <code>data dependent</code> 的 $$B/C$$ 的功能跟 RNN 的 <code>input/output gate</code> 类似。</p><ul><li>A，意味着对应这个维度的 <code>SSM</code> 来说，A 在每个 <code>hidden state</code> 维度上的作用可以不相同，起到 <code>multi-scale/fine-grained gating</code> 的作用，这也是 <code>LSTM</code> 网络里面用 <code>element-wise product</code> 的原因。</li></ul></blockquote><p>​	其次，通过之前的讲解，可知 $$\boldsymbol<ruby>A} \in \mathbb{R}<rp>(</rp><rt>{N \times N</rt><rp>)</rp></ruby>, \boldsymbol<ruby>B} \in \mathbb{R}<rp>(</rp><rt>{N \times 1</rt><rp>)</rp></ruby>, \boldsymbol<ruby>C} \in \mathbb{R}<rp>(</rp><rt>{1 \times N</rt><rp>)</rp></ruby>$$ 矩阵都可以由 N 个数字表示 (<em>the $$A ∈ ℝ^<ruby>𝑁×𝑁}$$, $$B ∈ ℝ<rp>(</rp><rt>{𝑁×1</rt><rp>)</rp></ruby>$$ , $$C ∈ ℝ^{1×𝑁}$$ matrices can all be represented by 𝑁 numbers.</em>)</p><p><img data-src="../../assets/7f0b831f99254a6683c03cf64eb60759.png" alt="image"></p><p>​	为了对批量大小为、长度为、具有个通道 (类似 R G B 三个通道) 的输入序列进行操作，SSM 被独立地应用于每个通道 <code>(To operate over an input sequence 𝑥 of batch size 𝐵 and length 𝐿 with 𝐷 channels, the SSM is applied independently to each channel)。</code></p><p><img data-src="../../assets/c736fb4bda884483bf470331b4dea8f7.png" alt="image"></p><p>​	请注意，在这种情况下，每个输入的总隐藏状态具有维，在序列长度上计算它需要时间和内存 (the total hidden state has dimension 𝐷𝑁 per input, and computing it over the sequence length requires 𝑂(𝐵𝐿𝐷𝑁) time and memory)。</p><p>​	最后，在 Mamaba 中，作者让这些参数矩阵、矩阵、成为输入的函数 (即可学习或可训练的)，让模型能够根据输入内容自适应地调整其行为。</p><p><img data-src="../../assets/8b89d699c647459e84b2768fb517dbfc.png" alt="image"></p><p>1. 从 S4 到 S6 的过程中：</p><ul><li><blockquote><p>影响输入的 B 矩阵、影响状态的 C 矩阵的大小从原来的 (D,N)「<em>其中，D 指的是输入向量的维度，比如一个颜色的变量一般有 R G B 三个维度，N</em> <em>指 SSM 的隐藏层维度 hidden dimension，当然 一般设的比较小</em>」。</p></blockquote></li></ul><p><img data-src="../../assets/c2831d5d41df41ff969a093bc92cdaaf.png" alt="image"></p><p>变成了 (B,L,N)「<em>这三个参数分别对应 batch size、sequence length、hidden state size</em>」</p><p><img data-src="../../assets/5bbb4286fd14415e94ede51aa4caf5f8.png" alt="image"></p><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mo>&gt;</mo></mrow><annotation encoding="application/x-tex">-&gt;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.66666em;vertical-align:-.08333em"></span><span class="mord">−</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">&gt;</span></span></span></span> 且 $$\Delta$$ 的大小由原来的 D 变成了 (B,L,D)<br>且每个位置的 B 矩阵、C 矩阵、$$\Delta$$ 都不相同，这意味着对于每个输入 token，现在有不同的 B 矩阵、C 矩阵，可以解决内容感知问题</p><p>进一步，咱们通过</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>s</mi><mi>B</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mo><mi mathvariant="normal">Linear</mi><mo>⁡</mo></mo><mi>N</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">s_{B}(x)=\operatorname{Linear}_{N}(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.32833099999999993em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:.05017em">B</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mop"><span class="mop"><span class="mord mathrm">L</span><span class="mord mathrm">i</span><span class="mord mathrm">n</span><span class="mord mathrm">e</span><span class="mord mathrm">a</span><span class="mord mathrm">r</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.32833099999999993em"><span style="top:-2.5500000000000003em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:.10903em">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span></p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>s</mi><mi>C</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mo><mi mathvariant="normal">Linear</mi><mo>⁡</mo></mo><mi>N</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">s_{C}(x)=\operatorname{Linear}_{N}(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.32833099999999993em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:.07153em">C</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mop"><span class="mop"><span class="mord mathrm">L</span><span class="mord mathrm">i</span><span class="mord mathrm">n</span><span class="mord mathrm">e</span><span class="mord mathrm">a</span><span class="mord mathrm">r</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.32833099999999993em"><span style="top:-2.5500000000000003em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:.10903em">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span></p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>s</mi><mi mathvariant="normal">Δ</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mo><mi mathvariant="normal">Linear</mi><mo>⁡</mo></mo><mi>D</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">s_{\Delta}(x)=\operatorname{Linear}_{D}(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.32833099999999993em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">Δ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mop"><span class="mop"><span class="mord mathrm">L</span><span class="mord mathrm">i</span><span class="mord mathrm">n</span><span class="mord mathrm">e</span><span class="mord mathrm">a</span><span class="mord mathrm">r</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.32833099999999993em"><span style="top:-2.5500000000000003em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:.02778em">D</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span></p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>τ</mi><mi mathvariant="normal">Δ</mi></msub><mo>=</mo><mtext>softplus</mtext></mrow><annotation encoding="application/x-tex">\tau_{\Delta}=\text { softplus }</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.58056em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.1132em">τ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.32833099999999993em"><span style="top:-2.5500000000000003em;margin-left:-.1132em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">Δ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord text"><span class="mord"> softplus</span></span></span></span></span></span></p><p>来逐一将 $$B, C, \Delta$$ 数据依赖化 (data dependent)「<em>其中的这个 $$\text {Linear}_{d}(x)$$ 代表把 D 维的输入向量 x 经过一个线性层映射到 d 维</em>」</p><p>2. 虽然 A 没有变成 data dependent，但是通过 SSM 的离散化操作之后，$$(\bar {A}, \bar {B})$$ 会经过 outer product 变成 (B, L, N, D) 的 data dependent 张量，算是以一种<strong> parameter efficient</strong> 的方式来达到 data dependent 的目的。</p><blockquote><p>总之，Mamba 通过合并输入的序列长度和批量大小来使矩阵 B 和 C，甚至步长 Δ 取决于输入 (<em>其意味着对于每个输入 token，现在有不同的 B 和 C 矩阵，可以解决内容感知问题</em>)，从而达到选择性地选择将哪些内容保留在隐藏状态以及忽略哪些内容的目标。至于步长 Δ，较小的步长 Δ 会也能做到忽略特定单词，而更多地使用先前的上下文，而较大的步长 Δ 会更多地关注输入单词而不是上下文。</p><p><img data-src="../../assets/623c8eacdcde4039ac5e1c89a807d8ed.png" alt="image"></p></blockquote><h4 id="并行扫描parallel-scan算法"><a class="anchor" href="#并行扫描parallel-scan算法">#</a> 并行扫描（parallel scan）算法</h4><p>​	由于 A B C 这些矩阵现在是动态的了，因此无法使用卷积表示来计算它们 (<em>CNN 需要固定的内核</em>)，因此，我们只能使用循环表示，如此也就而失去了卷积提供的并行训练能力。</p><p>so，为了实现并行化，让我们探讨如何使用循环计算输出：</p><h4 id="硬件感知的状态扩展借鉴flash-attention"><a class="anchor" href="#硬件感知的状态扩展借鉴flash-attention">#</a> 硬件感知的状态扩展：借鉴 Flash Attention</h4><h4 id="简化的ssm架构"><a class="anchor" href="#简化的ssm架构">#</a> 简化的 SSM 架构</h4><p>将大多数 SSM 架构比如 H3 的基础块，与现代神经网络比如 transformer 中普遍存在的门控 MLP 相结合，组成新的 Mamba 块，重复这个块，与归一化和残差连接结合，便构成了 Mamba 架构</p><p><img data-src="../../assets/3bc372bc26c341868565adab7563fbda.png" alt="image"></p><p>顺带提一嘴，transformer quality in linear time 以及 mega moving average equipped gated attention 的这两个工作，也用了类似的结</p><p>构：即删除 transformer 的 ffn/glu 结构</p><p>最终流程如下 (图源自 mamba 原论文)</p><p><img data-src="../../assets/9c47d6e186b648e79f22e1e1056e19c3.png" alt="image"></p></div><footer><div class="meta"><span class="item"><span class="icon"><i class="ic i-calendar-check"></i> </span><span class="text">更新于</span> <time title="修改时间：2025-01-06 00:05:38" itemprop="dateModified" datetime="2025-01-06T00:05:38+08:00">2025-01-06</time> </span><span id="research/Mamba串烧/" class="item leancloud_visitors" data-flag-title="Mamba串烧" title="阅读次数"><span class="icon"><i class="ic i-eye"></i> </span><span class="text">阅读次数</span> <span class="leancloud-visitors-count"></span> <span class="text">次</span></span></div><div class="reward"><button><i class="ic i-heartbeat"></i> 赞赏</button><p>请我喝[茶]~(￣▽￣)~*</p><div id="qr"><div><img data-src="/images/wechatpay.jpg" alt="Runhua Deng 微信支付"><p>微信支付</p></div><div><img data-src="/images/alipay.png" alt="Runhua Deng alipay"><p>alipay</p></div><div><img data-src="/images/paypal.png" alt="Runhua Deng paypal"><p>paypal</p></div></div></div><div id="copyright"><ul><li class="author"><strong>本文作者： </strong>Runhua Deng <i class="ic i-at"><em>@</em></i>枯萎的花将在另一彼岸悄然绽放</li><li class="link"><strong>本文链接：</strong> <a href="https://yunhdan.github.io/research/Mamba%E4%B8%B2%E7%83%A7/" title="Mamba串烧">https://yunhdan.github.io/research/Mamba串烧/</a></li><li class="license"><strong>版权声明： </strong>本站所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</span> 许可协议。转载请注明出处！</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/%E5%85%AC%E5%91%8A/" itemprop="url" rel="prev" data-background-image="https:&#x2F;&#x2F;s2.loli.net&#x2F;2024&#x2F;12&#x2F;04&#x2F;2XJnoeYcKy4pS6M.jpg" title="公告"><span class="type">上一篇</span> <span class="category"><i class="ic i-flag"></i></span><h3>公告</h3></a></div><div class="item right"><a href="/book/%E8%A2%AB%E8%AE%A8%E5%8E%8C%E7%9A%84%E5%8B%87%E6%B0%94/" itemprop="url" rel="next" data-background-image="https:&#x2F;&#x2F;s2.loli.net&#x2F;2024&#x2F;12&#x2F;04&#x2F;PxY1sBonTDrU5Xv.jpg" title="《被讨厌的勇气：“自我启发之父”阿德勒的哲学课》"><span class="type">下一篇</span> <span class="category"><i class="ic i-flag"></i> 读书</span><h3>《被讨厌的勇气：“自我启发之父”阿德勒的哲学课》</h3></a></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="文章目录"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#mamba%E4%B8%B2%E7%83%A7"><span class="toc-number">1.</span> <span class="toc-text">Mamba 串烧</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#mamba-linear-time-sequence-modeling-with-selective-state-spaces"><span class="toc-number">1.1.</span> <span class="toc-text">Mamba: Linear-Time Sequence Modeling with Selective State Spaces</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%8A%B6%E6%80%81%E7%A9%BA%E9%97%B4%E4%B8%8E%E7%8A%B6%E6%80%81%E7%A9%BA%E9%97%B4%E6%A8%A1%E5%9E%8Bssm"><span class="toc-number">1.1.1.</span> <span class="toc-text">状态空间与状态空间模型 SSM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ssm%E5%85%B3%E9%94%AE%E7%9A%84%E4%B8%A4%E4%B8%AA%E6%96%B9%E7%A8%8B%E7%8A%B6%E6%80%81%E6%96%B9%E7%A8%8B%E5%92%8C%E8%BE%93%E5%87%BA%E6%96%B9%E7%A8%8B"><span class="toc-number">1.1.2.</span> <span class="toc-text">SSM 关键的两个方程：状态方程和输出方程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BB%BA%E7%AB%8B%E5%AF%B9ssm%E4%B8%AD%E4%B8%A4%E4%B8%AA%E6%A0%B8%E5%BF%83%E6%96%B9%E7%A8%8B%E7%9A%84%E7%BB%9F%E4%B8%80%E8%A7%86%E8%A7%92"><span class="toc-number">1.1.3.</span> <span class="toc-text">建立对 SSM 中两个核心方程的统一视角</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%8Essm%E5%88%B0s4%E7%9A%84%E5%8D%87%E7%BA%A7"><span class="toc-number">1.1.4.</span> <span class="toc-text">从 SSM 到 S4 的升级</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A6%BB%E6%95%A3%E6%95%B0%E6%8D%AE%E7%9A%84%E8%BF%9E%E7%BB%AD%E5%8C%96%E5%9F%BA%E4%BA%8E%E9%9B%B6%E9%98%B6%E4%BF%9D%E6%8C%81%E6%8A%80%E6%9C%AF%E5%81%9A%E8%BF%9E%E7%BB%AD%E5%8C%96%E5%B9%B6%E9%87%87%E6%A0%B7"><span class="toc-number">1.1.4.1.</span> <span class="toc-text">离散数据的连续化：基于零阶保持技术做连续化并采样</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BE%AA%E7%8E%AF%E7%BB%93%E6%9E%84%E8%A1%A8%E7%A4%BAthe-recurrent-representation"><span class="toc-number">1.1.4.2.</span> <span class="toc-text">循环结构表示 The Recurrent Representation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%BB%93%E6%9E%84%E8%A1%A8%E7%A4%BAthe-convolution-representation"><span class="toc-number">1.1.4.3.</span> <span class="toc-text">卷积结构表示 The Convolution Representation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9F%A9%E9%98%B5a%E7%9A%84%E9%97%AE%E9%A2%98%E4%B8%8E%E8%A7%A3%E5%86%B3%E7%AD%96%E7%95%A5hippo"><span class="toc-number">1.1.4.4.</span> <span class="toc-text">矩阵 A 的问题与解决策略 ——HiPPO</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ssm%E7%9A%84%E9%97%AE%E9%A2%98%E7%9F%A9%E9%98%B5%E5%8F%82%E6%95%B0%E5%9B%BA%E5%AE%9A%E4%B8%8D%E5%8F%98%E6%97%A0%E6%B3%95%E9%92%88%E5%AF%B9%E8%BE%93%E5%85%A5%E5%81%9A%E9%92%88%E5%AF%B9%E6%80%A7%E6%8E%A8%E7%90%86"><span class="toc-number">1.1.5.</span> <span class="toc-text">SSM 的问题：矩阵参数固定不变，无法针对输入做针对性推理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#mamba%E7%BB%84%E6%88%90%E7%BB%93%E6%9E%84%E4%B8%8E%E5%8E%9F%E7%90%86%E7%AE%80%E6%9E%90"><span class="toc-number">1.1.6.</span> <span class="toc-text">Mamba 组成结构与原理简析</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#mamba-%E6%9C%89%E9%80%89%E6%8B%A9%E5%A4%84%E7%90%86%E4%BF%A1%E6%81%AF-%E7%A1%AC%E4%BB%B6%E6%84%9F%E7%9F%A5%E7%AE%97%E6%B3%95-%E6%9B%B4%E7%AE%80%E5%8D%95ssm%E6%9E%B6%E6%9E%84"><span class="toc-number">1.1.6.1.</span> <span class="toc-text">Mamba &#x3D; 有选择处理信息 + 硬件感知算法 + 更简单 SSM 架构</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%80%89%E6%8B%A9%E6%80%A7%E7%8A%B6%E6%80%81%E7%A9%BA%E9%97%B4%E6%A8%A1%E5%9E%8B%E4%BB%8Es4%E5%88%B0s6"><span class="toc-number">1.1.6.2.</span> <span class="toc-text">选择性状态空间模型：从 S4 到 S6</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B9%B6%E8%A1%8C%E6%89%AB%E6%8F%8Fparallel-scan%E7%AE%97%E6%B3%95"><span class="toc-number">1.1.6.3.</span> <span class="toc-text">并行扫描（parallel scan）算法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A1%AC%E4%BB%B6%E6%84%9F%E7%9F%A5%E7%9A%84%E7%8A%B6%E6%80%81%E6%89%A9%E5%B1%95%E5%80%9F%E9%89%B4flash-attention"><span class="toc-number">1.1.6.4.</span> <span class="toc-text">硬件感知的状态扩展：借鉴 Flash Attention</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AE%80%E5%8C%96%E7%9A%84ssm%E6%9E%B6%E6%9E%84"><span class="toc-number">1.1.6.5.</span> <span class="toc-text">简化的 SSM 架构</span></a></li></ol></li></ol></li></ol></li></ol></div><div class="related panel pjax" data-title="系列文章"><ul><li><a href="/research/PairLIE%E8%AE%BA%E6%96%87%E8%AF%A6%E8%A7%A3/" rel="bookmark" title="PairLIE论文详解">PairLIE论文详解</a></li><li><a href="/research/EnlightenGAN%E8%AE%BA%E6%96%87%E8%AF%A6%E8%A7%A3/" rel="bookmark" title="EnlightenGAN论文详解">EnlightenGAN论文详解</a></li><li><a href="/research/Retinexformer%E8%AE%BA%E6%96%87%E8%AF%A6%E8%A7%A3/" rel="bookmark" title="Retinexformer论文详解">Retinexformer论文详解</a></li><li class="active"><a href="/research/Mamba%E4%B8%B2%E7%83%A7/" rel="bookmark" title="Mamba串烧">Mamba串烧</a></li><li><a href="/research/Low%20Level%20Vision%E8%AE%BA%E6%96%87%E7%B2%BE%E7%82%BC/" rel="bookmark" title="Low Level Vision论文精炼">Low Level Vision论文精炼</a></li><li><a href="/research/%E5%85%B7%E8%BA%AB%E6%99%BA%E8%83%BD%E5%9F%BA%E7%A1%80%E6%8A%80%E6%9C%AF/" rel="bookmark" title="具身智能基础技术">具身智能基础技术</a></li><li><a href="/research/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%AE%BA%E6%96%87%E7%B2%BE%E7%82%BC/" rel="bookmark" title="多模态论文精炼">多模态论文精炼</a></li><li><a href="/research/2025%20CVPR%20NTIRE%20Image%20Denoise%E6%8A%A5%E5%91%8A%E7%A0%94%E7%A9%B6%E6%80%9D%E8%80%83/" rel="bookmark" title="2025 CVPR NTIRE Image Denoise报告研究思考">2025 CVPR NTIRE Image Denoise报告研究思考</a></li><li><a href="/research/%E6%9C%9F%E5%88%8A%E3%80%81%E4%BC%9A%E8%AE%AE%E6%8A%95%E7%A8%BF%E7%BB%8F%E9%AA%8C/" rel="bookmark" title="期刊、会议投稿经验">期刊、会议投稿经验</a></li></ul></div><div class="overview panel" data-title="站点概览"><div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="image" itemprop="image" alt="Runhua Deng" data-src="/images/me.jpg"><p class="name" itemprop="name">Runhua Deng</p><div class="description" itemprop="description">计算机视觉 & 图像恢复</div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">48</span> <span class="name">文章</span></a></div><div class="item categories"><a href="/categories/"><span class="count">7</span> <span class="name">分类</span></a></div></nav><div class="social"><span class="exturl item github" data-url="aHR0cHM6Ly9naXRodWIuY29tL1l1bkhEYW4=" title="https:&#x2F;&#x2F;github.com&#x2F;YunHDan"><i class="ic i-github"></i></span> <span class="exturl item music" data-url="aHR0cHM6Ly9tdXNpYy4xNjMuY29tLyMvdXNlci9ob21lP2lkPTIxMjYzMjE4OTI=" title="https:&#x2F;&#x2F;music.163.com&#x2F;#&#x2F;user&#x2F;home?id&#x3D;2126321892"><i class="ic i-cloud-music"></i></span> <a href="/alex2312666252@gmail.com" title="alex2312666252@gmail.com" class="item email"><i class="ic i-envelope"></i></a> <span class="exturl item csdn" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L20wXzczNTk5NzM4P3NwbT0xMDAwLjIxMTUuMzAwMS41MzQz" title="https:&#x2F;&#x2F;blog.csdn.net&#x2F;m0_73599738?spm&#x3D;1000.2115.3001.5343"><i class="ic i-link"></i></span></div><ul class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>首页</a></li><li class="item dropdown"><a href="/about/" rel="section"><i class="ic i-user"></i>关于</a><ul class="submenu"><li class="item"><a href="/about/me/" rel="section"><i class="ic i-file"></i>简历</a></li><li class="item"><a href="/about/learn_me/" rel="section"><i class="ic i-smile"></i>了解我</a></li></ul></li><li class="item dropdown"><a href="javascript:void(0);"><i class="ic i-feather"></i>文章</a><ul class="submenu"><li class="item"><a href="/archives/" rel="section"><i class="ic i-list-alt"></i>归档</a></li><li class="item"><a href="/categories/" rel="section"><i class="ic i-th"></i>分类</a></li><li class="item"><a href="/tags/" rel="section"><i class="ic i-tags"></i>标签</a></li></ul></li><li class="item"><a href="/friends/" rel="section"><i class="ic i-heart"></i>友達</a></li></ul></div></div></div><ul id="quick"><li class="prev pjax"><a href="/%E5%85%AC%E5%91%8A/" rel="prev" title="上一篇"><i class="ic i-chevron-left"></i></a></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/book/%E8%A2%AB%E8%AE%A8%E5%8E%8C%E7%9A%84%E5%8B%87%E6%B0%94/" rel="next" title="下一篇"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>随机文章</h2><ul><li class="item"><div class="breadcrumb"><a href="/categories/cs/" title="分类于 计算机">计算机</a></div><span><a href="/cs/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%90%86%E8%AE%BA1/" title="数据库理论1">数据库理论1</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/cs/" title="分类于 计算机">计算机</a></div><span><a href="/cs/Pip%E3%80%81Conda%E3%80%81github%E9%95%9C%E5%83%8F/" title="Pip、Conda、Github镜像">Pip、Conda、Github镜像</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/cs/" title="分类于 计算机">计算机</a></div><span><a href="/cs/Pytorch/" title="Pytorch">Pytorch</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/other/" title="分类于 琐碎">琐碎</a></div><span><a href="/other/%E7%BE%BD%E6%AF%9B%E7%90%832/" title="羽毛球2">羽毛球2</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/cs/" title="分类于 计算机">计算机</a></div><span><a href="/cs/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/" title="图像处理">图像处理</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/baoyan/" title="分类于 保研">保研</a></div><span><a href="/baoyan/%E4%BF%9D%E7%A0%94%E5%8E%86%E7%A8%8B/" title="保研历程">保研历程</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 人工智能">人工智能</a></div><span><a href="/ai/%E7%8E%B0%E4%BB%A3%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" title="现代深度学习">现代深度学习</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/baoyan/" title="分类于 保研">保研</a></div><span><a href="/baoyan/Low-level-Vision-Group/" title="Low-level-Vision-Group">Low-level-Vision-Group</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/cs/" title="分类于 计算机">计算机</a></div><span><a href="/cs/Vue/" title="Vue">Vue</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/other/" title="分类于 琐碎">琐碎</a></div><span><a href="/other/%E6%80%9D%E6%83%B3%E5%BB%BA%E8%AE%BE/" title="思想建设">思想建设</a></span></li></ul></div><div><h2>最新评论</h2><ul class="leancloud-recent-comment"></ul></div></div><div class="status"><div class="copyright">&copy; 2023 – <span itemprop="copyrightYear">2025</span> <span class="with-love"><i class="ic i-sakura rotate"></i> </span><span class="author" itemprop="copyrightHolder">Runhua Deng @ Runfar's Zone</span></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-chart-area"></i> </span><span title="站点总字数">194k 字</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="ic i-coffee"></i> </span><span title="站点阅读时长">2:56</span></div><div class="powered-by">基于 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL={path:"research/Mamba串烧/",favicon:{show:"（●´3｀●）やれやれだぜ",hide:"(´Д｀)大変だ！"},search:{placeholder:"文章搜索",empty:"关于 「 ${query} 」，什么也没搜到",stats:"${time} ms 内找到 ${hits} 条结果"},valine:!0,copy_tex:!0,katex:!0,fancybox:!0,copyright:'复制成功，转载请遵守 <i class="ic i-creative-commons"></i>BY-NC-SA 协议。',ignores:[function(e){return e.includes("#")},function(e){return new RegExp(LOCAL.path+"$").test(e)}]}</script><script src="https://cdn.polyfill.io/v2/polyfill.js"></script><script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script><script src="/js/app.js?v=0.2.5"></script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({log:!1,pluginJsPath:"lib/",pluginModelPath:"assets/",pluginRootPath:"live2dw/",tagMode:!1})</script></body></html><!-- rebuild by hrmmi -->