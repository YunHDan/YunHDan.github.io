<!-- build time:Mon Sep 08 2025 15:04:53 GMT+0800 (中国标准时间) --><!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#FFF"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/ico" sizes="32x32" href="/images/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="alternate" type="application/rss+xml" title="枯萎的花将在另一彼岸悄然绽放" href="https://yunhdan.github.io/rss.xml"><link rel="alternate" type="application/atom+xml" title="枯萎的花将在另一彼岸悄然绽放" href="https://yunhdan.github.io/atom.xml"><link rel="alternate" type="application/json" title="枯萎的花将在另一彼岸悄然绽放" href="https://yunhdan.github.io/feed.json"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CInconsolata:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/css/app.css?v=0.2.5"><link rel="canonical" href="https://yunhdan.github.io/research/Low%20Level%20Vision%E8%AE%BA%E6%96%87%E7%B2%BE%E7%82%BC/"><title>Low Level Vision论文精炼 - 学术 | Runfar's Zone = 枯萎的花将在另一彼岸悄然绽放</title><meta name="generator" content="Hexo 6.3.0"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">Low Level Vision论文精炼</h1><div class="meta"><span class="item" title="创建时间：2024-07-26 11:42:00"><span class="icon"><i class="ic i-calendar"></i> </span><span class="text">发表于</span> <time itemprop="dateCreated datePublished" datetime="2024-07-26T11:42:00+08:00">2024-07-26</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span class="text">本文字数</span> <span>9.5k</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span class="text">阅读时长</span> <span>9 分钟</span></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="切换导航栏"><span class="line"></span> <span class="line"></span> <span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">Runfar's Zone</a></li></ul><ul class="right"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div id="imgs" class="pjax"><ul><li class="item" data-background-image="https://s2.loli.net/2025/05/18/c3sbV5lQEWrCFup.jpg"></li><li class="item" data-background-image="https://s2.loli.net/2024/12/07/5QmIJHY1f26phnD.jpg"></li><li class="item" data-background-image="https://s2.loli.net/2024/12/07/2wNmqIx7i9grZWf.jpg"></li><li class="item" data-background-image="https://s2.loli.net/2025/05/18/fKabwVInh1kB8Xq.jpg"></li><li class="item" data-background-image="https://s2.loli.net/2024/12/04/TcIOxZAHaXNbG2o.jpg"></li><li class="item" data-background-image="https://s2.loli.net/2025/05/18/QColdBwvmPp8zXT.jpg"></li></ul></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"/></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"/><use xlink:href="#gentle-wave" x="48" y="3"/><use xlink:href="#gentle-wave" x="48" y="5"/><use xlink:href="#gentle-wave" x="48" y="7"/></g></svg></div><main><div class="inner"><div id="main" class="pjax"><div class="article wrap"><div class="breadcrumb" itemscope itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i> <span><a href="/">首页</a></span><i class="ic i-angle-right"></i> <span class="current" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/research/" itemprop="item" rel="index" title="分类于 学术"><span itemprop="name">学术</span></a><meta itemprop="position" content="1"></span></div><article itemscope itemtype="http://schema.org/Article" class="post block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://yunhdan.github.io/research/Low%20Level%20Vision%E8%AE%BA%E6%96%87%E7%B2%BE%E7%82%BC/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/me.jpg"><meta itemprop="name" content="Runhua Deng"><meta itemprop="description" content=", 计算机视觉 & 图像恢复"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="枯萎的花将在另一彼岸悄然绽放"></span><div class="body md" itemprop="articleBody"><div class="note info"><p>一篇论文，只有理解了所提出的问题是什么，为了解决这个问题提出了什么方法，以及有自己的思考和收获，才算是吸收完这篇论文。</p></div><h1 id="low-light-image-enhancement"><a class="anchor" href="#low-light-image-enhancement">#</a> <code>Low-Light Image Enhancement</code></h1><h2 id="2024-codeenhance-a-codebook-driven-approach-for-low-light-image-enhancementcodeenhance"><a class="anchor" href="#2024-codeenhance-a-codebook-driven-approach-for-low-light-image-enhancementcodeenhance">#</a> <code>2024 CodeEnhance: A Codebook-Driven Approach for Low-Light Image Enhancement(CodeEnhance)</code></h2><p><img data-src="../../assets/1.png" alt="image"></p><p><strong>思考</strong></p><ul><li>利用 <code>codebook</code> 从 <code>High-Quality</code> 图像中学习先验。</li><li>在 <code>Unet</code> 编码器端并行设计预训练的语义信息提取网络 <code>SEE</code> ，在编码器末端设计一个语义信息融合模块 <code>SEM</code> ，将语义信息与图像特征融合。</li><li>因为 <code>codebook</code> 学习的先验所用数据集和第二阶段训练所用数据集不同，设计了一个 <code>codebook</code> 偏移机制 <code>CS</code> 处理引入 codebook 产生的数据分布偏移问题。</li><li>在预训练的解码器末端附加一个特征转换模块 <code>IFT</code> 。一方面，利用参考图像 <code>Reference</code> 丰富解码器 <code>High-Quality</code> 解码图像的信息，另一方面，在 <code>IFT</code> 内的 <code>CPT</code> 加入两个人为设计的 $$\omega$$ 以实现可控式图像提亮。</li></ul><p><strong>创新</strong></p><blockquote><p>很明显知道这篇低光的论文是基于 <code>Towards Robust Blind Face Restoration with Codebook Lookup Transformer</code> 这篇文章的工作，所以创新是相对于 <code>Towards Robust Blind Face Restoration with Codebook Lookup Transformer</code> 的创新。我们可以把 <code>Towards Robust Blind Face Restoration with Codebook Lookup Transformer</code> 称为 <code>base work</code></p></blockquote><ul><li>在 <code>encoder</code> 并行引入一个语义感知模块，与 <code>encoder</code> 通过一个语义融合模块融合。这个做法是借鉴了 23 年 <code>ICCV</code> 的一篇与语义分割结合的低光工作： <code>Learning Semantic-Aware Knowledge Guidance for Low-Light Image Enhancement</code> 。</li><li><code>IFT</code> 源于 <code>base work</code> 的 <code>CPT</code> 。但是 <code>base work</code> 的 <code>CPT</code> 是将低质量图作为参考，而本文章是把 <code>ground-truth</code> 作为参考。</li><li>发现 <code>codebook</code> 两阶段输入的数据分布不一致，提出代码本数据偏移机制。这是较 <code>base work</code> 优秀的地方。</li></ul><blockquote><p>所以很明显，我们不应该是将别的领域的东西套过来到本领域去用，而是把自己领域的东西尽可能往别的领域靠、结合，才是更好的。要针对别的领域的工作，提出一个基于本领域的改进。</p></blockquote><p><strong>不足</strong></p><ul><li>极其依赖 <code>ground-truth</code> 和 <code>reference</code> 。 <code>codebook</code> 的训练依赖 <code>high-quality</code> 图像进行重建不说， <code>IFT</code> 需要一张正常光照的图作为参考。训练时期， <code>IFT</code> 尚且可以。然而测试如果再依赖一张正常光照的图像作为 <code>reference</code> ，那就没有什么应用意义。虽然通过这种方式能够实现光照恢复动态化，但缺点也很明显。</li><li>解决 <code>codebook</code> 的数据偏移问题过于粗糙，引入可学习的 <code>codebook shift</code> 增加了训练的困难。</li></ul><h2 id="2023-cvpr-learning-a-simple-low-light-image-enhancer-from-paired-low-light-instancespairlie"><a class="anchor" href="#2023-cvpr-learning-a-simple-low-light-image-enhancer-from-paired-low-light-instancespairlie">#</a> <code>2023 CVPR Learning a Simple Low-light Image Enhancer from Paired Low-light Instances(PairLIE)</code></h2><p><img data-src="../../assets/4.png" alt="image"></p><p><strong>思考</strong></p><ul><li>使用两张低光的图片，结合 <code>Retinex</code> 理论，实现不需要额外设计手工先验的无监督低光增强。</li><li>在使用 <code>Retinex</code> 分解前，使用一个映射网络有效增强分解准确率。</li><li>将超分辨率中多图像辅助修复单图像的做法引入到低光照增强领域中，为多图像辅助修复单图像提供了低光照增强的做法。</li><li>训练阶段，两张低光图经过 <code>L-Net</code> 得到的结果并未充分利用。</li><li>这个工作更重要的意义在于，将多图像的信息辅助单图像的恢复。然而作者的做法是，多引入图片就设计一个新的，与待恢复图像相同的网络架构，即 L-Net，R-Net 和 P-Net。倘若扩展到更多图像，不可能再同样为每一张新引入的图片设计一整套网络。因此这种做法限制了多图像的信息辅助单图像的恢复，局限于双图像。</li><li>推理阶段， <code>L-Net</code> 估计得到的 <code>L</code> 只进行一个指数变换，得到 $ L^\lambda $ 就作为提亮后的光照，这种做法能否优化？ <code>R-Net</code> 的输出 <code>R</code> 并未经过任何处理，是否影响了性能的上限？</li></ul><h2 id="2024-di-retinex-digital-imaging-retinex-theory-for-low-light-image-enhancementdi-retinex"><a class="anchor" href="#2024-di-retinex-digital-imaging-retinex-theory-for-low-light-image-enhancementdi-retinex">#</a> <code>2024 DI-Retinex: Digital-Imaging Retinex Theory for Low-Light Image Enhancement(DI-Retinex)</code></h2><p><img data-src="../../assets/5.png" alt="image"></p><p><strong>思考</strong></p><ul><li>指出经典的 <code>Retinex</code> 理论存在缺乏考虑噪声的问题，提出了一个新的适用于数码成像的 <code>Retinex</code> 理论 <code>DI-Retinex</code> 。</li><li>根据 <code>DI-Retinex</code> 理论，重新规定了光照提亮的范式。范式中的偏置项即为经典 <code>Retinex</code> 理论未考虑到的缺陷。</li><li>为 <code>DI-Retinex</code> 理论设计了两个损失，分别是将正常光照退化为低光图像的反向退化损失以及噪声的方差抑制损失。</li><li>相对于传统的 <code>Retinex</code> 理论以及改良版本的 <code>Retinex</code> 理论，提出的 <code>DI-Retinex</code> 理论考虑更加全面。</li></ul><h2 id="2024-cvpr-unsupervised-image-prior-via-prompt-learning-and-clip-semantic-guidance-for-low-light-image-enhancement"><a class="anchor" href="#2024-cvpr-unsupervised-image-prior-via-prompt-learning-and-clip-semantic-guidance-for-low-light-image-enhancement">#</a> <code>2024 CVPR Unsupervised Image Prior via Prompt Learning and CLIP Semantic Guidance for Low-Light Image Enhancement</code></h2><p><img data-src="../../assets/6.png" alt="image"></p><blockquote><p>前身工作是 <code>2023 ICCV Iterative Prompt Learning for Unsupervised Backlit Image Enhancement</code></p></blockquote><p><strong>存疑</strong></p><ul><li>在 <code>prompt learning</code> 阶段要训练 <code>positive prompt</code> 和 <code>negative prompt</code> ，这两个 <code>prompt</code> 是怎么初始化的？形式是怎么样的？</li><li>在 <code>training</code> 阶段，使用曲线估计提亮后面的部分没看明白。</li></ul><h2 id="2024-glare-low-light-image-enhancement-via-generative-latent-feature-based-codebook-retrievalglare"><a class="anchor" href="#2024-glare-low-light-image-enhancement-via-generative-latent-feature-based-codebook-retrievalglare">#</a> <code>2024 GLARE: Low Light Image Enhancement via Generative Latent Feature based Codebook Retrieval(GLARE)</code></h2><p><img data-src="../../assets/7.png" alt="image"></p><p><strong>思考</strong></p><ul><li>第一个将 <code>codebook</code> 引入低光照图像增强领域的工作。将预训练的 <code>VQGAN</code> 进行微调以更适合低光照图像增强背景。</li><li>为了解决 <code>codebook</code> 难题之阶段前后数据分布不一致，提出一个逆向潜特征正则流（ <code>Invertible Latent Normalizing Flow</code> ）的操作，将低光特征经过转换，逼近正常光照的特征，以更加适合 <code>codebook</code> 适用的数据分布。</li><li>为了解决 <code>codebook</code> 难题之纹理缺失，提出一个双译码器架构实现将先前 <code>Encoder</code> 编码的特征与 <code>codebook</code> 检索出来的特征融合。</li><li><code>codebook</code> 的工作，都有泛化性相对不足的缺点。在推理阶段， <code>codebook</code> 是在训练集上训练的，得到的是训练集的先验知识，在后续与测试集 <code>low-quality</code> 特征融合。因此需要从 <code>codebook</code> 得到知识后，需要有进一步优化操作。</li></ul><h2 id="2023-iccv-implicit-neural-representation-for-cooperative-low-light-image-enhancementnerco"><a class="anchor" href="#2023-iccv-implicit-neural-representation-for-cooperative-low-light-image-enhancementnerco">#</a> <code>2023 ICCV Implicit Neural Representation for Cooperative Low-light Image Enhancement(NeRco)</code></h2><p><img data-src="../../assets/10.png" alt="image"></p><blockquote><p>这次我们先只看这个工作的 <code>NRN</code> 部分，也就是 <code>Neural Representation Normalization</code> 。</p></blockquote><p><strong>思考</strong>：</p><ul><li>将隐式神经表征 <code>Implicit Neural Representation</code> 第一次引入到低光照图像增强领域中。</li><li>将隐式神经表征与低光背景很好地结合，并且解释了为何能实现去噪。</li><li>对隐式神经表征了解还不够多，应该往这个领域多研究研究。</li></ul><h2 id="2024-cvpr-zero-ig-zero-shot-illumination-guided-joint-denoising-and-adaptive-enhancement-for-low-light-imageszero-ig"><a class="anchor" href="#2024-cvpr-zero-ig-zero-shot-illumination-guided-joint-denoising-and-adaptive-enhancement-for-low-light-imageszero-ig">#</a> <code>2024 CVPR ZERO-IG: Zero-Shot Illumination-Guided Joint Denoising and Adaptive Enhancement for Low-Light Images(Zero-IG)</code></h2><p><img data-src="../../assets/zero-ig.png" alt="image"></p><p><strong>思考</strong>：</p><ul><li>重新思考 <code>Retinex</code> 的理论，认为光照的平滑性能消除光照的噪声毛刺，所以对一个经过去噪后的低光图像提取的照度能近似正常光照下的照度。</li><li>将 <code>Noise2Noise</code> 的思想引入到低光照图像增强领域，与 <code>Retinex</code> 理论很好地结合。</li><li>将正常光照的照度与带噪声的反射量拼接，然后使用 <code>Noise2Noise</code> 的方式去噪。在过程中用损失约束光照全程一致，可以是网络更加关注于反射量的去噪。另外，与光照拼接，能够引导网络在去噪时关注哪些区域需要提亮，哪些区域需要抑制光照。所以，这较单纯用带噪声的反射量去噪更加有效。</li><li>借鉴 <code>Noise2Noise</code> 思想，将噪声图下采样为两张图片，并彼此构建自监督的方式，避免了对配对数据集的依赖，也无需学习特定噪声分布即可应用在低光任务，实现 <code>zero-shot</code> 无监督。</li><li>在为反射去噪时，将光照与反射进行拼接，网络学习过程保持光照不变，使得网络更加关注对反射的去噪，这大大提高了去噪的效果。</li></ul><h2 id="you-only-need-one-color-space-an-efficient-network-for-low-light-image-enhancement"><a class="anchor" href="#you-only-need-one-color-space-an-efficient-network-for-low-light-image-enhancement">#</a> <code>You Only Need One Color Space: An Efficient Network for Low-light Image Enhancement</code></h2><p><img data-src="../../assets/cidnet1.jpg" alt="image"></p><p>&lt;img src=&quot;../../assets/cidnet2.jpg&quot; alt=&quot;image&quot; style=&quot;zoom:50%;&quot; /&gt;</p><p><strong>贡献</strong>：</p><ul><li>为了解决传统 <code>sRGB</code> 色彩空间存在的颜色与亮度存在的耦合问题，以及从 <code>sRGB</code> 色彩空间转换到 <code>HSV</code> 空间存在的，由于一对多映射导致信息丢失问题，本文提出了一个全新的 <code>HVI</code> 色彩空间。通过引入三个可学习的参数和一个可学习的函数提升 <code>HVI</code> 色彩空间的稳定性，同时解决了一对多映射导致的信息丢失问题。</li><li>将 <code>HVI</code> 空间的图像特征分解为 <code>HV</code> 颜色特征和 I 强度特征，专门设计一个双支网络分别处理这两个信息，并用交叉注意力为两个特征信息建立交互。</li><li>提出了一个基于交叉注意力的模块 <code>LCA</code> ，促进 <code>HV</code> 特征和 <code>I</code> 强度特征的信息交互。</li></ul><h1 id="image-dehazing"><a class="anchor" href="#image-dehazing">#</a> <code>Image Dehazing</code></h1><h2 id="2023-cvpr-ridcp-revitalizing-real-image-dehazing-via-high-quality-codebook-priors"><a class="anchor" href="#2023-cvpr-ridcp-revitalizing-real-image-dehazing-via-high-quality-codebook-priors">#</a> <code>2023 CVPR RIDCP: Revitalizing Real Image Dehazing via High-Quality Codebook Priors</code></h2><p><img data-src="../../assets/8.png" alt="image"></p><p><strong>贡献</strong></p><ul><li>第一个将 <code>codebook</code> 方法引入到去雾领域的工作。</li><li>设计了一个可控匹配机制 <code>CHM</code> 替代最近邻匹配机制。具体来说，在最近邻匹配公式内的欧氏距离前乘以权重，增强匹配的效果。权重是一个自然指数，以系数 <code>a</code> 与 <code>f</code> 乘积为底。系数 <code>a</code> 通过最小化 <code>codebook</code> 特征与 <code>encoder</code> 输出特征的 <code>KL</code> 散度得到。系数 <code>f</code> 是 <code>codebook</code> 激活频率与 <code>encoder</code> 激活频率的差值（说实话论文中这个什么激活频率没交代是怎么得到的，没看明白，大概理解就是 <code>codebook</code> 与 <code>encoder</code> 的特征距离）</li><li>设计一个双译码器，通过一个归一化特征匹配 <code>NFA</code> 模块，将第一个译码器 $ G_{vq} $ 的输出与 <code>encoder</code> 的输出进行特征融合。</li></ul><p><strong>创新</strong></p><ul><li>可控匹配机制 <code>CHM</code> 改良了最近邻匹配机制，减轻数据分布偏差的问题。</li></ul><p><strong>不足</strong></p><ul><li>在可控匹配机制 <code>CHM</code> 的实现上面，前面提到，通过乘以一个指数权重改良最近邻匹配，这个指数以两个系数乘积为底，第二个系数 <code>f</code> ，论文交代十分模糊，称为 <code>codebook</code> 激活频率差，我理解是两个数据分布的距离。在代码中也不详细， <code>f</code> 通过一个预训练数据给出，预训练也不知道如何训练得到的。</li></ul><h1 id="vision-prompt-learning"><a class="anchor" href="#vision-prompt-learning">#</a> <code>Vision Prompt Learning</code></h1><h2 id="2024-towards-effective-multiple-in-one-image-restoration-a-sequential-and-prompt-learning-strategymioir"><a class="anchor" href="#2024-towards-effective-multiple-in-one-image-restoration-a-sequential-and-prompt-learning-strategymioir">#</a> <code>2024 Towards Effective Multiple-in-One Image Restoration: A Sequential and Prompt Learning Strategy(MiOIR)</code></h2><p><img data-src="../../assets/2.png" alt="image"></p><p><strong>贡献</strong></p><ul><li>指出 <code>all-in-one</code> 模型的问题：考虑的退化任务少；每一次训练只在一个特定的退化任务数据集上训练，有可能导致灾难性遗忘。提出序列学习，每次训练的数据，是上次训练的数据与新特定退化类型数据的叠加，有效应对灾难性遗忘。</li><li>使用两种极端的提示学习方法，帮助模型识别特定退化任务，分别是精确提示学习与自适应提示学习。</li><li>精确提示学习：通过预先训练提示，提示分类器，在下一阶段通过输入的退化图片筛选出适合的提示，引导特定退化任务的图像恢复。</li><li>自适应提示学习：通过输入的退化图片经过卷积处理得到提示信息，引导特定退化任务的恢复。</li></ul><p><strong>创新</strong></p><ul><li>第一次将序列学习引入到图像恢复领域。</li></ul><p><strong>不足</strong></p><ul><li>个人认为，自适应提示学习不能算作提示学习，更像是特征融合。</li></ul><h1 id="blind-face-restoration"><a class="anchor" href="#blind-face-restoration">#</a> <code>Blind Face Restoration</code></h1><h2 id="2022-neurips-towards-robust-blind-face-restoration-with-codebook-lookup-transformercodeformer"><a class="anchor" href="#2022-neurips-towards-robust-blind-face-restoration-with-codebook-lookup-transformercodeformer">#</a> <code>2022 NeurIPS Towards Robust Blind Face Restoration with Codebook Lookup Transformer(CodeFormer)</code></h2><p><img data-src="../../assets/3.png" alt="image"></p><p><strong>贡献</strong></p><ul><li>将 <code>codebook</code> 引入到人脸修复领域中。</li><li>指出 <code>codebook</code> 索引方式即最近邻特征匹配方法应用到图像恢复领域的不足在于，输入的 <code>Low-Quality</code> 图像因为各种类型的退化严重导致 <code>Low-Quality</code> 与 <code>codebook</code> 的数据分布严重不匹配，进而造成了 <code>codebook</code> 检索的困难。因此，为了能够更好地进行 <code>codebook</code> 的检索，使用 <code>Transformer</code> 进行 <code>codebook</code> 的检索，提高检索精确度。</li><li>如果仅仅将 <code>codebook</code> 提取出来的特征进行解码，由于 <code>codebook</code> 是从其他数据集上训练得来的，因此解码得到的特征与当前数据集的 <code>GT</code> 会有很大差异，所以有必要将输入与 <code>codebook</code> 的特征进行融合，以得到一个质量更优于 <code>Low-Quality</code> ，数据分布更接近 <code>High-Quality</code> 的图片。进一步，引入一个可控的特征转换器 <code>CFT</code> ，通过一个人为设置的超参数 <code>w</code> ，实现 <code>Encoder</code> 到 <code>Decoder</code> 数据流的控制，即实现可控式的特征融合。</li></ul><p><strong>创新</strong></p><ul><li>将 <code>codebook</code> 引入到人脸修复领域。</li><li>提出最近邻特征匹配在图像恢复的不足，为未来的工作提供了很好的指引。</li><li>设计了一个可控的特征融合器，进一步弥补了 <code>codebook</code> 的不足。</li></ul><p><strong>不足</strong></p><ul><li>通过单一的 <code>Transformer</code> 块叠加进行 <code>codebook</code> 索引，造成索引耗时长的问题。</li></ul><h1 id="image-derain"><a class="anchor" href="#image-derain">#</a> <code>Image Derain</code></h1><h2 id="2024-cvpr-bidirectional-multi-scale-implicit-neural-representations-for-image-deraining"><a class="anchor" href="#2024-cvpr-bidirectional-multi-scale-implicit-neural-representations-for-image-deraining">#</a> <code>2024 CVPR Bidirectional Multi-Scale Implicit Neural Representations for Image Deraining</code></h2><p><img data-src="../../assets/11.png" alt="image"></p><blockquote><p>这里先主要研究前面的 <code>Implicit Neural Representation</code> 部分。</p></blockquote><p><strong>贡献</strong></p><ul><li>将隐式神经表征第一次引入图像去雨领域中，以更好地帮助模型学习普遍雨纹退化特征进行退化的去除。</li><li>将隐式神经表征与多尺度网络结合，能够在多尺度信息流动中进行退化的去除。</li></ul><p><strong>创新</strong></p><ul><li>在每个尺度的网络前引入 <code>Implicit Neural Representation</code> 去除雨纹，形成一个在尺度上的级联 <code>Implicit Neural Representation</code> 结构。</li></ul><p><strong>不足</strong></p><p>（暂时未知）</p><h1 id="image-denoise"><a class="anchor" href="#image-denoise">#</a> <code>Image Denoise</code></h1><h2 id="2023-cvpr-zero-shot-noise2noise-efficient-image-denoising-without-any-datazs-n2n"><a class="anchor" href="#2023-cvpr-zero-shot-noise2noise-efficient-image-denoising-without-any-datazs-n2n">#</a> <code>2023 CVPR Zero-Shot Noise2Noise: Efficient Image Denoising without any Data(ZS-N2N)</code></h2><p><img data-src="../../assets/zs-n2n1.jpg" alt="image"></p><p><strong>Contributions</strong>：</p><ul><li>They offered a downsampling-based unsupervised method. They downsampled the noisy image into two patches by using a special method at the pixel-wise level , and subsequently used two-layer convolution kernels to process the two patches.</li><li>Mapping between noisy patches can be equivalent to denoising the input image, where a consistency loss is applied to constrain the mapping process.</li><li>The two patches were considered as similar as possible, so there was a residual loss applied to the mapping process.</li></ul><p><strong>Innovations</strong>：</p><ul><li>By integrating the Noise2Nosie and Neighbour2Neighbour methods, the paper offered a unsupervised learning paradigm.</li><li>It was very light-weight.</li></ul><h1 id="image-enhancement"><a class="anchor" href="#image-enhancement">#</a> <code>Image Enhancement</code></h1><h2 id="2024-cvpr-color-shift-estimation-and-correction-for-image-enhancementcsec"><a class="anchor" href="#2024-cvpr-color-shift-estimation-and-correction-for-image-enhancementcsec">#</a> <code>2024 CVPR Color Shift Estimation-and-Correction for Image Enhancement(CSEC)</code></h2><p><img data-src="../../assets/csec1.jpg" alt="image"></p><p><strong>贡献</strong>：</p><ul><li>通过 <code>PCA</code> 分析发现一些图像增强数据集的图像里有欠曝光和过曝光的区域，而且这些区域的像素点特征存在明显的相反性。基于此发现，作者设计了一个能够处理一张图像同时存在过曝光和欠曝光的网络。作者利用一个基于 <code>Unet</code> 的特征提取器，提取图像中相反的两个曝光特征。</li><li>两种相反的曝光特征一定存在一个特征，是这两种特征的正常参照。作者为曝光程度不同的两种图像分别设计了颜色偏移估计模块，估计出颜色方面的偏移以获取正常参照的特征。鉴于可变形卷积能够为卷积核估计出图像像素点相较于卷积核的偏移，作者认为通过可变形卷积能够估计出过曝光和欠曝光相较于正常参照的颜色偏移。因此设计了 <code>COSE</code> 模块分别估计颜色偏移。</li><li>为了利用估计的颜色偏移去纠正原始图像中过曝光和欠曝光的特征偏移，作者设计了一个 <code>COMO</code> 模块，利用交叉注意力分别将原始图像的特征与过曝光颜色偏移特征和欠曝光颜色偏移特征进行建模，得到最后的增强结果图。</li></ul><p><strong>创新</strong>：</p><ul><li>发现同一张图像中的过曝光和欠曝光存在特征相反性，并通过估计曝光的参考特征来实现同时纠正一张图像中的过曝光和欠曝光。</li><li>在曝光的参考特征辅助下，对过曝光和欠曝光的区域分别估计颜色偏移，借鉴可变形卷积的思想去处理颜色的特征偏移。</li><li>利用交叉注意力建模三个特征信息：原始图像、过曝光偏移特征、欠曝光偏移特征。</li></ul><h2 id="2024-cvpr-empowering-resampling-operation-for-ultra-high-definition-image-enhancement-with-model-aware-guidancelmar"><a class="anchor" href="#2024-cvpr-empowering-resampling-operation-for-ultra-high-definition-image-enhancement-with-model-aware-guidancelmar">#</a> <code>2024 CVPR Empowering Resampling Operation for Ultra-High-Definition Image Enhancement with Model-Aware Guidance(LMAR)</code></h2><p><img data-src="../../assets/lmar1.jpg" alt="image"></p><p><strong>贡献</strong>：</p><ul><li>这篇文章发现通过插值降低分辨率后进行增强再上采样会导致信息丢失和性能下降的现象。具体来说，有如下方面：下采样降低分辨率导致了一些高频率的信息损失；高分辨率下采样为低分辨率，两种特征分布存在差异；传统的下采样的方法与中间增强模型无关，并未考虑模型对输入的偏好（如对特定方向的边缘或低频颜色更敏感），因此无法为模型保留偏好信息。作者提出了一个叫做 <code>LMAR</code> 的方法解决上述问题。</li><li><code>LMAR</code> 通过结合隐式神经表征，估计出下采样图像的补偿信息，嵌入到下采样图像中，使得低分辨率输入在中间增强模型的特征空间与原始 <code>UHD</code> 高分辨率图像的中间层特征保持一致。</li></ul><p><strong>创新</strong>：</p><ul><li>发现下采样和重采样由于传统插值方法导致的信息丢失问题，并设计了一个基于模型感知的方法补偿损失的信息。</li></ul><h1 id="all-in-one-restoration"><a class="anchor" href="#all-in-one-restoration">#</a> <code>All-in-One Restoration</code></h1><h2 id="2024-lora-ir-taming-low-rank-experts-for-efficient-all-in-one-image-restorationlora-ir"><a class="anchor" href="#2024-lora-ir-taming-low-rank-experts-for-efficient-all-in-one-image-restorationlora-ir">#</a> <code>2024 LoRA-IR: Taming Low-Rank Experts for Efficient All-in-One Image Restoration(LoRA-IR)</code></h2><p><img data-src="../../assets/lorair1.jpg" alt="image"></p><p><strong>贡献</strong>：</p><ul><li>针对单一退化设计专用模型的传统方法无法泛化到混合退化场景，而现有的 <code>All-in-one</code> 模型虽然能够处理多种退化，但存在参数冗余、计算效率低、无法捕捉退化间关联性等问题。为了解决这些问题，作者引入了 <code>LoRA</code> 作为 <code>MoE</code> 的专家处理网络，并通过路由引导来选择特定的专家处理特定的退化。</li><li>为了针对图像选择特定的专家对退化进行处理， <code>LoRA-IR</code> 的退化引导路由器 <code>DG-Router</code> 使用 <code>CLIP</code> 模型的强大图像表征能力，提取退化表示。其中，为了避免因下采样到 <code>CLIP</code> 特征空间时由于分辨率降低带来的信息损失问题， <code>DG-Router</code> 通过将滑动窗口与下采样结合来解决这个问题。另外，使用 <code>MLP</code> 和池化技术融合下采样的全局信息和滑动窗口的局部信息，生成了更加鲁棒的退化表示。</li><li>预训练阶段，通过 <code>DAM</code> （基于通道注意力的退化引导自适应调节器）将退化表征信息注入到统一的恢复网络中，增强特征的退化表示使得模型获得对特定退化的感知。微调阶段，基于 <code>MoE</code> 架构，借助 <code>DG-Router</code> 生成的退化表示动态选择多个低秩专家，进一步提升模型对未知退化的泛化能力。</li></ul><p><strong>创新</strong>：</p><ul><li><p>将 <code>LoRA</code> 引入到图像恢复领域。通过 <code>LoRA</code> 集成到 <code>MoE</code> 的具体专家中，实现更高效的特定退化处理。</p></li><li><p>借用 <code>CLIP</code> 强大的图像表征能力获得退化表示。</p></li><li><p>结合滑动窗口和下采样缓解图像分辨率降低带来的信息损失。</p></li></ul><h1 id="image-motion-deblur"><a class="anchor" href="#image-motion-deblur">#</a> <code>Image Motion Deblur</code></h1><h2 id="2024-cvpr-spike-guided-motion-deblurring-with-unknown-modal-spatiotemporal-alignmentuasdn"><a class="anchor" href="#2024-cvpr-spike-guided-motion-deblurring-with-unknown-modal-spatiotemporal-alignmentuasdn">#</a> <code>2024 CVPR Spike-guided Motion Deblurring with Unknown Modal Spatiotemporal Alignment(UaSDN)</code></h2><ul><li>传统 RGB 相机成像的各个帧之间存在时间间隔未被利用，导致了图像在时间维度的信息缺失，引发模糊问题。另外，传统 RGB 相机成像通常会因为曝光、极端环境而带来图像的纹理、结构信息缺失。</li><li>脉冲相机的时间分辨率高，能够准确获取连续时间内的成像信息，能为 RGB 相机成像补充关键信息。</li></ul><p><img data-src="../../assets/uasdn.png" alt=""></p><ul><li>引入脉冲相机，要求成像在空间、时间上对齐，这是十分困难的事情，这个工作提出在不知道时空对齐关系的情况下， <code>UaSDN</code> 通过神经网络学习脉冲成像和 RGB 成像的时空对齐关系。</li><li>某一曝光窗口的模糊图像帧，是曝光窗口所有时刻的清晰图像帧的平均值。</li><li>本质上其实是引入一种新形式的先验知识，或者也可以说是一个多模态的工作。脉冲和帧是两个不同的模态，脉冲是帧的提示信息。经过<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>M</mi><mrow><mi>S</mi><mn>2</mn><mi>I</mi></mrow></msub></mrow><annotation encoding="application/x-tex">M_{S2I}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.83333em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.10903em">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.32833099999999993em"><span style="top:-2.5500000000000003em;margin-left:-.10903em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:.05764em">S</span><span class="mord mtight">2</span><span class="mord mathnormal mtight" style="margin-right:.07847em">I</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span> 网络处理的脉冲序列实际上是灰度图序列，穿插到后续阶段的网络中引导恢复。</li></ul><p><img data-src="../../assets/uasdn2.png" alt=""></p><ul><li>二阶段用变形卷积对称地处理两个模态的信息并融合，三阶段用光流方法进一步融合。</li><li>实际上这个工作并没有使用到实际的脉冲相机，而是在原有的数据集的基础上用插帧的方式模拟脉冲相机的超高时间分辨率效果。而且方法长时间未开源，真实效果存疑。</li><li>一段话总结：因为考虑到，某一个时间内的模糊照片是这个时间内所有时刻的清晰照片的平均值，所以是不是说明其他时刻的清晰照片能够辅助恢复模糊照片，只要神经网络知道足够多这个时间内其他时刻的清晰照片的情况，就能学习到物体高速运动的方向，就能去除模糊。</li></ul><h2 id="2024-cvpr-efficient-multi-scale-network-with-learnable-discrete-wavelet-transform-for-blind-motion-deblurringmlwnet"><a class="anchor" href="#2024-cvpr-efficient-multi-scale-network-with-learnable-discrete-wavelet-transform-for-blind-motion-deblurringmlwnet">#</a> <code>2024 CVPR Efficient Multi-scale Network with Learnable Discrete Wavelet Transform for Blind Motion Deblurring(MLWNet)</code></h2><ul><li>文章指出现在去模糊中的多进多出结构（ <code>MIMO</code> ）的若干问题：需要人工合成不同尺度分辨率的下采样图像，容易进一步引入退化；低分辨率经常通过简单的插值方式合成为高分辨率的图像，既不高效，也不足够有效。</li><li><code>SIMO</code> 以原本的图像作为输入，以网络学习的方式，得到若干尺度的下采样图像，然后分别输出对应尺度的恢复图像，避免了人工合成下采样图像的问题。</li><li>像 UNet 这样的多尺度框架，特征在低分辨率虽然在空间尺度上保留着一定的语义信息，但是纹理细节受到压缩，因此高频信息的恢复是有必要的。</li></ul><p><img data-src="../../assets/mlwnet.png" alt="image"></p><ul><li>离散傅里叶变换 <code>DFT</code> 通常难以处理突变信号，而在信号领域一个更好的替代方法就是离散小波变换 <code>DWT</code> ，作者发现了小波变换与卷积上的联系，认为在原始信号上用小波基分解可以被视为使用特定卷积核的卷积操作，据此设计了 <code>2D-DWT</code> ，去学习图像的模糊方向性。</li><li><code>LWN</code> 模块中的小波卷积（ <code>2D-DWT</code> ）将图像特征分解为低频、水平高频、垂直高频、对角高频信息，在小波域进行学习与处理，最后再反转回空间域。</li><li>为了避免小波卷积在训练学习的过程中退化回群卷积，作者加入了小波损失作为监督。</li></ul><p><img data-src="../../assets/mlwnet2.png" alt=""></p></div><footer><div class="meta"><span class="item"><span class="icon"><i class="ic i-calendar-check"></i> </span><span class="text">更新于</span> <time title="修改时间：2025-08-03 22:10:37" itemprop="dateModified" datetime="2025-08-03T22:10:37+08:00">2025-08-03</time> </span><span id="research/Low Level Vision论文精炼/" class="item leancloud_visitors" data-flag-title="Low Level Vision论文精炼" title="阅读次数"><span class="icon"><i class="ic i-eye"></i> </span><span class="text">阅读次数</span> <span class="leancloud-visitors-count"></span> <span class="text">次</span></span></div><div class="reward"><button><i class="ic i-heartbeat"></i> 赞赏</button><p>请我喝[茶]~(￣▽￣)~*</p><div id="qr"><div><img data-src="/images/wechatpay.jpg" alt="Runhua Deng 微信支付"><p>微信支付</p></div><div><img data-src="/images/alipay.png" alt="Runhua Deng alipay"><p>alipay</p></div><div><img data-src="/images/paypal.png" alt="Runhua Deng paypal"><p>paypal</p></div></div></div><div id="copyright"><ul><li class="author"><strong>本文作者： </strong>Runhua Deng <i class="ic i-at"><em>@</em></i>枯萎的花将在另一彼岸悄然绽放</li><li class="link"><strong>本文链接：</strong> <a href="https://yunhdan.github.io/research/Low%20Level%20Vision%E8%AE%BA%E6%96%87%E7%B2%BE%E7%82%BC/" title="Low Level Vision论文精炼">https://yunhdan.github.io/research/Low Level Vision论文精炼/</a></li><li class="license"><strong>版权声明： </strong>本站所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</span> 许可协议。转载请注明出处！</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/book/%E8%A2%AB%E8%AE%A8%E5%8E%8C%E7%9A%84%E5%8B%87%E6%B0%94/" itemprop="url" rel="prev" data-background-image="https:&#x2F;&#x2F;s2.loli.net&#x2F;2024&#x2F;12&#x2F;04&#x2F;OLhuxRpcjMQWIX2.jpg" title="《被讨厌的勇气：“自我启发之父”阿德勒的哲学课》"><span class="type">上一篇</span> <span class="category"><i class="ic i-flag"></i> 读书</span><h3>《被讨厌的勇气：“自我启发之父”阿德勒的哲学课》</h3></a></div><div class="item right"><a href="/other/My%20%E4%BA%8C%E6%AC%A1%E5%85%83/" itemprop="url" rel="next" data-background-image="https:&#x2F;&#x2F;s2.loli.net&#x2F;2024&#x2F;12&#x2F;04&#x2F;6buchRFIEO4qjsa.jpg" title="My 二次元"><span class="type">下一篇</span> <span class="category"><i class="ic i-flag"></i> 琐碎</span><h3>My 二次元</h3></a></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="文章目录"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#low-light-image-enhancement"><span class="toc-number">1.</span> <span class="toc-text">Low-Light Image Enhancement</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2024-codeenhance-a-codebook-driven-approach-for-low-light-image-enhancementcodeenhance"><span class="toc-number">1.1.</span> <span class="toc-text">2024 CodeEnhance: A Codebook-Driven Approach for Low-Light Image Enhancement(CodeEnhance)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2023-cvpr-learning-a-simple-low-light-image-enhancer-from-paired-low-light-instancespairlie"><span class="toc-number">1.2.</span> <span class="toc-text">2023 CVPR Learning a Simple Low-light Image Enhancer from Paired Low-light Instances(PairLIE)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2024-di-retinex-digital-imaging-retinex-theory-for-low-light-image-enhancementdi-retinex"><span class="toc-number">1.3.</span> <span class="toc-text">2024 DI-Retinex: Digital-Imaging Retinex Theory for Low-Light Image Enhancement(DI-Retinex)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2024-cvpr-unsupervised-image-prior-via-prompt-learning-and-clip-semantic-guidance-for-low-light-image-enhancement"><span class="toc-number">1.4.</span> <span class="toc-text">2024 CVPR Unsupervised Image Prior via Prompt Learning and CLIP Semantic Guidance for Low-Light Image Enhancement</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2024-glare-low-light-image-enhancement-via-generative-latent-feature-based-codebook-retrievalglare"><span class="toc-number">1.5.</span> <span class="toc-text">2024 GLARE: Low Light Image Enhancement via Generative Latent Feature based Codebook Retrieval(GLARE)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2023-iccv-implicit-neural-representation-for-cooperative-low-light-image-enhancementnerco"><span class="toc-number">1.6.</span> <span class="toc-text">2023 ICCV Implicit Neural Representation for Cooperative Low-light Image Enhancement(NeRco)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2024-cvpr-zero-ig-zero-shot-illumination-guided-joint-denoising-and-adaptive-enhancement-for-low-light-imageszero-ig"><span class="toc-number">1.7.</span> <span class="toc-text">2024 CVPR ZERO-IG: Zero-Shot Illumination-Guided Joint Denoising and Adaptive Enhancement for Low-Light Images(Zero-IG)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#you-only-need-one-color-space-an-efficient-network-for-low-light-image-enhancement"><span class="toc-number">1.8.</span> <span class="toc-text">You Only Need One Color Space: An Efficient Network for Low-light Image Enhancement</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#image-dehazing"><span class="toc-number">2.</span> <span class="toc-text">Image Dehazing</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2023-cvpr-ridcp-revitalizing-real-image-dehazing-via-high-quality-codebook-priors"><span class="toc-number">2.1.</span> <span class="toc-text">2023 CVPR RIDCP: Revitalizing Real Image Dehazing via High-Quality Codebook Priors</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#vision-prompt-learning"><span class="toc-number">3.</span> <span class="toc-text">Vision Prompt Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2024-towards-effective-multiple-in-one-image-restoration-a-sequential-and-prompt-learning-strategymioir"><span class="toc-number">3.1.</span> <span class="toc-text">2024 Towards Effective Multiple-in-One Image Restoration: A Sequential and Prompt Learning Strategy(MiOIR)</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#blind-face-restoration"><span class="toc-number">4.</span> <span class="toc-text">Blind Face Restoration</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2022-neurips-towards-robust-blind-face-restoration-with-codebook-lookup-transformercodeformer"><span class="toc-number">4.1.</span> <span class="toc-text">2022 NeurIPS Towards Robust Blind Face Restoration with Codebook Lookup Transformer(CodeFormer)</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#image-derain"><span class="toc-number">5.</span> <span class="toc-text">Image Derain</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2024-cvpr-bidirectional-multi-scale-implicit-neural-representations-for-image-deraining"><span class="toc-number">5.1.</span> <span class="toc-text">2024 CVPR Bidirectional Multi-Scale Implicit Neural Representations for Image Deraining</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#image-denoise"><span class="toc-number">6.</span> <span class="toc-text">Image Denoise</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2023-cvpr-zero-shot-noise2noise-efficient-image-denoising-without-any-datazs-n2n"><span class="toc-number">6.1.</span> <span class="toc-text">2023 CVPR Zero-Shot Noise2Noise: Efficient Image Denoising without any Data(ZS-N2N)</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#image-enhancement"><span class="toc-number">7.</span> <span class="toc-text">Image Enhancement</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2024-cvpr-color-shift-estimation-and-correction-for-image-enhancementcsec"><span class="toc-number">7.1.</span> <span class="toc-text">2024 CVPR Color Shift Estimation-and-Correction for Image Enhancement(CSEC)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2024-cvpr-empowering-resampling-operation-for-ultra-high-definition-image-enhancement-with-model-aware-guidancelmar"><span class="toc-number">7.2.</span> <span class="toc-text">2024 CVPR Empowering Resampling Operation for Ultra-High-Definition Image Enhancement with Model-Aware Guidance(LMAR)</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#all-in-one-restoration"><span class="toc-number">8.</span> <span class="toc-text">All-in-One Restoration</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2024-lora-ir-taming-low-rank-experts-for-efficient-all-in-one-image-restorationlora-ir"><span class="toc-number">8.1.</span> <span class="toc-text">2024 LoRA-IR: Taming Low-Rank Experts for Efficient All-in-One Image Restoration(LoRA-IR)</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#image-motion-deblur"><span class="toc-number">9.</span> <span class="toc-text">Image Motion Deblur</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2024-cvpr-spike-guided-motion-deblurring-with-unknown-modal-spatiotemporal-alignmentuasdn"><span class="toc-number">9.1.</span> <span class="toc-text">2024 CVPR Spike-guided Motion Deblurring with Unknown Modal Spatiotemporal Alignment(UaSDN)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2024-cvpr-efficient-multi-scale-network-with-learnable-discrete-wavelet-transform-for-blind-motion-deblurringmlwnet"><span class="toc-number">9.2.</span> <span class="toc-text">2024 CVPR Efficient Multi-scale Network with Learnable Discrete Wavelet Transform for Blind Motion Deblurring(MLWNet)</span></a></li></ol></li></ol></div><div class="related panel pjax" data-title="系列文章"><ul><li><a href="/research/PairLIE%E8%AE%BA%E6%96%87%E8%AF%A6%E8%A7%A3/" rel="bookmark" title="PairLIE论文详解">PairLIE论文详解</a></li><li><a href="/research/EnlightenGAN%E8%AE%BA%E6%96%87%E8%AF%A6%E8%A7%A3/" rel="bookmark" title="EnlightenGAN论文详解">EnlightenGAN论文详解</a></li><li><a href="/research/Retinexformer%E8%AE%BA%E6%96%87%E8%AF%A6%E8%A7%A3/" rel="bookmark" title="Retinexformer论文详解">Retinexformer论文详解</a></li><li><a href="/research/Mamba%E4%B8%B2%E7%83%A7/" rel="bookmark" title="Mamba串烧">Mamba串烧</a></li><li class="active"><a href="/research/Low%20Level%20Vision%E8%AE%BA%E6%96%87%E7%B2%BE%E7%82%BC/" rel="bookmark" title="Low Level Vision论文精炼">Low Level Vision论文精炼</a></li><li><a href="/research/%E5%85%B7%E8%BA%AB%E6%99%BA%E8%83%BD%E5%9F%BA%E7%A1%80%E6%8A%80%E6%9C%AF/" rel="bookmark" title="具身智能基础技术">具身智能基础技术</a></li><li><a href="/research/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%AE%BA%E6%96%87%E7%B2%BE%E7%82%BC/" rel="bookmark" title="多模态论文精炼">多模态论文精炼</a></li><li><a href="/research/2025%20CVPR%20NTIRE%20Image%20Denoise%E6%8A%A5%E5%91%8A%E7%A0%94%E7%A9%B6%E6%80%9D%E8%80%83/" rel="bookmark" title="2025 CVPR NTIRE Image Denoise报告研究思考">2025 CVPR NTIRE Image Denoise报告研究思考</a></li><li><a href="/research/%E6%9C%9F%E5%88%8A%E3%80%81%E4%BC%9A%E8%AE%AE%E6%8A%95%E7%A8%BF%E7%BB%8F%E9%AA%8C/" rel="bookmark" title="期刊、会议投稿经验">期刊、会议投稿经验</a></li></ul></div><div class="overview panel" data-title="站点概览"><div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="image" itemprop="image" alt="Runhua Deng" data-src="/images/me.jpg"><p class="name" itemprop="name">Runhua Deng</p><div class="description" itemprop="description">计算机视觉 & 图像恢复</div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">50</span> <span class="name">文章</span></a></div><div class="item categories"><a href="/categories/"><span class="count">7</span> <span class="name">分类</span></a></div></nav><div class="social"><span class="exturl item github" data-url="aHR0cHM6Ly9naXRodWIuY29tL1l1bkhEYW4=" title="https:&#x2F;&#x2F;github.com&#x2F;YunHDan"><i class="ic i-github"></i></span> <span class="exturl item music" data-url="aHR0cHM6Ly9tdXNpYy4xNjMuY29tLyMvdXNlci9ob21lP2lkPTIxMjYzMjE4OTI=" title="https:&#x2F;&#x2F;music.163.com&#x2F;#&#x2F;user&#x2F;home?id&#x3D;2126321892"><i class="ic i-cloud-music"></i></span> <a href="/alex2312666252@gmail.com" title="alex2312666252@gmail.com" class="item email"><i class="ic i-envelope"></i></a> <span class="exturl item csdn" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L20wXzczNTk5NzM4P3NwbT0xMDAwLjIxMTUuMzAwMS41MzQz" title="https:&#x2F;&#x2F;blog.csdn.net&#x2F;m0_73599738?spm&#x3D;1000.2115.3001.5343"><i class="ic i-link"></i></span></div><ul class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>首页</a></li><li class="item dropdown"><a href="/about/" rel="section"><i class="ic i-user"></i>关于</a><ul class="submenu"><li class="item"><a href="/about/me/" rel="section"><i class="ic i-file"></i>简历</a></li><li class="item"><a href="/about/learn_me/" rel="section"><i class="ic i-smile"></i>了解我</a></li></ul></li><li class="item dropdown"><a href="javascript:void(0);"><i class="ic i-feather"></i>文章</a><ul class="submenu"><li class="item"><a href="/archives/" rel="section"><i class="ic i-list-alt"></i>归档</a></li><li class="item"><a href="/categories/" rel="section"><i class="ic i-th"></i>分类</a></li><li class="item"><a href="/tags/" rel="section"><i class="ic i-tags"></i>标签</a></li></ul></li><li class="item"><a href="/friends/" rel="section"><i class="ic i-heart"></i>友達</a></li></ul></div></div></div><ul id="quick"><li class="prev pjax"><a href="/book/%E8%A2%AB%E8%AE%A8%E5%8E%8C%E7%9A%84%E5%8B%87%E6%B0%94/" rel="prev" title="上一篇"><i class="ic i-chevron-left"></i></a></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/other/My%20%E4%BA%8C%E6%AC%A1%E5%85%83/" rel="next" title="下一篇"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>随机文章</h2><ul><li class="item"><div class="breadcrumb"><a href="/categories/cs/" title="分类于 计算机">计算机</a></div><span><a href="/cs/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/" title="图像处理">图像处理</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 人工智能">人工智能</a></div><span><a href="/ai/Deep-Learning-Experiment-Tricks/" title="Deep Learning Experiment Tricks">Deep Learning Experiment Tricks</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/baoyan/" title="分类于 保研">保研</a></div><span><a href="/baoyan/Low-level-Vision-Group/" title="Low-level-Vision-Group">Low-level-Vision-Group</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/cs/" title="分类于 计算机">计算机</a></div><span><a href="/cs/ACM%E7%AE%97%E6%B3%95%E9%A2%98%E5%8D%95/" title="ACM算法题单">ACM算法题单</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/research/" title="分类于 学术">学术</a></div><span><a href="/research/%E6%9C%9F%E5%88%8A%E3%80%81%E4%BC%9A%E8%AE%AE%E6%8A%95%E7%A8%BF%E7%BB%8F%E9%AA%8C/" title="期刊、会议投稿经验">期刊、会议投稿经验</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/research/" title="分类于 学术">学术</a></div><span><a href="/research/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%AE%BA%E6%96%87%E7%B2%BE%E7%82%BC/" title="多模态论文精炼">多模态论文精炼</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/project/" title="分类于 项目与实践">项目与实践</a></div><span><a href="/project/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E8%BD%AC%E6%8D%A2%E4%B8%BA%E8%87%AA%E5%8A%A8%E6%9C%BA/" title="正则表达式转换为自动机">正则表达式转换为自动机</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/cs/" title="分类于 计算机">计算机</a></div><span><a href="/cs/Pip%E3%80%81Conda%E3%80%81github%E9%95%9C%E5%83%8F/" title="Pip、Conda、Github镜像">Pip、Conda、Github镜像</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/cs/" title="分类于 计算机">计算机</a></div><span><a href="/cs/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%90%86%E8%AE%BA1/" title="数据库理论1">数据库理论1</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/research/" title="分类于 学术">学术</a></div><span><a href="/research/Mamba%E4%B8%B2%E7%83%A7/" title="Mamba串烧">Mamba串烧</a></span></li></ul></div><div><h2>最新评论</h2><ul class="leancloud-recent-comment"></ul></div></div><div class="status"><div class="copyright">&copy; 2023 – <span itemprop="copyrightYear">2025</span> <span class="with-love"><i class="ic i-sakura rotate"></i> </span><span class="author" itemprop="copyrightHolder">Runhua Deng @ Runfar's Zone</span></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-chart-area"></i> </span><span title="站点总字数">199k 字</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="ic i-coffee"></i> </span><span title="站点阅读时长">3:01</span></div><div class="powered-by">基于 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL={path:"research/Low Level Vision论文精炼/",favicon:{show:"（●´3｀●）やれやれだぜ",hide:"(´Д｀)大変だ！"},search:{placeholder:"文章搜索",empty:"关于 「 ${query} 」，什么也没搜到",stats:"${time} ms 内找到 ${hits} 条结果"},valine:!0,copy_tex:!0,katex:!0,fancybox:!0,copyright:'复制成功，转载请遵守 <i class="ic i-creative-commons"></i>BY-NC-SA 协议。',ignores:[function(e){return e.includes("#")},function(e){return new RegExp(LOCAL.path+"$").test(e)}]}</script><script src="https://cdn.polyfill.io/v2/polyfill.js"></script><script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script><script src="/js/app.js?v=0.2.5"></script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({log:!1,pluginJsPath:"lib/",pluginModelPath:"assets/",pluginRootPath:"live2dw/",tagMode:!1})</script></body></html><!-- rebuild by hrmmi -->