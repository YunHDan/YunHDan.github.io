<!-- build time:Tue Jul 22 2025 11:01:57 GMT+0800 (中国标准时间) --><!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#FFF"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/ico" sizes="32x32" href="/images/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="alternate" type="application/rss+xml" title="枯萎的花将在另一彼岸悄然绽放" href="https://yunhdan.github.io/rss.xml"><link rel="alternate" type="application/atom+xml" title="枯萎的花将在另一彼岸悄然绽放" href="https://yunhdan.github.io/atom.xml"><link rel="alternate" type="application/json" title="枯萎的花将在另一彼岸悄然绽放" href="https://yunhdan.github.io/feed.json"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CInconsolata:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/css/app.css?v=0.2.5"><link rel="canonical" href="https://yunhdan.github.io/ai/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"><title>强化学习 - 人工智能 | Runfar's Zone = 枯萎的花将在另一彼岸悄然绽放</title><meta name="generator" content="Hexo 6.3.0"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">强化学习</h1><div class="meta"><span class="item" title="创建时间：2025-03-01 17:04:23"><span class="icon"><i class="ic i-calendar"></i> </span><span class="text">发表于</span> <time itemprop="dateCreated datePublished" datetime="2025-03-01T17:04:23+08:00">2025-03-01</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span class="text">本文字数</span> <span>7.1k</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span class="text">阅读时长</span> <span>6 分钟</span></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="切换导航栏"><span class="line"></span> <span class="line"></span> <span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">Runfar's Zone</a></li></ul><ul class="right"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div id="imgs" class="pjax"><ul><li class="item" data-background-image="https://s2.loli.net/2024/12/07/6Bxol9QWmLbTAJr.jpg"></li><li class="item" data-background-image="https://s2.loli.net/2024/12/07/3tSu8pcfUG1eivj.jpg"></li><li class="item" data-background-image="https://s2.loli.net/2024/12/07/WK8pgGa4EM5nBNX.jpg"></li><li class="item" data-background-image="https://s2.loli.net/2024/12/04/eWvx2STbizqcHZl.jpg"></li><li class="item" data-background-image="https://s2.loli.net/2025/05/18/7hN6PLzoCXFSZxQ.jpg"></li><li class="item" data-background-image="https://s2.loli.net/2024/12/07/QT4KaMBUogywHsd.png"></li></ul></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"/></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"/><use xlink:href="#gentle-wave" x="48" y="3"/><use xlink:href="#gentle-wave" x="48" y="5"/><use xlink:href="#gentle-wave" x="48" y="7"/></g></svg></div><main><div class="inner"><div id="main" class="pjax"><div class="article wrap"><div class="breadcrumb" itemscope itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i> <span><a href="/">首页</a></span><i class="ic i-angle-right"></i> <span class="current" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/ai/" itemprop="item" rel="index" title="分类于 人工智能"><span itemprop="name">人工智能</span></a><meta itemprop="position" content="1"></span></div><article itemscope itemtype="http://schema.org/Article" class="post block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://yunhdan.github.io/ai/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/me.jpg"><meta itemprop="name" content="Runhua Deng"><meta itemprop="description" content=", 计算机视觉 & 图像恢复"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="枯萎的花将在另一彼岸悄然绽放"></span><div class="body md" itemprop="articleBody"><p>:::info</p><p>学习自李宏毅与蘑菇书。蘑菇书链接：<span class="exturl" data-url="aHR0cHM6Ly9kYXRhd2hhbGVjaGluYS5naXRodWIuaW8vZWFzeS1ybC8jLw==">蘑菇书EasyRL (datawhalechina.github.io)</span></p><p>搭配李宏毅的强化学习课程使用最佳。</p><p>这篇博文内容结合了自己的理解，不可避免地会存在不当，欢迎指正。</p><p>:::</p><h1 id="情景定义-rainbow"><a href="#情景定义-rainbow" class="headerlink" title="[情景定义]{.rainbow}"></a>[情景定义]{.rainbow}</h1><p>智能体<code>Actor</code>、环境<code>Environment</code>与奖励<code>Reward</code>。在后面的内容中，你都可以将智能体理解为玩游戏的机器人，将环境理解为游戏主机，奖励理解为机器人玩游戏干掉怪兽得到的分数。</p><h1 id="策略梯度算法（Policy-gradient-Algorithm）-rainbow"><a href="#策略梯度算法（Policy-gradient-Algorithm）-rainbow" class="headerlink" title="[策略梯度算法（Policy gradient Algorithm）]{.rainbow}"></a>[策略梯度算法（Policy gradient Algorithm）]{.rainbow}</h1><p>智能体通常作为<code>Actor</code>，在 ++策略++ 不断调整与指引下使得其在环境重获取到最大的奖励。策略在具体的实现上，现代强化学习通常用网络替代之。在前面的比喻中，你可以将策略理解为机器人取得高分的方法和手段。</p><h2 id="符号定义"><a href="#符号定义" class="headerlink" title="++符号定义++"></a>++符号定义++</h2><div class="table-container"><table><thead><tr><th style="text-align:center">符号</th><th style="text-align:center">释义</th></tr></thead><tbody><tr><td style="text-align:center">$t$</td><td style="text-align:center">同一时间下的某个状态</td></tr><tr><td style="text-align:center">$T$</td><td style="text-align:center">某个轨迹下的所有状态数</td></tr><tr><td style="text-align:center">$a_t$</td><td style="text-align:center">某个状态下智能体的动作</td></tr><tr><td style="text-align:center">$s_t$</td><td style="text-align:center">某个状态下的环境</td></tr><tr><td style="text-align:center">$\theta$</td><td style="text-align:center">智能体的策略模型参数</td></tr><tr><td style="text-align:center">$r_t$</td><td style="text-align:center">某个状态下智能体采取动作后的奖励</td></tr><tr><td style="text-align:center">$\tau_i$</td><td style="text-align:center">某次轨迹：所有状态的环境与动作的组合。$\tau=\{s_1,a_1,s_2,a_2,…s_t,a_t\}$</td></tr><tr><td style="text-align:center">$p$</td><td style="text-align:center">概率</td></tr><tr><td style="text-align:center">$R$</td><td style="text-align:center">总奖励</td></tr></tbody></table></div><h2 id="手推策略梯度公式"><a href="#手推策略梯度公式" class="headerlink" title="++手推策略梯度公式++"></a>++手推策略梯度公式++</h2><p>给定智能体<code>Actor</code>的参数$\theta$，可以计算轨迹$\tau$的发生的概率：</p><script type="math/tex;mode=display">p_{\theta}(\tau) = p(s_1)p_{\theta}(a_1|s_1)p(s_2|s_1,a_1)p_{\theta}(a_2|s_2)p(s_3|s_2,a_2)\cdots    \\
=p(s_1)\prod_{t=1}^{T}p_{\theta}(a_t|s_t)p(s_{t+1}|s_t,a_t)    \tag{1.1}</script><p>$p_{\theta}(a_1|s_1)$是策略里面的网络参数$\theta$决定的、观察到环境$s_1$后采取的动作概率。因此策略网络的输出是一个分布，是智能体采取动作的概率分布。$p(s_2|s_1,a_1)$是环境根据前一个环境的状态和智能体的动作给出的下一个环境状态，通常是环境内部的规则决定。</p><p>一个轨迹$\tau$会在某个时刻终止，其内部的每一个$s_t$和$a_t$的组合均能够产生对应的奖励$r_t$。所以，所有的组合能够得到一个关于这个轨迹的总奖励$R(\tau)$。</p><p>我们的目的就是要调整策略网络的参数$\theta$使得总奖励$R(\tau)$越大越好。因此，$R(\tau)$是一个随机的变量，我们可以计算$R(\tau)$：</p><script type="math/tex;mode=display">R(\tau) = \sum_{t=1}^{T}r_tp_t(r_t|s_t,a_t)    \tag{1.2}</script><p>当策略参数$\theta$给定，那么在这组参数上一定有一个关于轨迹的分布$p_{\theta}(\tau)$，智能体的多次尝试都是一个轨迹。对于所有的轨迹$\tau_i$，存在总奖励$\overline{R}_{\theta}$的期望值为：</p><script type="math/tex;mode=display">\overline{R}_{\theta} = \sum_{\tau}R(\tau)p_{\theta}(\tau) = \mathbb{E}_{\tau \sim p_{\theta}(\tau)}[R(\tau)]    \tag{1.3}</script><p>为了让 ++期望奖励++ 越大越好，要进行 ++梯度上升++ 。所以要计算期望奖励关于策略参数$\theta$的梯度：</p><script type="math/tex;mode=display">\nabla \overline{R}_{\theta} = \sum_{\tau}R(\tau)\nabla p_{\theta}(\tau)    \\
\because \nabla f(x) = f(x) \nabla logf(x) (不难证得)    \\
\therefore \frac{\nabla p_{\theta(\tau)}}{p_{\theta(\tau)}} = \nabla logp_{\theta}(\tau)</script><p>因此有：</p><script type="math/tex;mode=display">\nabla \overline{R}_{\theta} = \sum_{\tau}R(\tau)p_{\theta}(\tau)\frac{\nabla p_{\theta}(\tau)}{p_{\theta}(\tau)}    \\
= \sum_{\tau}R(\tau)p_{\theta}(\tau)\nabla logp_{\theta}(\tau)     \\
= \mathbb{E}_{\tau \sim p_{\theta}(\tau)}[R(\tau)\nabla logp_{\theta}(\tau)]</script><p>但是在实际上，我们是无法直接和准确地求期望值$\mathbb{E}_{\tau \sim p_{\theta}(\tau)}[R(\tau)\nabla logp_{\theta}(\tau)]$的。但是我们可以通过采样<code>N</code>个$\tau$，然后计算$R(\tau)\nabla logp_{\theta}(\tau)$的<code>N</code>个和，来近似地得到这个期望：</p><script type="math/tex;mode=display">\nabla \overline{R}_{\theta} = \mathbb{E}_{\tau \sim p_{\theta}(\tau)}[R(\tau)\nabla logp_{\theta}(\tau)] ≈ \frac{1}{N}\sum_{n=1}^{N}R(\tau^n)\nabla log p_{\theta}(\tau^n)    \\
= \frac{1}{N}\sum_{n=1}^{N}\sum_{t=1}^{T_n}R(\tau^n)\nabla log p_{\theta}(a_t^n|s_t^n)    \tag{1.4}</script><p>得到最后一行的原因，是因为$\nabla log p_{\theta}(\tau^n)$可以被展开：</p><script type="math/tex;mode=display">\nabla log p_{\theta}(\tau^n) = \nabla \left( logp(s_1)+\sum_{t=1}^{T}logp_{\theta}(a_t|s_t) + \sum_{t=1}^{T}logp(s_{t+1}|s_t,a_t)\right)    \\
= \nabla logp(s_1) + \nabla \sum_{t=1}^{T}logp_{\theta}(a_t|s_t)+\nabla \sum_{t=1}^{T}logp(s_{t+1}|s_t,a_t)    \\</script><p>因为，$p(s_1)$和$p(s_{t+1}|s_t,a_t)$来自环境，与智能体的策略参数$\theta$无关，因此第一项和第三项为0，原式化简：</p><script type="math/tex;mode=display">= \nabla \sum_{t=1}^{T}logp_{\theta}(a_t|s_t)    \\
= \sum_{t=1}^{T} \nabla logp_{\theta}(a_t|s_t)</script><h2 id="训练思路"><a href="#训练思路" class="headerlink" title="++训练思路++"></a>++训练思路++</h2><p>现在整理一下思路。在给出一个初始策略参数$\theta$的情况下，经过训练，我们要得到一个不错的策略参数$\theta$，使得智能体在多个轨迹下得到的总奖励$\overline{R}_{\theta}$最大化。如何优化这个$\theta$呢？我们可以通过梯度上升法优化这个$\theta$，使得当$s_t$状态下执行$a_t$导致$R(\tau)$增大时增加在$s_t$下执行$a_t$的概率，反之减小这个概率：</p><script type="math/tex;mode=display">\theta \leftarrow \theta + \eta \nabla \overline{R}_{\theta}    \tag{1.5}</script><p>根据公式1.4和公式1.5，我们知道优化$\theta$的关键在于求出$\nabla \overline{R}_{\theta}$，与$\nabla \overline{R}_{\theta}$有关的是$s_t^n$、$a_t^n$和$\tau^n$。所以我们要采集这些数据：</p><script type="math/tex;mode=display">\tau^1:(s_1^1, a_1^1) \quad\quad R(\tau^1)    \\
\quad\quad (s_2^1, a_2^1) \quad\quad R(\tau^1)    \\
\quad\quad (s_3^1, a_3^1) \quad\quad R(\tau^1)    \\
\quad\quad\quad \vdots \quad\quad\quad\quad\quad \vdots    \\
\tau^2:(s_1^2, a_1^2) \quad\quad R(\tau^2)    \\
\quad\quad (s_2^2, a_2^2) \quad\quad R(\tau^2)    \\
\quad\quad (s_3^2, a_3^2) \quad\quad R(\tau^2)    \\
\quad\quad\quad \vdots \quad\quad\quad\quad\quad \vdots    \\</script><p>$\tau$的采集量是人为设定的，采集完一次后，会获得多组$\tau$的数据，然后使用公式1.4和公式1.5一次性更新模型：</p><p><img data-src="../../assets/reinforce.jpg" alt="image"></p><h2 id="直观理解"><a href="#直观理解" class="headerlink" title="++直观理解++"></a>++直观理解++</h2><p>如何直观理解公式1.4？既然我们要通过求出$\nabla \overline{R}_{\theta}$来得到$\theta$的更新值，就不得不依赖反向传播得到梯度。你可以想象这样一个场景：智能体在玩一个游戏，控制一个飞机射击外星人，它的策略$\theta$是智能体内部的网络<code>NN</code>（<code>Neural Network</code>），在游戏过程中保持不变（游戏过程就是在采样若干组轨迹的<code>s</code>，<code>a</code>，<code>R</code>(<script type="math/tex">\tau</script>)对），智能体看到场景$s$，送给策略网络<code>NN</code>计算出要做的动作$a$的概率。如，向左移动的概率，向右移动的概率，开火的概率，所以这本质上是一个分类问题。</p><p><img data-src="../../assets/reinforce2.jpg" alt=""></p><p>通常在训练过程中，我们需要智能体根据当前场景做出的动作是人为指定的，也就是说对于某个轨迹$\tau$的一个特定的$s$有个<code>Ground-Truth</code>，这个<code>Ground-Truth</code>就是在这个特定场景应该做出的动作$a$。比如，在某个场景$s_t$下，我们想要智能体向左，于是标签就是<code>[1, 0, 0]</code>。智能体将$s_t$作为策略网络$\theta$的输入，估计出向左的概率要尽可能接近1，即$p_{\theta}(a_t|s_t)$要尽可能接近1。</p><p>所以，在采样过程中，大量的$s_t^n$计算出大量的$p_{\theta}(a_t|s_t)$，那么根据每个$s_t$的标签，就能够计算一个交叉熵损失（<code>cross entropy loss</code>），最小化交叉熵损失就是在最大化似然：</p><script type="math/tex;mode=display">Loss = - \sum_{t=1}^{T} y_i \hat{y}_i    \\
Min(Loss) = Min(- \sum_{t=1}^{T} y_i \hat{y}_i) \\
= Max(\sum_{t=1}^{T} y_i p_{\theta}(a_t|s_t))    \\
\because y_i是常数1    \\
\therefore Min(Loss) = Max(\sum_{t=1}^{T} p_{\theta}(a_t|s_t)) = Max(\sum_{t=1}^{T} log p_{\theta}(a_t|s_t))</script><p>所以，在训练过程中，最小化交叉熵损失，实际上在最大化某个轨迹的预测值的和。观察上面的式子，有没有发现在最小化损失，实际上在最大化$\nabla \overline{R}_{\theta}$。</p><p>在计算出损失后，通过反向传播，可以直接计算出$\nabla \overline{R}_{\theta}$的值，因此就能更新参数$\theta$了。</p><h2 id="实现上的一些技巧"><a href="#实现上的一些技巧" class="headerlink" title="++实现上的一些技巧++"></a>++实现上的一些技巧++</h2><h3 id="添加基线-dot"><a href="#添加基线-dot" class="headerlink" title="++添加基线++{.dot}"></a>++添加基线++{.dot}</h3><p>我们总是希望，对于某一给定的状态$s$采取动作$a$后，整场游戏$\tau$获得的奖励是正的，我们就增加$(s,a)$的概率。如果这个给定的状态$s$采取的动作$a$使得最后整场游戏$\tau$的奖励是负的，我们就减小$(s,a)$的概率。</p><p>理想情况是这样，现实是虽然这些动作对整场游戏$\tau$的奖励贡献有大有小，但整场游戏下来的奖励总是非负的，有的动作采取后得到了20分，有的动作采取后得到0分。这种情况下，一场游戏的奖励$R(\tau)$总是正的，最低也只是0，而且要求提升贡献度大的动作的概率，降低贡献度低的动作的概率。</p><p>另外一个现实是，由于本质上一个轨迹$\tau$只是在轨迹空间$p_\theta(\tau)$的一个采样，所以采样的数量较少时，一些动作可能未被采样到。那么相较于其他被采样到的动作而言，这个未被采样到的动作的概率就会被下降。这并不意味着这个未被采样到的动作贡献更小，仅仅只是未被采样到而已。相反，贡献度低的动作因为奖励总是正的、经常被采样到，所以概率提升地比未被采样、但贡献度高的动作幅度大。这就导致了不公平的出现。</p><p><img data-src="../../assets/reinforce3.jpg" alt="image"></p><p>奖励总是正的，就会导致丈量动作的贡献度相较于奖励有正有负而更加困难。除此之外，还引出了概率提升膨胀的问题，未被采样到的动作的概率不升反降，而其他采样到的动作因为奖励总是正的而概率得到很大幅度的提升。</p><p>为了解决这个问题，可以把奖励减去一个基线<code>b</code>：</p><script type="math/tex;mode=display">\nabla \overline{R}_\theta ≈ \frac{1}{N}\sum_{n=1}^{N}\sum_{t=1}^{T_n}(R(\tau^n) - b)\nabla log p_{\theta}(a_t^n|s_t^n)    \tag{1.5}</script><p>此时总奖励就是$R(\tau^n) - b$。我们可以令$b≈E[R(\tau)]$，也就是说，我们在训练中不断地将$R(\tau)$的值记录下来，然后不断地记录$R(\tau)$的平均值，将这个平均值当作$b$来使用。这样一来，总奖励$R(\tau^n) - b$就会有正有负。</p><h3 id="分配合适的分数-dot"><a href="#分配合适的分数-dot" class="headerlink" title="++分配合适的分数++{.dot}"></a>++分配合适的分数++{.dot}</h3><p>观察公式1.5，只要在同一场游戏里（同一轨迹），所有的动作-状态对都要使用同样的奖励权重进行加权。也就是说，对于$(a_1^1,s_1^1),(a_2^1,s_2^1),(a_3^1,s_3^1),…,(a_T^1,s_T^1)$都使用$R(\tau^1)-b$进行加权，当$n=2$时同理。</p><p>这是不公平的，在蘑菇书中结合例子解释的非常深入浅出。简单来说就是，一场游戏的结果是好的，并不意味着每一个采取的动作都是好的。相反，若是整场游戏的结果不好，并不代表每一个动作都是不好的。</p><p>一种解决办法是，计算某个动作状态对的奖励权重时，不把整场游戏的奖励加起来，而是只计算从这个动作执行以后到整场游戏结束时得到的奖励。这样做是因为这个动作执行之前发生的事情是与这个动作没有关系的，所以执行当前这个动作之前所获得的所有奖励都不能算作是当前这个动作的贡献。将执行这个动作后获得所有奖励加起来，才算做这个动作真正的贡献。</p><p><img data-src="../../assets/reinforce4.jpg" alt="image"></p><p>比如，这张图$(s_a,a_1)$的权重是$(+5+0-2)=+3$，$(s_b,a_2)$的权重是$(+0-2)=-2$。即使是第二场游戏也是如此规律。</p><p>于是，重写公式1.5：</p><script type="math/tex;mode=display">\nabla \overline{R}_\theta ≈ \frac{1}{N}\sum_{n=1}^{N}\sum_{t=1}^{T_n}(\sum_{t'=t}^{T_n}{r_{t'}^{n}} - b)\nabla log p_{\theta}(a_t^n|s_t^n)    \tag{1.6}</script><p>原来的权重是整场游戏的奖励的总和，现在改成从某个时刻$t$开始，假设这个动作是在开$t$始执行的，从$t$一直到游戏结束所有奖励的总和才能代表这个动作的好坏。</p><p>进一步，我们可以为未来的奖励做折扣，继续改写公式1.6：</p><script type="math/tex;mode=display">\nabla \overline{R}_\theta ≈ \frac{1}{N}\sum_{n=1}^{N}\sum_{t=1}^{T_n}(\sum_{t'=t}^{T_n}{\gamma^{t'-t}r_{t'}^{n}} - b)\nabla log p_{\theta}(a_t^n|s_t^n)    \tag{1.7}</script><p>为什么为未来时刻的奖励乘一个系数做折扣？因为当前时刻动作对下一时刻影响较大，但是随着时间推移，到某一时刻$t$时所受那个动作影响就越来越小。可以取系数$\gamma=0.9或0.99 \in [0,1]$。例如，假设游戏有两个回合，我们在游戏的第二回合的某一个 $s_t$执行$a_t$得到+1分，在$s_{t+1}$执行$a_{t+1}$得到+3分，在$s_{t+2}$执行$a_{t+2}$得到−5分，第二回合结束。$a_t$的分数应该是：$1 + \gamma \times 3 + \gamma^2 \times (-5)$。</p><h3 id="优势函数与评论员-dot"><a href="#优势函数与评论员-dot" class="headerlink" title="++优势函数与评论员++{.dot}"></a>++优势函数与评论员++{.dot}</h3><p>观察公式1.7。事实上，$b$通常是一个网络估计出来的，是一个网络的输出。而$\sum_{t’=t}^{T_n}{\gamma^{t’-t}r_{t’}^{n}} - b$这一项通常简写为$A^\theta(s_t,a_t)$，被称为优势函数。此时，公式1.7又可以写成：</p><script type="math/tex;mode=display">\nabla \overline{R}_\theta = \mathbb{E}_{\tau \sim p_\theta(\tau)}[R(\tau)\nabla logp_\theta(\tau)] ≈ \frac{1}{N}\sum_{n=1}^{N}\sum_{t=1}^{T_n}A^\theta(s_t,a_t)\nabla log p_{\theta}(a_t^n|s_t^n)    \tag{1.8}</script><p>这个优势函数也通常可以被一个网络估计出来，这个网络被称为评论员。优势函数的意义是，在某个特定状态$s_t$下采取动作$a_t$，相较于其他可能执行的动作，$a_t$有多好。这个优势函数作为权重反映了动作相对的好，而不是绝对的好。</p><h1 id="近端策略优化算法（PPO）-rainbow"><a href="#近端策略优化算法（PPO）-rainbow" class="headerlink" title="[近端策略优化算法（PPO）]{.rainbow}"></a>[近端策略优化算法（PPO）]{.rainbow}</h1><h2 id="On-Policy-to-Off-Policy"><a href="#On-Policy-to-Off-Policy" class="headerlink" title="++On Policy to Off Policy++"></a>++On Policy to Off Policy++</h2><h1 id="深度Q网络"><a href="#深度Q网络" class="headerlink" title="深度Q网络"></a>深度Q网络</h1></div><footer><div class="meta"><span class="item"><span class="icon"><i class="ic i-calendar-check"></i> </span><span class="text">更新于</span> <time title="修改时间：2025-05-25 11:29:23" itemprop="dateModified" datetime="2025-05-25T11:29:23+08:00">2025-05-25</time> </span><span id="ai/强化学习/" class="item leancloud_visitors" data-flag-title="强化学习" title="阅读次数"><span class="icon"><i class="ic i-eye"></i> </span><span class="text">阅读次数</span> <span class="leancloud-visitors-count"></span> <span class="text">次</span></span></div><div class="reward"><button><i class="ic i-heartbeat"></i> 赞赏</button><p>请我喝[茶]~(￣▽￣)~*</p><div id="qr"><div><img data-src="/images/wechatpay.jpg" alt="Runhua Deng 微信支付"><p>微信支付</p></div><div><img data-src="/images/alipay.png" alt="Runhua Deng alipay"><p>alipay</p></div><div><img data-src="/images/paypal.png" alt="Runhua Deng paypal"><p>paypal</p></div></div></div><div id="copyright"><ul><li class="author"><strong>本文作者： </strong>Runhua Deng <i class="ic i-at"><em>@</em></i>枯萎的花将在另一彼岸悄然绽放</li><li class="link"><strong>本文链接：</strong> <a href="https://yunhdan.github.io/ai/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" title="强化学习">https://yunhdan.github.io/ai/强化学习/</a></li><li class="license"><strong>版权声明： </strong>本站所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</span> 许可协议。转载请注明出处！</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/cs/Pip%E3%80%81Conda%E3%80%81github%E9%95%9C%E5%83%8F/" itemprop="url" rel="prev" data-background-image="https:&#x2F;&#x2F;s2.loli.net&#x2F;2024&#x2F;12&#x2F;04&#x2F;6buchRFIEO4qjsa.jpg" title="Pip、Conda、Github镜像"><span class="type">上一篇</span> <span class="category"><i class="ic i-flag"></i> 计算机</span><h3>Pip、Conda、Github镜像</h3></a></div><div class="item right"><a href="/baoyan/Embodied-3D-Labs/" itemprop="url" rel="next" data-background-image="https:&#x2F;&#x2F;s2.loli.net&#x2F;2025&#x2F;05&#x2F;18&#x2F;7hN6PLzoCXFSZxQ.jpg" title="Embodied AI Labs"><span class="type">下一篇</span> <span class="category"><i class="ic i-flag"></i> 保研</span><h3>Embodied AI Labs</h3></a></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="文章目录"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%83%85%E6%99%AF%E5%AE%9A%E4%B9%89-rainbow"><span class="toc-number">1.</span> <span class="toc-text">[情景定义]{.rainbow}</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E7%AE%97%E6%B3%95%EF%BC%88Policy-gradient-Algorithm%EF%BC%89-rainbow"><span class="toc-number">2.</span> <span class="toc-text">[策略梯度算法（Policy gradient Algorithm）]{.rainbow}</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%A6%E5%8F%B7%E5%AE%9A%E4%B9%89"><span class="toc-number">2.1.</span> <span class="toc-text">++符号定义++</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%89%8B%E6%8E%A8%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E5%85%AC%E5%BC%8F"><span class="toc-number">2.2.</span> <span class="toc-text">++手推策略梯度公式++</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%80%9D%E8%B7%AF"><span class="toc-number">2.3.</span> <span class="toc-text">++训练思路++</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3"><span class="toc-number">2.4.</span> <span class="toc-text">++直观理解++</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0%E4%B8%8A%E7%9A%84%E4%B8%80%E4%BA%9B%E6%8A%80%E5%B7%A7"><span class="toc-number">2.5.</span> <span class="toc-text">++实现上的一些技巧++</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B7%BB%E5%8A%A0%E5%9F%BA%E7%BA%BF-dot"><span class="toc-number">2.5.1.</span> <span class="toc-text">++添加基线++{.dot}</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E9%85%8D%E5%90%88%E9%80%82%E7%9A%84%E5%88%86%E6%95%B0-dot"><span class="toc-number">2.5.2.</span> <span class="toc-text">++分配合适的分数++{.dot}</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E5%8A%BF%E5%87%BD%E6%95%B0%E4%B8%8E%E8%AF%84%E8%AE%BA%E5%91%98-dot"><span class="toc-number">2.5.3.</span> <span class="toc-text">++优势函数与评论员++{.dot}</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%BF%91%E7%AB%AF%E7%AD%96%E7%95%A5%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%EF%BC%88PPO%EF%BC%89-rainbow"><span class="toc-number">3.</span> <span class="toc-text">[近端策略优化算法（PPO）]{.rainbow}</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#On-Policy-to-Off-Policy"><span class="toc-number">3.1.</span> <span class="toc-text">++On Policy to Off Policy++</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6Q%E7%BD%91%E7%BB%9C"><span class="toc-number">4.</span> <span class="toc-text">深度Q网络</span></a></li></ol></div><div class="related panel pjax" data-title="系列文章"><ul><li><a href="/ai/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/" rel="bookmark" title="注意力机制">注意力机制</a></li><li class="active"><a href="/ai/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="bookmark" title="强化学习">强化学习</a></li><li><a href="/ai/Deep-Learning-Experiment-Tricks/" rel="bookmark" title="Deep Learning Experiment Tricks">Deep Learning Experiment Tricks</a></li><li><a href="/ai/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/" rel="bookmark" title="深度学习理论">深度学习理论</a></li><li><a href="/ai/%E7%8E%B0%E4%BB%A3%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="bookmark" title="现代深度学习">现代深度学习</a></li></ul></div><div class="overview panel" data-title="站点概览"><div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="image" itemprop="image" alt="Runhua Deng" data-src="/images/me.jpg"><p class="name" itemprop="name">Runhua Deng</p><div class="description" itemprop="description">计算机视觉 & 图像恢复</div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">45</span> <span class="name">文章</span></a></div><div class="item categories"><a href="/categories/"><span class="count">7</span> <span class="name">分类</span></a></div></nav><div class="social"><span class="exturl item github" data-url="aHR0cHM6Ly9naXRodWIuY29tL1l1bkhEYW4=" title="https:&#x2F;&#x2F;github.com&#x2F;YunHDan"><i class="ic i-github"></i></span> <span class="exturl item music" data-url="aHR0cHM6Ly9tdXNpYy4xNjMuY29tLyMvdXNlci9ob21lP2lkPTIxMjYzMjE4OTI=" title="https:&#x2F;&#x2F;music.163.com&#x2F;#&#x2F;user&#x2F;home?id&#x3D;2126321892"><i class="ic i-cloud-music"></i></span> <a href="/alex2312666252@gmail.com" title="alex2312666252@gmail.com" class="item email"><i class="ic i-envelope"></i></a> <span class="exturl item csdn" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L20wXzczNTk5NzM4P3NwbT0xMDAwLjIxMTUuMzAwMS41MzQz" title="https:&#x2F;&#x2F;blog.csdn.net&#x2F;m0_73599738?spm&#x3D;1000.2115.3001.5343"><i class="ic i-link"></i></span></div><ul class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>首页</a></li><li class="item dropdown"><a href="/about/" rel="section"><i class="ic i-user"></i>关于</a><ul class="submenu"><li class="item"><a href="/about/me/" rel="section"><i class="ic i-file"></i>简历</a></li><li class="item"><a href="/about/learn_me/" rel="section"><i class="ic i-smile"></i>了解我</a></li></ul></li><li class="item dropdown"><a href="javascript:void(0);"><i class="ic i-feather"></i>文章</a><ul class="submenu"><li class="item"><a href="/archives/" rel="section"><i class="ic i-list-alt"></i>归档</a></li><li class="item"><a href="/categories/" rel="section"><i class="ic i-th"></i>分类</a></li><li class="item"><a href="/tags/" rel="section"><i class="ic i-tags"></i>标签</a></li></ul></li><li class="item"><a href="/friends/" rel="section"><i class="ic i-heart"></i>友達</a></li></ul></div></div></div><ul id="quick"><li class="prev pjax"><a href="/cs/Pip%E3%80%81Conda%E3%80%81github%E9%95%9C%E5%83%8F/" rel="prev" title="上一篇"><i class="ic i-chevron-left"></i></a></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/baoyan/Embodied-3D-Labs/" rel="next" title="下一篇"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>随机文章</h2><ul><li class="item"><div class="breadcrumb"><a href="/categories/baoyan/" title="分类于 保研">保研</a></div><span><a href="/baoyan/%E4%BF%9D%E7%A0%94%E5%8E%86%E7%A8%8B/" title="保研历程">保研历程</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/cs/" title="分类于 计算机">计算机</a></div><span><a href="/cs/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%90%86%E8%AE%BA2/" title="数据库理论2">数据库理论2</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 人工智能">人工智能</a></div><span><a href="/ai/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/" title="注意力机制">注意力机制</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/baoyan/" title="分类于 保研">保研</a></div><span><a href="/baoyan/Low-level-Vision-Group/" title="Low-level-Vision-Group">Low-level-Vision-Group</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/baoyan/" title="分类于 保研">保研</a></div><span><a href="/baoyan/Embodied-3D-Labs/" title="Embodied AI Labs">Embodied AI Labs</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 人工智能">人工智能</a></div><span><a href="/ai/Deep-Learning-Experiment-Tricks/" title="Deep Learning Experiment Tricks">Deep Learning Experiment Tricks</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/cs/" title="分类于 计算机">计算机</a></div><span><a href="/cs/Linux/" title="Linux">Linux</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/baoyan/" title="分类于 保研">保研</a></div><span><a href="/baoyan/%E4%BF%9D%E7%A0%94%E4%BF%A1%E6%81%AF%E6%B1%87%E6%80%BB/" title="保研信息汇总">保研信息汇总</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/cs/" title="分类于 计算机">计算机</a></div><span><a href="/cs/Springboot%E3%80%81Mybatis/" title="Springboot、Mybatis">Springboot、Mybatis</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/research/" title="分类于 学术">学术</a></div><span><a href="/research/Mamba%E4%B8%B2%E7%83%A7/" title="Mamba串烧">Mamba串烧</a></span></li></ul></div><div><h2>最新评论</h2><ul class="leancloud-recent-comment"></ul></div></div><div class="status"><div class="copyright">&copy; 2023 – <span itemprop="copyrightYear">2025</span> <span class="with-love"><i class="ic i-sakura rotate"></i> </span><span class="author" itemprop="copyrightHolder">Runhua Deng @ Runfar's Zone</span></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-chart-area"></i> </span><span title="站点总字数">196k 字</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="ic i-coffee"></i> </span><span title="站点阅读时长">2:58</span></div><div class="powered-by">基于 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL={path:"ai/强化学习/",favicon:{show:"（●´3｀●）やれやれだぜ",hide:"(´Д｀)大変だ！"},search:{placeholder:"文章搜索",empty:"关于 「 ${query} 」，什么也没搜到",stats:"${time} ms 内找到 ${hits} 条结果"},valine:!0,copy_tex:!0,katex:!0,fancybox:!0,copyright:'复制成功，转载请遵守 <i class="ic i-creative-commons"></i>BY-NC-SA 协议。',ignores:[function(e){return e.includes("#")},function(e){return new RegExp(LOCAL.path+"$").test(e)}]}</script><script src="https://cdn.polyfill.io/v2/polyfill.js"></script><script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script><script src="/js/app.js?v=0.2.5"></script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({log:!1,pluginJsPath:"lib/",pluginModelPath:"assets/",pluginRootPath:"live2dw/",tagMode:!1})</script></body></html><!-- rebuild by hrmmi -->