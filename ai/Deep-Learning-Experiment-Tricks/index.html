<!-- build time:Tue Jul 22 2025 11:00:57 GMT+0800 (中国标准时间) --><!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#FFF"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/ico" sizes="32x32" href="/images/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="alternate" type="application/rss+xml" title="枯萎的花将在另一彼岸悄然绽放" href="https://yunhdan.github.io/rss.xml"><link rel="alternate" type="application/atom+xml" title="枯萎的花将在另一彼岸悄然绽放" href="https://yunhdan.github.io/atom.xml"><link rel="alternate" type="application/json" title="枯萎的花将在另一彼岸悄然绽放" href="https://yunhdan.github.io/feed.json"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CInconsolata:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/css/app.css?v=0.2.5"><link rel="canonical" href="https://yunhdan.github.io/ai/Deep-Learning-Experiment-Tricks/"><title>Deep Learning Experiment Tricks - 人工智能 | Runfar's Zone = 枯萎的花将在另一彼岸悄然绽放</title><meta name="generator" content="Hexo 6.3.0"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">Deep Learning Experiment Tricks</h1><div class="meta"><span class="item" title="创建时间：2025-03-02 23:58:57"><span class="icon"><i class="ic i-calendar"></i> </span><span class="text">发表于</span> <time itemprop="dateCreated datePublished" datetime="2025-03-02T23:58:57+08:00">2025-03-02</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span class="text">本文字数</span> <span>15k</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span class="text">阅读时长</span> <span>13 分钟</span></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="切换导航栏"><span class="line"></span> <span class="line"></span> <span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">Runfar's Zone</a></li></ul><ul class="right"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div id="imgs" class="pjax"><ul><li class="item" data-background-image="https://s2.loli.net/2025/05/18/6Vb1fpcOe2lzunk.jpg"></li><li class="item" data-background-image="https://s2.loli.net/2024/12/07/mONZDMUz524GbtI.jpg"></li><li class="item" data-background-image="https://s2.loli.net/2024/12/07/WVCXkEfT2dSregQ.jpg"></li><li class="item" data-background-image="https://s2.loli.net/2025/05/18/lSDdiHmpCWQyLxF.jpg"></li><li class="item" data-background-image="https://s2.loli.net/2024/12/07/iTFAlstDuy3ZO78.jpg"></li><li class="item" data-background-image="https://s2.loli.net/2024/12/04/KqSM9hOFDC1uxaH.jpg"></li></ul></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"/></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"/><use xlink:href="#gentle-wave" x="48" y="3"/><use xlink:href="#gentle-wave" x="48" y="5"/><use xlink:href="#gentle-wave" x="48" y="7"/></g></svg></div><main><div class="inner"><div id="main" class="pjax"><div class="article wrap"><div class="breadcrumb" itemscope itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i> <span><a href="/">首页</a></span><i class="ic i-angle-right"></i> <span class="current" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/ai/" itemprop="item" rel="index" title="分类于 人工智能"><span itemprop="name">人工智能</span></a><meta itemprop="position" content="1"></span></div><article itemscope itemtype="http://schema.org/Article" class="post block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://yunhdan.github.io/ai/Deep-Learning-Experiment-Tricks/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/me.jpg"><meta itemprop="name" content="Runhua Deng"><meta itemprop="description" content=", 计算机视觉 & 图像恢复"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="枯萎的花将在另一彼岸悄然绽放"></span><div class="body md" itemprop="articleBody"><p>:::info</p><p>一般在一个新的<code>trick</code>和<code>experience</code>开坑时，都会先暂时粗略地搬运一些其他地方的内容，或者简略描述。偶尔精进与专门研究时，会特别地丰富和细致化该内容。</p><p>:::</p><h1 id="pytorch-mssim-ssim的使用"><a href="#pytorch-mssim-ssim的使用" class="headerlink" title="pytorch_mssim.ssim的使用"></a><code>pytorch_mssim.ssim</code>的使用</h1><p>以下面计算<code>ssim</code>的代码为例：</p><pre><code class="lang-python">ssim_value = ssim(final, gt_batch, data_range=2.0, size_average=True)
</code></pre><p><code>data_range</code>表示图像像素值的动态范围（最大值与最小值的差）。如果输入图像经过归一化处理（如 <code>transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])</code>），则像素值范围会被映射到<code>[-1, 1]</code>。此时 <code>data_range</code> 应设为 <code>2.0</code>（因为 <code>1 - (-1) = 2</code>），而不是 <code>1.0</code>。</p><p><code>SSIM</code>默认假设输入范围是<code>[0, 1]</code>（当 <code>data_range=1</code>）或 <code>[0, 255]</code>（当 <code>data_range=255</code>）。</p><p><code>size_average=True</code> 会将 <code>SSIM</code> 值在所有图像和通道上取平均。<code>pytorch_msssim</code>的新版本使用reduction取代这个参数，重写为：<code>reduction=&#39;mean&#39;</code>。</p><p>有时候，应该确保输入给<code>ssim</code>做计算时的数据范围在<code>[-1, 1]</code>之间，不然，则要裁剪处理：</p><pre><code class="lang-python">final_clamped = torch.clamp(final, 0.0, 1.0)
ssim_value = ssim(final_clamped, gt_batch, data_range=1.0, size_average=True)
</code></pre><h1 id="损失函数下降、震荡、上升的原因"><a href="#损失函数下降、震荡、上升的原因" class="headerlink" title="损失函数下降、震荡、上升的原因"></a>损失函数下降、震荡、上升的原因</h1><blockquote><p>在训练包含多个损失函数的模型时，各子损失函数的变化趋势能够反映模型的学习动态和优化方向。以下是不同趋势的详细分析及应对策略：</p><hr><h4 id="一、损失函数整体持续下降"><a href="#一、损失函数整体持续下降" class="headerlink" title="一、损失函数整体持续下降"></a><strong>一、损失函数整体持续下降</strong></h4><h4 id="含义解析"><a href="#含义解析" class="headerlink" title="含义解析"></a><strong>含义解析</strong></h4><ul><li><strong>良性学习信号</strong>：模型正在有效优化该任务目标，权重分配合理，数据质量良好。</li><li><strong>潜在风险</strong>：<ul><li><strong>过拟合倾向</strong>：若验证集对应指标未同步下降，可能过拟合训练数据。</li><li><strong>任务主导性</strong>：其他损失未充分优化，模型可能偏向该任务。</li></ul></li></ul><h4 id="典型案例"><a href="#典型案例" class="headerlink" title="典型案例"></a><strong>典型案例</strong></h4><ul><li>重建损失（如L1/L2）持续下降，但对抗损失震荡 → 模型过度拟合像素级精度，忽视生成真实性。</li></ul><h4 id="应对策略"><a href="#应对策略" class="headerlink" title="应对策略"></a><strong>应对策略</strong></h4><ol><li><strong>验证泛化性</strong>：检查验证集对应指标是否同步改善</li><li><strong>调整权重</strong>：若其他损失停滞，适当降低该损失权重（如从1.0→0.7）</li><li><strong>早停机制</strong>：当验证损失不再下降时停止训练</li></ol><hr><h4 id="二、损失函数震荡波动"><a href="#二、损失函数震荡波动" class="headerlink" title="二、损失函数震荡波动"></a><strong>二、损失函数震荡波动</strong></h4><h4 id="含义解析-1"><a href="#含义解析-1" class="headerlink" title="含义解析"></a><strong>含义解析</strong></h4><ul><li><strong>优化不稳定</strong>：学习率过高、批次过小或损失间存在冲突。</li><li><strong>数据问题</strong>：噪声数据或类别不均衡导致梯度方向不一致。</li><li><strong>对抗性博弈</strong>：典型于GAN的判别器与生成器损失交替上升。</li></ul><h4 id="数值特征"><a href="#数值特征" class="headerlink" title="数值特征"></a><strong>数值特征</strong></h4><ul><li><strong>高频震荡</strong>（如±5%）：常由学习率过大引起</li><li><strong>低频震荡</strong>（如每5个epoch变化）：多任务目标冲突</li></ul><h4 id="典型案例-1"><a href="#典型案例-1" class="headerlink" title="典型案例"></a><strong>典型案例</strong></h4><ul><li>分类损失下降但正则化损失震荡 → L2正则化强度过高导致参数更新不稳定</li></ul><h4 id="应对策略-1"><a href="#应对策略-1" class="headerlink" title="应对策略"></a><strong>应对策略</strong></h4><ol><li><strong>降低学习率</strong>：将初始学习率减少3-5倍（如2e-4→5e-5）</li><li><strong>增大批次大小</strong>：从32提升至128，稳定梯度估计</li><li><strong>梯度裁剪</strong>：设置<code>max_grad_norm=1.0</code></li><li><strong>冲突分析</strong>：计算损失梯度余弦相似度，对负相关损失解耦训练</li></ol><hr><h4 id="三、损失函数持续上升"><a href="#三、损失函数持续上升" class="headerlink" title="三、损失函数持续上升"></a><strong>三、损失函数持续上升</strong></h4><h4 id="含义解析-2"><a href="#含义解析-2" class="headerlink" title="含义解析"></a><strong>含义解析</strong></h4><ul><li><strong>严重警告信号</strong>：模型在该任务上性能退化，优化方向错误。</li><li><strong>常见诱因</strong>：<ul><li><strong>损失权重倒置</strong>：如误将权重设为负数</li><li><strong>任务本质冲突</strong>：如超分辨率任务中，L1损失下降但感知损失上升</li><li><strong>数值不稳定</strong>：梯度爆炸导致损失进入病态区域</li></ul></li></ul><h4 id="典型案例-2"><a href="#典型案例-2" class="headerlink" title="典型案例"></a><strong>典型案例</strong></h4><ul><li>对抗损失上升而重建损失下降 → 判别器过强导致生成器无法有效学习</li></ul><h4 id="应对策略-2"><a href="#应对策略-2" class="headerlink" title="应对策略"></a><strong>应对策略</strong></h4><ol><li><strong>立即暂停训练</strong>：检查损失计算代码和权重符号</li><li><strong>损失权重热力图</strong>：可视化各损失对总损失的贡献比例</li><li><strong>渐进式训练</strong>：分阶段引入上升的损失项（如先用L1预训练，第50epoch加入对抗损失）</li><li><strong>架构改进</strong>：对于根本性冲突，修改网络结构（如增加多尺度特征融合模块）</li></ol><hr><h4 id="四、综合优化建议"><a href="#四、综合优化建议" class="headerlink" title="四、综合优化建议"></a><strong>四、综合优化建议</strong></h4><ol><li><strong>动态权重调整</strong>：采用不确定性加权法（如《Multi-Task Learning Using Uncertainty to Weigh Losses》）<pre><code class="lang-python"># 各损失自动加权示例
log_var = torch.nn.Parameter(torch.zeros(3)) # 3个损失
loss = 0.5*(loss1/torch.exp(log_var[0]) + loss2/torch.exp(log_var[1]) + log_var.sum())
</code></pre></li><li><strong>损失相关性监控</strong>：计算各损失间的Pearson相关系数矩阵，识别冲突组合</li><li><strong>课程学习策略</strong>：早期侧重易优化损失（如L1），后期加强高阶损失（如SSIM、VGG感知损失）</li><li><strong>可视化工具</strong>：使用TensorBoard的并行坐标视图对比超参数与损失关系</li></ol><hr><h4 id="五、调试检查清单"><a href="#五、调试检查清单" class="headerlink" title="五、调试检查清单"></a><strong>五、调试检查清单</strong></h4><p>当出现异常损失趋势时，按以下顺序排查：</p><ol><li><strong>数值检查</strong>：<ul><li>确认损失计算未出现NaN/Inf</li><li>检查梯度幅值（<code>torch.nn.utils.clip_grad_norm_</code>）</li></ul></li><li><strong>数据流验证</strong>：<pre><code class="lang-python"># 数据检查代码片段
for batch in val_loader:
    print(batch[&#39;image&#39;].min(), batch[&#39;image&#39;].max()) # 应为[0,1]或[-1,1]
    visualize(batch[&#39;image&#39;][0]) # 肉眼验证图像质量
</code></pre></li><li><strong>权重合理性</strong>：确保各损失量级匹配（如L1≈0.1，对抗损失≈2.0时，需调整权重平衡）</li><li><strong>模型容量测试</strong>：在小数据集（如100样本）上过拟合，验证能否达到预期损失</li></ol><p>通过系统分析损失动态，可精准定位模型优化瓶颈，实现多目标协同优化。</p></blockquote><h1 id="transforms-Resize"><a href="#transforms-Resize" class="headerlink" title="transforms.Resize()"></a><code>transforms.Resize()</code></h1><p>这个与<code>RandomCrop()</code>还不太一样，<code>Resize()</code>是等比例的缩放原图。所以存在一定的信息损失，一般不使用这种操作。</p><h1 id="不确定性损失加权法——多任务损失均衡"><a href="#不确定性损失加权法——多任务损失均衡" class="headerlink" title="不确定性损失加权法——多任务损失均衡"></a>不确定性损失加权法——多任务损失均衡</h1><blockquote><p>以下是使用不确定性加权法改造后的损失函数实现：</p><pre><code class="lang-python">import torch
import torch.nn as nn
import torch.nn.functional as F
from pytorch_msssim import ssim

class UncertaintyWeightedLoss(nn.Module):
    def __init__(self, trans, num_tasks=5):
        &quot;&quot;&quot;
        trans: HVI转换器实例
        num_tasks: 需要加权的损失项数量（这里包含L1, SSIM, Res, Cons, HVI）
        &quot;&quot;&quot;
        super().__init__()
        self.trans = trans
        # 初始化可学习的不确定性参数（log方差）
        self.log_vars = nn.Parameter(torch.zeros(num_tasks))
        # 初始化参数（可选）
        nn.init.uniform_(self.log_vars, -3, -1)  # 初始方差在0.05~0.37之间

    def compute_losses(self, final, gt, output, S1, P1, S2, P2):
        &quot;&quot;&quot;分解计算各个基础损失项&quot;&quot;&quot;
        # RGB空间损失
        l1_loss = F.l1_loss(final, gt)
        ssim_loss = 1 - ssim(final, gt, data_range=1.0, size_average=True)

        # 残差一致性损失
        loss_res1 = F.mse_loss(S1, P1 + S2) / (S1.detach().var() + 1e-6)
        loss_res2 = F.mse_loss(S2, P2 + S1) / (S2.detach().var() + 1e-6)
        res_loss = loss_res1 + loss_res2

        # 下采样一致性损失
        g1, g2 = pair_downsampler(output)
        cons_loss = F.l1_loss(S1, P1 + g1) + F.l1_loss(S2, P2 + g2)

        # HVI空间损失
        final_hvi = self.trans.HVIT(final)
        gt_hvi = self.trans.HVIT(gt)
        hvi_l1 = F.l1_loss(final_hvi, gt_hvi)
        hvi_edge = edge_loss(final_hvi, gt_hvi)

        return [l1_loss, ssim_loss, res_loss, cons_loss, hvi_l1, hvi_edge]

    def forward(self, final, gt, output, S1, P1, S2, P2):
        # 获取所有基础损失项
        losses = self.compute_losses(final, gt, output, S1, P1, S2, P2)

        # 应用不确定性加权
        total_loss = 0.0
        for i, loss in enumerate(losses):
            precision = torch.exp(-self.log_vars[i])
            total_loss += precision * loss + self.log_vars[i]

        # 返回总损失和详细损失项（用于监控）
        loss_details = &#123;
            &#39;total&#39;: total_loss,
            &#39;l1&#39;: losses[0],
            &#39;ssim&#39;: losses[1],
            &#39;res&#39;: losses[2],
            &#39;cons&#39;: losses[3],
            &#39;hvi_l1&#39;: losses[4],
            &#39;hvi_edge&#39;: losses[5],
            &#39;log_vars&#39;: self.log_vars
        &#125;
        return total_loss, loss_details
</code></pre><p>主要改进点说明：</p><ol><li><strong>模块化设计</strong>：<br>```python<h1 id="初始化方式变化"><a href="#初始化方式变化" class="headerlink" title="初始化方式变化"></a>初始化方式变化</h1>trans = RGB_HVI().to(device)<br>criterion = UncertaintyWeightedLoss(trans).to(device)</li></ol><h1 id="前向计算变化"><a href="#前向计算变化" class="headerlink" title="前向计算变化"></a>前向计算变化</h1><p>total_loss, loss_details = criterion(final, gt, output, S1, P1, S2, P2)</p><pre><code>
2. **动态权重机制**：
- 每个损失项自动获得权重：weight = exp(-log_var)
- 包含正则项：log_var 防止方差无限增大
- 初始权重范围：exp(-3)=0.05 ~ exp(-1)=0.37

3. **训练监控增强**：
```python
# 在训练循环中添加监控
writer.add_scalars(&#39;Loss/Train&#39;, &#123;
    &#39;total&#39;: loss_details[&#39;total&#39;].item(),
    &#39;l1&#39;: loss_details[&#39;l1&#39;].item(),
    &#39;ssim&#39;: loss_details[&#39;ssim&#39;].item(),
    &#39;res&#39;: loss_details[&#39;res&#39;].item(),
    &#39;cons&#39;: loss_details[&#39;cons&#39;].item(),
    &#39;hvi_l1&#39;: loss_details[&#39;hvi_l1&#39;].item(),
    &#39;hvi_edge&#39;: loss_details[&#39;hvi_edge&#39;].item()
&#125;, total_iter)

# 监控不确定性参数
for i, log_var in enumerate(loss_details[&#39;log_vars&#39;]):
    writer.add_scalar(f&#39;Uncertainty/var_&#123;i&#125;&#39;, torch.exp(log_var).item(), epoch)
</code></pre><ol><li><p><strong>优化器调整</strong>：</p><pre><code class="lang-python"># 需要将损失参数加入优化器
optimizer = optim.Adam(
 list(model.parameters()) + list(criterion.parameters()),
 lr=args.lr, 
 betas=(0.9, 0.999)
)
</code></pre></li><li><p><strong>梯度稳定性处理</strong>：</p><pre><code class="lang-python"># 在训练步骤中添加梯度裁剪
torch.nn.utils.clip_grad_norm_(
 list(model.parameters()) + list(criterion.parameters()),
 max_norm=1.0
)
</code></pre></li></ol><p>该实现通过以下方式解决原代码问题：</p><ol><li><strong>自动平衡机制</strong>：</li></ol><ul><li>难任务（高方差项）自动获得低权重</li><li>简单任务（低方差项）获得高权重</li><li>通过反向传播自动调整权重</li></ul><ol><li><strong>数值稳定性改进</strong>：</li></ol><ul><li>使用log方差代替直接学习方差，避免除零错误</li><li>初始值限制在合理范围</li><li>梯度裁剪防止参数爆炸</li></ul><ol><li><strong>可解释性增强</strong>：</li></ol><ul><li>通过TensorBoard可直接观察每个任务的：<ul><li>损失值变化曲线</li><li>自适应权重变化（exp(-log_var)）</li><li>方差参数演化</li></ul></li></ul><p>实际使用时需要注意：</p><ol><li><p><strong>参数初始化</strong>：</p><pre><code class="lang-python"># 如果某些任务需要强调，可以调整初始值
nn.init.constant_(self.log_vars[4], -2.0)  # 强调HVI_L1损失
</code></pre></li><li><p><strong>学习率设置</strong>：</p><pre><code class="lang-python"># 为损失参数设置更小的学习率
optimizer = optim.Adam([
    &#123;&#39;params&#39;: model.parameters(), &#39;lr&#39;: 1e-4&#125;,
    &#123;&#39;params&#39;: criterion.parameters(), &#39;lr&#39;: 1e-5&#125;
])
</code></pre></li><li><p><strong>多阶段训练</strong>：</p><pre><code class="lang-python"># 初始阶段固定部分损失权重
if epoch &lt; warmup_epochs:
    criterion.log_vars.requires_grad_(False)
else:
    criterion.log_vars.requires_grad_(True)
</code></pre></li></ol><p>这种实现方式比手动调整lambda参数更科学，尤其在处理类似HVI这类新增损失时，能自动平衡其与原有损失项的关系。</p></blockquote><h1 id="余弦退火学习率调度器"><a href="#余弦退火学习率调度器" class="headerlink" title="余弦退火学习率调度器"></a>余弦退火学习率调度器</h1><p>一代经典的学习率调度器。</p><p>模板代码参考如：</p><pre><code class="lang-python"># ... 其他导入 ...
from torch.optim.lr_scheduler import CosineAnnealingLR

def main():
    # 参数解析
    parser = argparse.ArgumentParser(&quot;MAI_Denoised_Train&quot;)
    # ... 原有参数 ...
    parser.add_argument(&#39;--min_lr&#39;, type=float, default=1e-6, help=&#39;minimum learning rate for cosine annealing&#39;)
    args = parser.parse_args()

    # ... 模型初始化 ...

    # 初始化优化器和调度器
    optimizer = optim.Adam(model.parameters(), lr=args.lr, betas=(0.9, 0.999), weight_decay=1e-6)
    scheduler = CosineAnnealingLR(optimizer, T_max=args.epochs, eta_min=args.min_lr)

    # 训练循环
    for epoch in range(args.epochs):
        # 训练过程...

        # 验证过程...

        # 更新学习率
        scheduler.step()

        # 记录学习率
        current_lr = scheduler.get_last_lr()[0]
        writer.add_scalar(&#39;Learning Rate&#39;, current_lr, epoch)
        logging.info(f&#39;Epoch [&#123;epoch+1&#125;/&#123;args.epochs&#125;] Learning Rate: &#123;current_lr:.7f&#125;&#39;)

    # ... 后续代码 ...
</code></pre><h1 id="Optuna自动化调参"><a href="#Optuna自动化调参" class="headerlink" title="Optuna自动化调参"></a><code>Optuna</code>自动化调参</h1><blockquote><p>为了使用 Optuna 进行超参数调优，我们需要对代码进行以下主要修改。这里展示关键修改步骤（完整代码见最后）：</p><pre><code class="lang-python">import optuna
from optuna.trial import Trial

def objective(trial):
    # 定义可调参数范围
    args = argparse.Namespace(
        batch_size=trial.suggest_categorical(&#39;batch_size&#39;, [32, 64, 128]),
        lr=trial.suggest_float(&#39;lr&#39;, 1e-6, 1e-3, log=True),
        lambda_hvi1=trial.suggest_float(&#39;lambda_hvi1&#39;, 0.001, 0.1),
        lambda_hvi2=trial.suggest_float(&#39;lambda_hvi2&#39;, 0.001, 0.1),
        lambda_rgb1=trial.suggest_float(&#39;lambda_rgb1&#39;, 0.1, 2),
        lambda_rgb2=trial.suggest_float(&#39;lambda_rgb2&#39;, 0.1, 2),
        # 保留其他固定参数...
        cuda=True,
        gpu=&#39;cuda:0&#39;,
        epochs=100,  # 调试时可减少epoch
        save=f&#39;./EXP/&#123;trial.number&#125;&#39;  # 为每个trial创建独立目录
    )

    # 运行训练并返回验证指标
    final_psnr = train_model(args, trial)
    return final_psnr

def train_model(args, trial=None):
    # [原main()函数内容在这里重构]
    # 在验证循环中报告中间结果
    if trial:
        trial.report(val_psnr, epoch)
        if trial.should_prune():
            raise optuna.exceptions.TrialPruned()

    return best_val_psnr  # 返回需要优化的指标

if __name__ == &quot;__main__&quot;:
    # 创建Optuna study
    study = optuna.create_study(
        direction=&#39;maximize&#39;,
        sampler=optuna.samplers.TPESampler(),
        pruner=optuna.pruners.MedianPruner()
    )

    # 运行优化
    study.optimize(objective, n_trials=50, timeout=3600*24)

    # 输出结果
    print(&quot;Best trial:&quot;)
    trial = study.best_trial
    print(f&quot;  Value: &#123;trial.value&#125;&quot;)
    print(&quot;  Params: &quot;)
    for key, value in trial.params.items():
        print(f&quot;    &#123;key&#125;: &#123;value&#125;&quot;)
</code></pre><p>主要修改点说明：</p><ol><li><p><strong>参数建议</strong>：使用 trial.suggest_* 方法替代原argparse参数：</p><pre><code class="lang-python">trial.suggest_categorical(&#39;batch_size&#39;, [32, 64, 128])
trial.suggest_float(&#39;lr&#39;, 1e-6, 1e-3, log=True)
</code></pre></li><li><p><strong>目标函数</strong>：将训练过程封装到目标函数中，返回验证PSNR作为优化指标</p></li><li><p><strong>中间报告</strong>：在训练过程中定期报告验证指标，支持提前终止：</p><pre><code class="lang-python">trial.report(val_psnr, epoch)
if trial.should_prune():
    raise optuna.exceptions.TrialPruned()
</code></pre></li><li><p><strong>独立目录</strong>：为每个trial创建独立的保存目录，避免文件冲突：</p><pre><code class="lang-python">args.save = f&#39;./EXP/&#123;trial.number&#125;&#39;
</code></pre></li><li><p><strong>Study配置</strong>：创建优化study时指定优化方向（最大化PSNR）和采样策略：</p><pre><code class="lang-python">study = optuna.create_study(direction=&#39;maximize&#39;)
</code></pre></li></ol><p>完整整合后的代码示例：</p><pre><code class="lang-python">import optuna
from optuna.trial import Trial
import argparse
import torch

def main():
    # 原始训练代码...

def objective(trial):
    # 定义超参数搜索空间
    params = &#123;
        &#39;batch_size&#39;: trial.suggest_categorical(&#39;batch_size&#39;, [32, 64, 128]),
        &#39;lr&#39;: trial.suggest_float(&#39;lr&#39;, 1e-6, 1e-3, log=True),
        &#39;lambda_hvi1&#39;: trial.suggest_float(&#39;lambda_hvi1&#39;, 0.001, 0.1),
        &#39;lambda_hvi2&#39;: trial.suggest_float(&#39;lambda_hvi2&#39;, 0.001, 0.1),
        &#39;lambda_rgb1&#39;: trial.suggest_float(&#39;lambda_rgb1&#39;, 0.5, 2.0),
        &#39;lambda_rgb2&#39;: trial.suggest_float(&#39;lambda_rgb2&#39;, 0.5, 2.0),
        &#39;lambda_res&#39;: trial.suggest_float(&#39;lambda_res&#39;, 0.5, 2.0),
        &#39;lambda_cons&#39;: trial.suggest_float(&#39;lambda_cons&#39;, 0.5, 2.0),
    &#125;

    # 固定参数
    fixed_params = &#123;
        &#39;cuda&#39;: True,
        &#39;gpu&#39;: &#39;cuda:0&#39;,
        &#39;epochs&#39;: 100,  # 调优时epoch可以适当减少
        &#39;data_dir&#39;: &#39;/path/to/data&#39;,
        &#39;save&#39;: f&#39;./EXP/trial_&#123;trial.number&#125;&#39;,
    &#125;

    # 合并参数
    args = argparse.Namespace(**&#123;**params, **fixed_params&#125;)

    # 运行训练
    best_psnr = train_with_args(args, trial)
    return best_psnr

def train_with_args(args, trial=None):
    # [原main()函数内容重构到这里]
    # 初始化模型、数据加载等...

    best_psnr = 0
    for epoch in range(args.epochs):
        # 训练循环...

        # 验证循环
        with torch.no_grad():
            val_psnr = evaluate(model, val_loader)

            # 向Optuna报告中间结果
            if trial:
                trial.report(val_psnr, epoch)

                # 提前终止
                if trial.should_prune():
                    raise optuna.exceptions.TrialPruned()

        if val_psnr &gt; best_psnr:
            best_psnr = val_psnr
            torch.save(model.state_dict(), f&quot;&#123;args.save&#125;/best_model.pth&quot;)

    return best_psnr

if __name__ == &quot;__main__&quot;:
    # 创建Optuna study
    study = optuna.create_study(
        direction=&#39;maximize&#39;,
        sampler=optuna.samplers.TPESampler(),
        pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=10)
    )

    # 运行优化
    study.optimize(objective, n_trials=50, timeout=3600*24)

    # 输出最佳结果
    print(&quot;Best trial:&quot;)
    trial = study.best_trial
    print(f&quot;  PSNR: &#123;trial.value&#125;&quot;)
    print(&quot;  Params: &quot;)
    for key, value in trial.params.items():
        print(f&quot;    &#123;key&#125;: &#123;value&#125;&quot;)
</code></pre><p>关键优化技巧：</p><ol><li><p><strong>参数范围设置</strong>：根据参数性质选择适当的范围：</p><pre><code class="lang-python">trial.suggest_float(&#39;lr&#39;, 1e-6, 1e-3, log=True)  # 对数尺度更适合学习率
trial.suggest_categorical(&#39;batch_size&#39;, [32, 64, 128])
</code></pre></li><li><p><strong>提前终止</strong>：使用MedianPruner避免资源浪费：</p><pre><code class="lang-python">pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=10)
</code></pre></li><li><p><strong>并行优化</strong>：通过指定n_jobs并行运行：</p><pre><code class="lang-python">study.optimize(objective, n_trials=100, n_jobs=4)
</code></pre></li><li><p><strong>持久化存储</strong>：使用数据库保存进度：</p><pre><code class="lang-python">study = optuna.create_study(
    storage=&#39;sqlite:///optuna.db&#39;,
    study_name=&#39;denoising_study&#39;,
    load_if_exists=True
)
</code></pre></li></ol><p>注意事项：</p><ol><li><p><strong>资源管理</strong>：调优时适当减少epoch数量（如50-100），最终训练时再用完整epoch</p></li><li><p><strong>参数空间</strong>：初始搜索使用较宽范围，后期可基于初步结果缩小范围</p></li><li><p><strong>指标选择</strong>：建议使用验证集PSNR作为优化目标，而非训练损失</p></li><li><p><strong>随机种子</strong>：为保持可比性，可在每个trial中固定随机种子：</p><pre><code class="lang-python">torch.manual_seed(trial.suggest_int(&#39;seed&#39;, 0, 1000))
</code></pre></li><li><p><strong>GPU内存</strong>：注意batch_size与GPU显存的匹配，建议在suggest_categorical中包含可行值</p></li></ol><p>这种集成方式可以在不破坏原有训练逻辑的基础上，系统性地探索超参数空间。最终可以通过study.best_trial.params获取最佳参数组合，用于最终模型的训练。</p></blockquote><h1 id="混合精度训练"><a href="#混合精度训练" class="headerlink" title="混合精度训练"></a>混合精度训练</h1><h1 id="梯度累积"><a href="#梯度累积" class="headerlink" title="梯度累积"></a>梯度累积</h1><p>通过多次小批量迭代累积梯度，模拟大 <code>Batch Size</code> 的效果，模板代码参考可见下方：</p><pre><code class="lang-python">accumulation_steps = 4  # 累积4个batch的梯度
for i, (inputs, labels) in enumerate(dataloader):
    outputs = model(inputs)
    loss = criterion(outputs, labels)
    loss = loss / accumulation_steps  # 损失按累积步数缩放
    loss.backward()

    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
</code></pre><h1 id="torchinfo统计模型的显存占用"><a href="#torchinfo统计模型的显存占用" class="headerlink" title="torchinfo统计模型的显存占用"></a><code>torchinfo</code>统计模型的显存占用</h1><p>参考如下代码：</p><pre><code class="lang-python">from torchinfo import summary
model = MyModel().cuda()
summary(model, input_size=(batch_size, 3, 256, 256))
</code></pre><h1 id="显存分析器memory-profiler"><a href="#显存分析器memory-profiler" class="headerlink" title="显存分析器memory_profiler"></a>显存分析器<code>memory_profiler</code></h1><p>这个工具可以可以统计每行代码的显存变化</p><pre><code class="lang-python">from pytorch_memlab import LineProfiler

@profile
def train_batch(inputs, labels):
    outputs = model(inputs)
    loss = criterion(outputs, labels)
    loss.backward()
    optimizer.step()

# 运行后会打印每行代码的显存变化
train_batch(inputs, labels)
</code></pre><h1 id="Dataloader的num-workers设置"><a href="#Dataloader的num-workers设置" class="headerlink" title="Dataloader的num_workers设置"></a><code>Dataloader</code>的<code>num_workers</code>设置</h1><p><code>num_workers</code>通常设置为0，<code>CPU</code>线程数的<code>75%</code>，<code>CPU</code>线程数，<code>CPU</code>线程数的两倍。</p><p><code>CPU</code>的线程数计算公式为：线程数 = 逻辑核心数 = 物理核心数 * 单核线程数。</p><p><code>num_workers</code>很大程度上影响<code>GPU</code>的占用率。保持长时间的高<code>GPU</code>占用率是高效率训练深度学习的基础。</p><h1 id="损失为Nan的分析"><a href="#损失为Nan的分析" class="headerlink" title="损失为Nan的分析"></a>损失为<code>Nan</code>的分析</h1><blockquote><p>损失值出现NaN（Not a Number）通常由数值不稳定引起，以下是可能原因及解决方案：</p><h4 id="1-输入数据问题"><a href="#1-输入数据问题" class="headerlink" title="1. 输入数据问题"></a>1. <strong>输入数据问题</strong></h4><ul><li><p><strong>检查数据中的NaN或异常值</strong>：确保输入数据无缺失或无效值。</p><pre><code class="lang-python">import numpy as np
print(&quot;NaN in data:&quot;, np.isnan(data).any())
print(&quot;数据范围:&quot;, data.min(), data.max())
</code></pre></li><li><p><strong>数据标准化/归一化</strong>：过大或过小的输入值可能导致梯度爆炸。</p><pre><code class="lang-python">data = (data - data.mean()) / data.std()  # 标准化
</code></pre></li></ul><h4 id="2-学习率过高"><a href="#2-学习率过高" class="headerlink" title="2. 学习率过高"></a>2. <strong>学习率过高</strong></h4><ul><li><strong>降低学习率</strong>：过大的学习率会导致参数更新不稳定。<pre><code class="lang-python">optimizer = torch.optim.SGD(model.parameters(), lr=0.01)  # 初始学习率设为0.01或更小
</code></pre></li></ul><h4 id="3-损失函数实现问题"><a href="#3-损失函数实现问题" class="headerlink" title="3. 损失函数实现问题"></a>3. <strong>损失函数实现问题</strong></h4><ul><li><p><strong>避免对零取对数</strong>：在交叉熵损失中增加极小值ε（如1e-8）。</p><pre><code class="lang-python">loss = -tf.reduce_sum(y_true * tf.math.log(y_pred + 1e-8))
</code></pre></li><li><strong>使用框架内置函数</strong>：如TensorFlow的<code>CategoricalCrossentropy(from_logits=True)</code>，避免手动实现中的错误。</li></ul><h4 id="4-梯度爆炸（前提是你的其他代码得写对）"><a href="#4-梯度爆炸（前提是你的其他代码得写对）" class="headerlink" title="4. 梯度爆炸（前提是你的其他代码得写对）"></a>4. <strong>梯度爆炸（前提是你的其他代码得写对）</strong></h4><ul><li><p><strong>梯度裁剪</strong>：限制梯度最大范数。</p><pre><code class="lang-python"># PyTorch示例
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
</code></pre><pre><code class="lang-python"># TensorFlow示例
gradients = tape.gradient(loss, model.trainable_variables)
gradients, _ = tf.clip_by_global_norm(gradients, 1.0)
optimizer.apply_gradients(zip(gradients, model.trainable_variables))
</code></pre></li></ul><h4 id="5-模型结构问题"><a href="#5-模型结构问题" class="headerlink" title="5. 模型结构问题"></a>5. <strong>模型结构问题</strong></h4><ul><li><strong>激活函数与输出层匹配</strong>：分类任务最后一层需用Softmax（或配合<code>from_logits=True</code>）。</li><li><strong>权重初始化</strong>：使用He/Xavier初始化避免初始值过大。<pre><code class="lang-python"># PyTorch示例
torch.nn.init.kaiming_normal_(layer.weight)
</code></pre></li></ul><h4 id="6-数值稳定性技巧"><a href="#6-数值稳定性技巧" class="headerlink" title="6. 数值稳定性技巧"></a>6. <strong>数值稳定性技巧</strong></h4><ul><li><strong>添加Batch Normalization</strong>：稳定层间输出分布。<pre><code class="lang-python">model.add(tf.keras.layers.BatchNormalization())
</code></pre></li><li><strong>混合精度训练</strong>：使用FP16时，开启梯度缩放。<pre><code class="lang-python">optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)
</code></pre></li></ul><h4 id="7-调试步骤"><a href="#7-调试步骤" class="headerlink" title="7. 调试步骤"></a>7. <strong>调试步骤</strong></h4><ul><li><strong>小数据集测试</strong>：用少量样本过拟合，快速复现问题。</li><li><strong>打印中间结果</strong>：检查前向传播输出和梯度。<pre><code class="lang-python"># 检查输出层
print(&quot;模型输出:&quot;, outputs)
# 检查梯度
for name, param in model.named_parameters():
    if param.grad is not None:
        print(f&quot;梯度 &#123;name&#125;: &#123;param.grad.norm()&#125;&quot;)
</code></pre></li></ul><h4 id="8-其他可能原因"><a href="#8-其他可能原因" class="headerlink" title="8. 其他可能原因"></a>8. <strong>其他可能原因</strong></h4><ul><li><strong>正则化过强</strong>：降低L2正则化系数。</li><li><strong>数据预处理错误</strong>：检查标准化时是否除以零（如方差为零的特征）。</li></ul><h4 id="总结流程"><a href="#总结流程" class="headerlink" title="总结流程"></a>总结流程</h4><ol><li><strong>检查输入数据</strong>：确保无NaN且已标准化。</li><li><strong>降低学习率</strong>：尝试0.001或更低。</li><li><strong>验证损失函数</strong>：使用内置函数或添加ε。</li><li><strong>梯度裁剪</strong>：限制梯度大小。</li><li><strong>检查模型结构</strong>：激活函数、初始化、添加BatchNorm。</li><li><strong>逐步调试</strong>：缩小数据范围，打印中间变量。</li></ol><p>通过以上步骤逐步排查，通常可以定位并解决NaN损失问题。</p></blockquote><h1 id="torchvision-utils-save-image"><a href="#torchvision-utils-save-image" class="headerlink" title="torchvision.utils.save_image"></a><code>torchvision.utils.save_image</code></h1><p>有个参数叫做<code>Normalize</code>，这个将数值映射到<code>[0,255]</code>的区间。相关使用说明如下：</p><pre><code class="lang-python">当设置normalize=True时：
- 会自动将张量的数值范围从[min, max]线性映射到[0, 255]
- 例如：输入张量范围是[-1, 1]，会被映射到0-255
- 例如：输入张量范围是[0, 1]，会被映射到0-255（相当于直接乘以255）
</code></pre><p><code>torchvision.utils.save_image</code>保存图像要求图像的数值范围必须是指定范围，即在<code>[0,1]</code>或<code>[0,255]</code>。如果数据范围在其他区间，则需要保证数据范围符合<code>torchvision.utils.save_image</code>的要求，可通过设置<code>normalize</code>为<code>True</code>解决这个问题。</p><h1 id="确保Python优先加载本地项目的代码而不是Anaconda环境中的库"><a href="#确保Python优先加载本地项目的代码而不是Anaconda环境中的库" class="headerlink" title="确保Python优先加载本地项目的代码而不是Anaconda环境中的库"></a>确保<code>Python</code>优先加载本地项目的代码而不是<code>Anaconda</code>环境中的库</h1><p>情景：<code>TinyNeuralNetwork</code>库代码在项目文件夹<code>Retinexformer</code>下面，<code>Anaconda</code>也有一个<code>TinyNeuralNetwork</code>库。现在我们在本地更新了<code>TinyNeuralNetwork</code>库代码，想要运行更新后的库代码中的<code>convert.py</code>代码。这个时候Python有可能会在执行新库代码<code>convert.py</code>的时候，调用<code>Anaconda</code>环境的旧库代码。</p><p>一种方法是指定优先级，强制优先加载项目中的本地库：</p><pre><code class="lang-python">import sys
import os

# 获取当前脚本所在目录（TinyNeuralNetwork文件夹的路径）
TINYNN_DIR = os.path.dirname(os.path.abspath(__file__))
# 获取项目根目录（假设 TinyNeuralNetwork 是 Retinexformer 的子目录）
PROJECT_ROOT = os.path.dirname(TINYNN_DIR)

# 将本地库路径插入到 sys.path 的最前面
sys.path.insert(0, TINYNN_DIR)
sys.path.insert(0, PROJECT_ROOT)

# 打印验证路径是否正确添加（可选）
print(&quot;当前 Python 路径:&quot;)
for p in sys.path:
    print(p)
</code></pre><p>另一种方式就是在终端执行<code>convert.py</code>而不是在<code>IDE</code>中运行：</p><pre><code class="lang-python">&gt; cd Retinexformer
&gt; export PYTHONPATH=&quot;$PWD:$PYTHONPATH&quot;
&gt; python convert.py
</code></pre></div><footer><div class="meta"><span class="item"><span class="icon"><i class="ic i-calendar-check"></i> </span><span class="text">更新于</span> <time title="修改时间：2025-07-06 02:17:25" itemprop="dateModified" datetime="2025-07-06T02:17:25+08:00">2025-07-06</time> </span><span id="ai/Deep-Learning-Experiment-Tricks/" class="item leancloud_visitors" data-flag-title="Deep Learning Experiment Tricks" title="阅读次数"><span class="icon"><i class="ic i-eye"></i> </span><span class="text">阅读次数</span> <span class="leancloud-visitors-count"></span> <span class="text">次</span></span></div><div class="reward"><button><i class="ic i-heartbeat"></i> 赞赏</button><p>请我喝[茶]~(￣▽￣)~*</p><div id="qr"><div><img data-src="/images/wechatpay.jpg" alt="Runhua Deng 微信支付"><p>微信支付</p></div><div><img data-src="/images/alipay.png" alt="Runhua Deng alipay"><p>alipay</p></div><div><img data-src="/images/paypal.png" alt="Runhua Deng paypal"><p>paypal</p></div></div></div><div id="copyright"><ul><li class="author"><strong>本文作者： </strong>Runhua Deng <i class="ic i-at"><em>@</em></i>枯萎的花将在另一彼岸悄然绽放</li><li class="link"><strong>本文链接：</strong> <a href="https://yunhdan.github.io/ai/Deep-Learning-Experiment-Tricks/" title="Deep Learning Experiment Tricks">https://yunhdan.github.io/ai/Deep-Learning-Experiment-Tricks/</a></li><li class="license"><strong>版权声明： </strong>本站所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</span> 许可协议。转载请注明出处！</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/baoyan/Embodied-3D-Labs/" itemprop="url" rel="prev" data-background-image="https:&#x2F;&#x2F;s2.loli.net&#x2F;2024&#x2F;12&#x2F;07&#x2F;iTFAlstDuy3ZO78.jpg" title="Embodied AI Labs"><span class="type">上一篇</span> <span class="category"><i class="ic i-flag"></i> 保研</span><h3>Embodied AI Labs</h3></a></div><div class="item right"><a href="/other/%E5%8D%9A%E5%AE%A2%E6%96%87%E6%A1%A3%E7%BE%8E%E5%8C%96%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E/" itemprop="url" rel="next" data-background-image="https:&#x2F;&#x2F;s2.loli.net&#x2F;2024&#x2F;12&#x2F;04&#x2F;1te6EQPpcdZFOw2.jpg" title="博客文档美化使用说明"><span class="type">下一篇</span> <span class="category"><i class="ic i-flag"></i> 琐碎</span><h3>博客文档美化使用说明</h3></a></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="文章目录"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#pytorch-mssim-ssim%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="toc-number">1.</span> <span class="toc-text">pytorch_mssim.ssim的使用</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8B%E9%99%8D%E3%80%81%E9%9C%87%E8%8D%A1%E3%80%81%E4%B8%8A%E5%8D%87%E7%9A%84%E5%8E%9F%E5%9B%A0"><span class="toc-number">2.</span> <span class="toc-text">损失函数下降、震荡、上升的原因</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%95%B4%E4%BD%93%E6%8C%81%E7%BB%AD%E4%B8%8B%E9%99%8D"><span class="toc-number">2.0.0.1.</span> <span class="toc-text">一、损失函数整体持续下降</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%90%AB%E4%B9%89%E8%A7%A3%E6%9E%90"><span class="toc-number">2.0.0.2.</span> <span class="toc-text">含义解析</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B8%E5%9E%8B%E6%A1%88%E4%BE%8B"><span class="toc-number">2.0.0.3.</span> <span class="toc-text">典型案例</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BA%94%E5%AF%B9%E7%AD%96%E7%95%A5"><span class="toc-number">2.0.0.4.</span> <span class="toc-text">应对策略</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E9%9C%87%E8%8D%A1%E6%B3%A2%E5%8A%A8"><span class="toc-number">2.0.0.5.</span> <span class="toc-text">二、损失函数震荡波动</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%90%AB%E4%B9%89%E8%A7%A3%E6%9E%90-1"><span class="toc-number">2.0.0.6.</span> <span class="toc-text">含义解析</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E5%80%BC%E7%89%B9%E5%BE%81"><span class="toc-number">2.0.0.7.</span> <span class="toc-text">数值特征</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B8%E5%9E%8B%E6%A1%88%E4%BE%8B-1"><span class="toc-number">2.0.0.8.</span> <span class="toc-text">典型案例</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BA%94%E5%AF%B9%E7%AD%96%E7%95%A5-1"><span class="toc-number">2.0.0.9.</span> <span class="toc-text">应对策略</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%8C%81%E7%BB%AD%E4%B8%8A%E5%8D%87"><span class="toc-number">2.0.0.10.</span> <span class="toc-text">三、损失函数持续上升</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%90%AB%E4%B9%89%E8%A7%A3%E6%9E%90-2"><span class="toc-number">2.0.0.11.</span> <span class="toc-text">含义解析</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B8%E5%9E%8B%E6%A1%88%E4%BE%8B-2"><span class="toc-number">2.0.0.12.</span> <span class="toc-text">典型案例</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BA%94%E5%AF%B9%E7%AD%96%E7%95%A5-2"><span class="toc-number">2.0.0.13.</span> <span class="toc-text">应对策略</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E7%BB%BC%E5%90%88%E4%BC%98%E5%8C%96%E5%BB%BA%E8%AE%AE"><span class="toc-number">2.0.0.14.</span> <span class="toc-text">四、综合优化建议</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E8%B0%83%E8%AF%95%E6%A3%80%E6%9F%A5%E6%B8%85%E5%8D%95"><span class="toc-number">2.0.0.15.</span> <span class="toc-text">五、调试检查清单</span></a></li></ol></li></ol><li class="toc-item toc-level-1"><a class="toc-link" href="#transforms-Resize"><span class="toc-number">3.</span> <span class="toc-text">transforms.Resize()</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7%E6%8D%9F%E5%A4%B1%E5%8A%A0%E6%9D%83%E6%B3%95%E2%80%94%E2%80%94%E5%A4%9A%E4%BB%BB%E5%8A%A1%E6%8D%9F%E5%A4%B1%E5%9D%87%E8%A1%A1"><span class="toc-number">4.</span> <span class="toc-text">不确定性损失加权法——多任务损失均衡</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E6%96%B9%E5%BC%8F%E5%8F%98%E5%8C%96"><span class="toc-number">5.</span> <span class="toc-text">初始化方式变化</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%89%8D%E5%90%91%E8%AE%A1%E7%AE%97%E5%8F%98%E5%8C%96"><span class="toc-number">6.</span> <span class="toc-text">前向计算变化</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BD%99%E5%BC%A6%E9%80%80%E7%81%AB%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E5%BA%A6%E5%99%A8"><span class="toc-number">7.</span> <span class="toc-text">余弦退火学习率调度器</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Optuna%E8%87%AA%E5%8A%A8%E5%8C%96%E8%B0%83%E5%8F%82"><span class="toc-number">8.</span> <span class="toc-text">Optuna自动化调参</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%B7%B7%E5%90%88%E7%B2%BE%E5%BA%A6%E8%AE%AD%E7%BB%83"><span class="toc-number">9.</span> <span class="toc-text">混合精度训练</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E7%B4%AF%E7%A7%AF"><span class="toc-number">10.</span> <span class="toc-text">梯度累积</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#torchinfo%E7%BB%9F%E8%AE%A1%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%98%BE%E5%AD%98%E5%8D%A0%E7%94%A8"><span class="toc-number">11.</span> <span class="toc-text">torchinfo统计模型的显存占用</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%98%BE%E5%AD%98%E5%88%86%E6%9E%90%E5%99%A8memory-profiler"><span class="toc-number">12.</span> <span class="toc-text">显存分析器memory_profiler</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Dataloader%E7%9A%84num-workers%E8%AE%BE%E7%BD%AE"><span class="toc-number">13.</span> <span class="toc-text">Dataloader的num_workers设置</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E4%B8%BANan%E7%9A%84%E5%88%86%E6%9E%90"><span class="toc-number">14.</span> <span class="toc-text">损失为Nan的分析</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E8%BE%93%E5%85%A5%E6%95%B0%E6%8D%AE%E9%97%AE%E9%A2%98"><span class="toc-number">14.0.0.1.</span> <span class="toc-text">1. 输入数据问题</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%BF%87%E9%AB%98"><span class="toc-number">14.0.0.2.</span> <span class="toc-text">2. 学习率过高</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%AE%9E%E7%8E%B0%E9%97%AE%E9%A2%98"><span class="toc-number">14.0.0.3.</span> <span class="toc-text">3. 损失函数实现问题</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%EF%BC%88%E5%89%8D%E6%8F%90%E6%98%AF%E4%BD%A0%E7%9A%84%E5%85%B6%E4%BB%96%E4%BB%A3%E7%A0%81%E5%BE%97%E5%86%99%E5%AF%B9%EF%BC%89"><span class="toc-number">14.0.0.4.</span> <span class="toc-text">4. 梯度爆炸（前提是你的其他代码得写对）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84%E9%97%AE%E9%A2%98"><span class="toc-number">14.0.0.5.</span> <span class="toc-text">5. 模型结构问题</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7%E6%8A%80%E5%B7%A7"><span class="toc-number">14.0.0.6.</span> <span class="toc-text">6. 数值稳定性技巧</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#7-%E8%B0%83%E8%AF%95%E6%AD%A5%E9%AA%A4"><span class="toc-number">14.0.0.7.</span> <span class="toc-text">7. 调试步骤</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#8-%E5%85%B6%E4%BB%96%E5%8F%AF%E8%83%BD%E5%8E%9F%E5%9B%A0"><span class="toc-number">14.0.0.8.</span> <span class="toc-text">8. 其他可能原因</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%80%BB%E7%BB%93%E6%B5%81%E7%A8%8B"><span class="toc-number">14.0.0.9.</span> <span class="toc-text">总结流程</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#torchvision-utils-save-image"><span class="toc-number">15.</span> <span class="toc-text">torchvision.utils.save_image</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%A1%AE%E4%BF%9DPython%E4%BC%98%E5%85%88%E5%8A%A0%E8%BD%BD%E6%9C%AC%E5%9C%B0%E9%A1%B9%E7%9B%AE%E7%9A%84%E4%BB%A3%E7%A0%81%E8%80%8C%E4%B8%8D%E6%98%AFAnaconda%E7%8E%AF%E5%A2%83%E4%B8%AD%E7%9A%84%E5%BA%93"><span class="toc-number">16.</span> <span class="toc-text">确保Python优先加载本地项目的代码而不是Anaconda环境中的库</span></a></li></div><div class="related panel pjax" data-title="系列文章"><ul><li><a href="/ai/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/" rel="bookmark" title="注意力机制">注意力机制</a></li><li><a href="/ai/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="bookmark" title="强化学习">强化学习</a></li><li class="active"><a href="/ai/Deep-Learning-Experiment-Tricks/" rel="bookmark" title="Deep Learning Experiment Tricks">Deep Learning Experiment Tricks</a></li><li><a href="/ai/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/" rel="bookmark" title="深度学习理论">深度学习理论</a></li><li><a href="/ai/%E7%8E%B0%E4%BB%A3%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="bookmark" title="现代深度学习">现代深度学习</a></li></ul></div><div class="overview panel" data-title="站点概览"><div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="image" itemprop="image" alt="Runhua Deng" data-src="/images/me.jpg"><p class="name" itemprop="name">Runhua Deng</p><div class="description" itemprop="description">计算机视觉 & 图像恢复</div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">45</span> <span class="name">文章</span></a></div><div class="item categories"><a href="/categories/"><span class="count">7</span> <span class="name">分类</span></a></div></nav><div class="social"><span class="exturl item github" data-url="aHR0cHM6Ly9naXRodWIuY29tL1l1bkhEYW4=" title="https:&#x2F;&#x2F;github.com&#x2F;YunHDan"><i class="ic i-github"></i></span> <span class="exturl item music" data-url="aHR0cHM6Ly9tdXNpYy4xNjMuY29tLyMvdXNlci9ob21lP2lkPTIxMjYzMjE4OTI=" title="https:&#x2F;&#x2F;music.163.com&#x2F;#&#x2F;user&#x2F;home?id&#x3D;2126321892"><i class="ic i-cloud-music"></i></span> <a href="/alex2312666252@gmail.com" title="alex2312666252@gmail.com" class="item email"><i class="ic i-envelope"></i></a> <span class="exturl item csdn" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L20wXzczNTk5NzM4P3NwbT0xMDAwLjIxMTUuMzAwMS41MzQz" title="https:&#x2F;&#x2F;blog.csdn.net&#x2F;m0_73599738?spm&#x3D;1000.2115.3001.5343"><i class="ic i-link"></i></span></div><ul class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>首页</a></li><li class="item dropdown"><a href="/about/" rel="section"><i class="ic i-user"></i>关于</a><ul class="submenu"><li class="item"><a href="/about/me/" rel="section"><i class="ic i-file"></i>简历</a></li><li class="item"><a href="/about/learn_me/" rel="section"><i class="ic i-smile"></i>了解我</a></li></ul></li><li class="item dropdown"><a href="javascript:void(0);"><i class="ic i-feather"></i>文章</a><ul class="submenu"><li class="item"><a href="/archives/" rel="section"><i class="ic i-list-alt"></i>归档</a></li><li class="item"><a href="/categories/" rel="section"><i class="ic i-th"></i>分类</a></li><li class="item"><a href="/tags/" rel="section"><i class="ic i-tags"></i>标签</a></li></ul></li><li class="item"><a href="/friends/" rel="section"><i class="ic i-heart"></i>友達</a></li></ul></div></div></div><ul id="quick"><li class="prev pjax"><a href="/baoyan/Embodied-3D-Labs/" rel="prev" title="上一篇"><i class="ic i-chevron-left"></i></a></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/other/%E5%8D%9A%E5%AE%A2%E6%96%87%E6%A1%A3%E7%BE%8E%E5%8C%96%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E/" rel="next" title="下一篇"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>随机文章</h2><ul><li class="item"><div class="breadcrumb"><a href="/categories/cs/" title="分类于 计算机">计算机</a></div><span><a href="/cs/Pip%E3%80%81Conda%E3%80%81github%E9%95%9C%E5%83%8F/" title="Pip、Conda、Github镜像">Pip、Conda、Github镜像</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/project/" title="分类于 项目与实践">项目与实践</a></div><span><a href="/project/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%B0%8F%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94%E6%9E%84%E5%BB%BAEnv%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%96%B9%E6%B3%95/" title="强化学习小实践——构建Env的基本方法">强化学习小实践——构建Env的基本方法</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/baoyan/" title="分类于 保研">保研</a></div><span><a href="/baoyan/Low-level-Vision-Group/" title="Low-level-Vision-Group">Low-level-Vision-Group</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/cs/" title="分类于 计算机">计算机</a></div><span><a href="/cs/Springboot%E3%80%81Mybatis/" title="Springboot、Mybatis">Springboot、Mybatis</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/other/" title="分类于 琐碎">琐碎</a></div><span><a href="/other/%E6%88%91%E7%9A%84%E7%A7%91%E7%A0%94%E6%9D%82%E8%B0%88/" title="我的科研杂谈">我的科研杂谈</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/project/" title="分类于 项目与实践">项目与实践</a></div><span><a href="/project/GAN%E7%94%9F%E6%88%90%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E5%AE%9E%E8%B7%B5/" title="GAN生成手写数字实践">GAN生成手写数字实践</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/research/" title="分类于 学术">学术</a></div><span><a href="/research/EnlightenGAN%E8%AE%BA%E6%96%87%E8%AF%A6%E8%A7%A3/" title="EnlightenGAN论文详解">EnlightenGAN论文详解</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/cs/" title="分类于 计算机">计算机</a></div><span><a href="/cs/Pytorch/" title="Pytorch">Pytorch</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/other/" title="分类于 琐碎">琐碎</a></div><span><a href="/other/%E7%BE%BD%E6%AF%9B%E7%90%831/" title="羽毛球1">羽毛球1</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/research/" title="分类于 学术">学术</a></div><span><a href="/research/Retinexformer%E8%AE%BA%E6%96%87%E8%AF%A6%E8%A7%A3/" title="Retinexformer论文详解">Retinexformer论文详解</a></span></li></ul></div><div><h2>最新评论</h2><ul class="leancloud-recent-comment"></ul></div></div><div class="status"><div class="copyright">&copy; 2023 – <span itemprop="copyrightYear">2025</span> <span class="with-love"><i class="ic i-sakura rotate"></i> </span><span class="author" itemprop="copyrightHolder">Runhua Deng @ Runfar's Zone</span></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-chart-area"></i> </span><span title="站点总字数">196k 字</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="ic i-coffee"></i> </span><span title="站点阅读时长">2:58</span></div><div class="powered-by">基于 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL={path:"ai/Deep-Learning-Experiment-Tricks/",favicon:{show:"（●´3｀●）やれやれだぜ",hide:"(´Д｀)大変だ！"},search:{placeholder:"文章搜索",empty:"关于 「 ${query} 」，什么也没搜到",stats:"${time} ms 内找到 ${hits} 条结果"},valine:!0,fancybox:!0,copyright:'复制成功，转载请遵守 <i class="ic i-creative-commons"></i>BY-NC-SA 协议。',ignores:[function(e){return e.includes("#")},function(e){return new RegExp(LOCAL.path+"$").test(e)}]}</script><script src="https://cdn.polyfill.io/v2/polyfill.js"></script><script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script><script src="/js/app.js?v=0.2.5"></script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({log:!1,pluginJsPath:"lib/",pluginModelPath:"assets/",pluginRootPath:"live2dw/",tagMode:!1})</script></body></html><!-- rebuild by hrmmi -->