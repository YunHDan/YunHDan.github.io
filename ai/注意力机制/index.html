<!-- build time:Mon Jun 23 2025 12:50:44 GMT+0800 (中国标准时间) --><!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#FFF"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/ico" sizes="32x32" href="/images/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="alternate" type="application/rss+xml" title="枯萎的花将在另一彼岸悄然绽放" href="https://yunhdan.github.io/rss.xml"><link rel="alternate" type="application/atom+xml" title="枯萎的花将在另一彼岸悄然绽放" href="https://yunhdan.github.io/atom.xml"><link rel="alternate" type="application/json" title="枯萎的花将在另一彼岸悄然绽放" href="https://yunhdan.github.io/feed.json"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CInconsolata:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/css/app.css?v=0.2.5"><link rel="canonical" href="https://yunhdan.github.io/ai/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"><title>注意力机制 - 人工智能 | Runfar's Zone = 枯萎的花将在另一彼岸悄然绽放</title><meta name="generator" content="Hexo 6.3.0"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">注意力机制</h1><div class="meta"><span class="item" title="创建时间：2024-02-23 15:06:26"><span class="icon"><i class="ic i-calendar"></i> </span><span class="text">发表于</span> <time itemprop="dateCreated datePublished" datetime="2024-02-23T15:06:26+08:00">2024-02-23</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span class="text">本文字数</span> <span>19k</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span class="text">阅读时长</span> <span>17 分钟</span></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="切换导航栏"><span class="line"></span> <span class="line"></span> <span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">Runfar's Zone</a></li></ul><ul class="right"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div id="imgs" class="pjax"><ul><li class="item" data-background-image="https://s2.loli.net/2024/12/07/PHaOg7vl59DmYEu.png"></li><li class="item" data-background-image="https://s2.loli.net/2025/05/18/L5SitkGUCNoaYBP.jpg"></li><li class="item" data-background-image="https://s2.loli.net/2024/12/04/C5clUonWQVD8sKO.jpg"></li><li class="item" data-background-image="https://s2.loli.net/2024/12/04/fHhmBNQKlqOowjk.jpg"></li><li class="item" data-background-image="https://s2.loli.net/2024/12/04/zTSsqXtYJxQC47Z.jpg"></li><li class="item" data-background-image="https://s2.loli.net/2024/12/07/mONZDMUz524GbtI.jpg"></li></ul></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"/></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"/><use xlink:href="#gentle-wave" x="48" y="3"/><use xlink:href="#gentle-wave" x="48" y="5"/><use xlink:href="#gentle-wave" x="48" y="7"/></g></svg></div><main><div class="inner"><div id="main" class="pjax"><div class="article wrap"><div class="breadcrumb" itemscope itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i> <span><a href="/">首页</a></span><i class="ic i-angle-right"></i> <span class="current" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/ai/" itemprop="item" rel="index" title="分类于 人工智能"><span itemprop="name">人工智能</span></a><meta itemprop="position" content="1"></span></div><article itemscope itemtype="http://schema.org/Article" class="post block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://yunhdan.github.io/ai/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/me.jpg"><meta itemprop="name" content="Runhua Deng"><meta itemprop="description" content=", 计算机视觉 & 图像恢复"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="枯萎的花将在另一彼岸悄然绽放"></span><div class="body md" itemprop="articleBody"><blockquote><p>所学的自注意力机制的知识由李沐老师教授，感恩沐神！</p></blockquote><h1 id="注意力提示双组件与查询、键、值"><a href="#注意力提示双组件与查询、键、值" class="headerlink" title="注意力提示双组件与查询、键、值"></a>注意力提示双组件与查询、键、值</h1><p>​ 注意力提示双组件分为非自主性提示与自主性提示。<strong>非自主性提示</strong>可以理解为<strong>环境</strong>中物体的突出性带来的<strong>提示</strong>。比如，一杯红咖啡摆在一堆报纸中间，咖啡是红色的，这种突出性的提示就是环境带来的非自主性提示。与之相反的是自主性提示，当我们喝完咖啡，会兴奋，兴奋起来开始阅读报纸。也就是说人们受主观意愿推动去集中于看报纸，这就是自主性提示，这个自主性提示来自人们自己。</p><p><img data-src="../../assets/屏幕截图-2023-11-24-230449.png" alt="生物学双组件"></p><p><img data-src="../../assets/屏幕截图-2023-11-24-230637.png" alt="iamge"></p><p>​ 在非自主性提示中，分为两种，一种是感官的输入，称为<strong>值</strong>，通常是一些<strong>感官输入</strong>，比如咖啡的颜色，书本的字体颜色。另一种是非意志的线索，称为<strong>键</strong>，比如书本就是一个键，咖啡就是一个键，都是<strong>客观存在的非意志的线索</strong>。</p><p><strong>自主性提示</strong>，通常被称为<strong>查询</strong>，它们是一些<strong>意志线索</strong>，是人的主观意愿。</p><p>​ 注意力机制做的是什么事情？一般来说，环境中的非意志线索——键会与感官输入——值，一一对应。这时，主观的意志因素——查询通过注意力汇聚（也称注意力池化）选择其中一个值，然后输出。这样就做到了所谓的集中注意力。</p><p><img data-src="../../assets/屏幕截图-2023-11-24-231509.png" alt="image"></p><h1 id="非参数注意力汇聚：-Nadaraya-Watson-核回归"><a href="#非参数注意力汇聚：-Nadaraya-Watson-核回归" class="headerlink" title="非参数注意力汇聚：$Nadaraya-Watson$ 核回归"></a>非参数注意力汇聚：$Nadaraya-Watson$ 核回归</h1><p>​ 在注意力机制的早期，或者说是注意力机制未出现时，人们通过用平均汇聚（平均池化）处理问题，效果比较差。后来，$Nadaraya$和$Watson$提出了<strong>Nadaraya-Watson 核回归</strong>。公式如下：</p><script type="math/tex;mode=display">f(x) = \sum^{n}_{i=1}{\frac{K(x-x_i)}{\sum^{n}_{j=1}K(x-x_j)}y_i},</script><p>​ 给定查询x，通过这个f函数，可以对每个键值对附上权重，权重最大的值，即为注意力集中处。其中，xi是每个键，$y_i$是每个值，K是核，本质上是一个函数，后面我们再举例核的选取。这里我们再注意一下分母，分母的$x_j$也是每个键，只是为了保证公式内符号不冲突，两次遍历一个用i一个用j，实际代表的意义是一样的。</p><p>当K是高斯核时，即K公式为：</p><script type="math/tex;mode=display">K(u) = \frac{1}{\sqrt{2\pi}}exp(-\frac{u^2}{2}).</script><p>代入核回归公式，有：</p><script type="math/tex;mode=display">f(x) = \sum^{n}_{i=1}{\alpha(x,x_i)}y_i    \\
= \sum^{n}_{i=1}{\frac{exp(-\frac{1}{2}(x-x_i)^2)}{\sum^{n}_{j=1}{exp(-\frac{1}{2}(x-x_j)^2)}}y_i}    \\
= \sum^{n}_{i=1}{softmax(-\frac{1}{2}(x-x_i)^2)y_i}.</script><p>这样，当选取的是高斯核，其实注意力机制与$softmax$有关了。</p><p>​ 在$Nadaraya-Watson$ 核回归下，如果一个键xi越是接近给定的查询x，那么分配给这个键对应值$y_i$的注意力权重就越大，也就获得了更多的注意力。</p><p>这是早期的注意力机制，使用的$Nadaraya-Watson$ 还只是一个<strong>非参数的注意力汇聚方法</strong>。</p><h1 id="带参数的注意力汇聚"><a href="#带参数的注意力汇聚" class="headerlink" title="带参数的注意力汇聚"></a>带参数的注意力汇聚</h1><p>​ 前面提到，$Nadaraya-Watson$ 实际上是一个非参数的注意力汇聚方法，我们当然可以自己加上一个可学习的参数到该注意力汇聚中，得到一个带参数的注意力汇聚。</p><p>具体操作，只需要在下面的查询x和键xi之间的距离<strong>乘以可学习参数w</strong>即可：</p><script type="math/tex;mode=display">f(x) = \sum^{n}_{i=1}{\alpha(x,x_i)}y_i    \\
= \sum^{n}_{i=1}{\frac{exp(-\frac{1}{2}((x-x_i)w)^2)}{\sum^{n}_{j=1}{exp(-\frac{1}{2}((x-x_j)w)^2)}}y_i}    \\
= \sum^{n}_{i=1}{softmax(-\frac{1}{2}((x-x_i)w)^2)y_i}.</script><p>通过训练，即可学习到较合适的w，得到较合适的注意力汇聚函数。</p><h1 id="注意力评分函数"><a href="#注意力评分函数" class="headerlink" title="注意力评分函数"></a>注意力评分函数</h1><p>注意力汇聚函数f，可以被写成下面的加权和：</p><script type="math/tex;mode=display">f(q, (k_1, v_1), ..., (k_m, v_m)) = \sum^{m}_{i=1}{\alpha(q,k_i)v_i} \in \mathbb{R}^v</script><p>​ 其中，q是查询<code>query</code>，k是键<code>key</code>，v是值<code>value</code>，m是键值对的个数，α是一个用q和k，通过特定的函数计算出来的注意力权重，公式如下：</p><script type="math/tex;mode=display">\alpha(q,k_i) = softmax(a(q, k_i)) = \frac{exp(a(q, k_i))}{\sum^{m}_{j=1}{exp(a(q,k_j))}} \in \mathbb{R}</script><p>​ 其中a就是我们的<strong>注意力评分函数</strong>。注意力评分函数a，通过查询q和键k，输出一个结果，然后将结果送入到$softmax$中，计算出注意力权重，本质上是键对应的值的概率分布，概率小，选择该键值对的机会就小，对应得到更少的注意力。</p><p>​ 特别地，高斯核函数的注意力评分函数是指数<code>exp</code>里的部分，即<code>(-1 / 2)(x - xi)²</code>。<strong>注意力评分函数不是固定的</strong>，可以自己选择注意力评分函数，然后把这个函数的输出结果输入到$softmax$中计算出注意力权重。</p><p>宏观来看，注意力机制可以用下图描述，它实现了上面的公式。</p><p><img data-src="../../assets/屏幕截图-2023-11-27-213911.png" alt="image"></p><h1 id="加性注意力"><a href="#加性注意力" class="headerlink" title="加性注意力"></a>加性注意力</h1><p>一般来说，当<strong>查询和键是不同长度的矢量</strong>时，可以使用加性注意力作为评分函数。</p><p>比如，<strong>q</strong>是长为q的向量$\large{\mathbb{R}^q}$，<strong>k</strong>是长度为k的向量$\large{\mathbb{R}^k}$，k与q不相同。这时，加性注意力的评分函数为：</p><script type="math/tex;mode=display">a(q,k) = w_v^{T}tanh(W_qq + W_kk) \in \mathbb{R},</script><p>​ 其中，可学习的参数是$\large{W_q}$、$\large{W_k}$和$\large{W_v}$，$\large{W_q}$是h×q的矩阵$\large{R^{h×q}}$，$\large{W_k}$是h×k的矩阵$\large{R^{h×k}}$，$\large{W_v}$是h×1的向量$\large{R^h}$。这个公式保证了最后得到的注意力权重是一个<strong>实数</strong>。</p><p>​ 观察式子，不难发现，其实注意力评分函数是通过将query和key链接起来然后传入<strong>感知机</strong>中实现的，使用$\tanh$作为激活函数，而且无偏置项。h是隐藏单元数，是一个超参数。</p><p>看<strong>代码实现</strong>会促进理解：</p><pre><code class="lang-python">class AdditiveAttention(nn.Module):
    &quot;&quot;&quot;加性注意力&quot;&quot;&quot;
    def __init__(self, key_size, query_size, num_hiddens, dropout, **kwargs):
        super(AdditiveAttention, self).__init__(**kwargs)
        self.W_k = nn.Linear(key_size, num_hiddens, bias=False)
        self.W_q = nn.Linear(query_size, num_hiddens, bias=False)
        self.w_v = nn.Linear(num_hiddens, 1, bias=False)
        self.dropout = nn.Dropout(dropout)

    def forward(self, queries, keys, values, valid_lens):
        queries, keys = self.W_q(queries), self.W_k(keys)
        # 在维度扩展后，
        # queries的形状：(batch_size，查询的个数，1，num_hidden)
        # key的形状：(batch_size，1，“键－值”对的个数，num_hiddens)
        # 使用广播方式进行求和
        features = queries.unsqueeze(2) + keys.unsqueeze(1)
        features = torch.tanh(features)
        # self.w_v仅有一个输出，因此从形状中移除最后那个维度。
        # scores的形状：(batch_size，查询的个数，“键-值”对的个数)
        scores = self.w_v(features).squeeze(-1)
        self.attention_weights = masked_softmax(scores, valid_lens)
        # values的形状：(batch_size，“键－值”对的个数，值的维度)
        return torch.bmm(self.dropout(self.attention_weights), values)
</code></pre><h1 id="Masked-softmax-：掩蔽-softmax-操作"><a href="#Masked-softmax-：掩蔽-softmax-操作" class="headerlink" title="Masked-$softmax$：掩蔽$softmax$操作"></a>Masked-$softmax$：掩蔽$softmax$操作</h1><p>前面的代码中，可以注意到有个叫做masked-$softmax$操作，这里简单记录一下。</p><p>​ $softmax$操作是用于<strong>输出一个概率分布</strong>，在注意力机制中，可以作为注意力的权重。在数据中，某些文本序列有时被填充了没有意义的特殊词元，<strong>为了只将有意义的词元注入到注意力汇聚中获得注意力权重</strong>，可以指定一个有效词元长度，使得计算$softmax$时，超出有效长度的部分被过滤掉，这就是Masked-$softmax$。</p><p><strong>超出有效长度的部分归为0</strong>，达到被掩蔽的作用。</p><h1 id="缩放点积注意力"><a href="#缩放点积注意力" class="headerlink" title="缩放点积注意力"></a>缩放点积注意力</h1><p>当<strong>查询和键长度相同</strong>，可以用缩放点积注意力评分函数。</p><p>查询$\mathbf Q\in\mathbb R^{n\times d}$、键$\mathbf K\in\mathbb R^{m\times d}$和值$\mathbf V\in\mathbb R^{m\times v}$的<strong>缩放点积注意力</strong>是：</p><script type="math/tex;mode=display">softmax(\frac{QK^T}{\sqrt{d}})V \in \mathbb{R}^{n\times v}.</script><p>​ $softmax$里的内容就是<strong>缩放点积注意力评分函数</strong>。其中，<strong>d</strong>是查询和键的长度，<strong>n</strong>是小批量，<strong>m</strong>是键值对数，<strong>v</strong>是值的长度。</p><p><strong>代码实现</strong>如下，促进理解：</p><pre><code class="lang-python">class DotProductAttention(nn.Module):
    &quot;&quot;&quot;缩放点积注意力&quot;&quot;&quot;
    def __init__(self, dropout, **kwargs):
        super(DotProductAttention, self).__init__(**kwargs)
        self.dropout = nn.Dropout(dropout)

    # queries的形状：(batch_size，查询的个数，d)
    # keys的形状：(batch_size，“键－值”对的个数，d)
    # values的形状：(batch_size，“键－值”对的个数，值的维度)
    # valid_lens的形状:(batch_size，)或者(batch_size，查询的个数)
    def forward(self, queries, keys, values, valid_lens=None):
        d = queries.shape[-1]
        # 设置transpose_b=True为了交换keys的最后两个维度
        scores = torch.bmm(queries, keys.transpose(1,2)) / math.sqrt(d)
        self.attention_weights = masked_softmax(scores, valid_lens)
        return torch.bmm(self.dropout(self.attention_weights), values)
</code></pre><h1 id="自注意力"><a href="#自注意力" class="headerlink" title="自注意力"></a>自注意力</h1><p>​ 给定由词元组成的序列$x_1$，$x_2$，……，$x_n$，其中任意$x_i \in \mathbb{R}^d$(1 $\leq$ i $\leq$ n)，n是序列长度，d是词元特征。自注意力输出为一个长度相同的序列$y_1$，$y_2$，……，$y_n$，其中：</p><script type="math/tex;mode=display">\huge{\mathbf{y_i} = f(\mathbf{x_i}, (\mathbf{x_1}, \mathbf{x_1}), ...,(\mathbf{x_n}, \mathbf{x_n})) \in \mathbb{R}^d}</script><p>每一个$y_i$都是对应词元的注意力权重。</p><p>​ 也就是说，<strong>自注意力其实就是把查询变成每个键自己</strong>，通过某种运算来直接计算得到句子在编码过程中每个位置上的注意力权重；然后再以权重和的形式来计算得到整个句子的隐含向量表示。</p><p><img data-src="../../assets/屏幕截图-2023-12-08-195010.png" alt="image-20231128133343482"></p><h1 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h1><h2 id="绝对位置信息"><a href="#绝对位置信息" class="headerlink" title="绝对位置信息"></a>绝对位置信息</h2><p>​ 在处理词元序列时，循环神经网络是逐个的重复地处理词元的， 而自注意力则因为并行计算而放弃了顺序操作。 为了使用序列的顺序信息，通过在输入表示中添加<strong>位置编码</strong>，来注入绝对的或相对的位置信息。</p><p>​ 位置编码信息可以通过学习得到，也可以通过直接固定得到。这里介绍Transformer使用的<strong>基于正弦函数和余弦函数的固定位置编码方法</strong>。</p><p>​ 假设输入$\bf{X} \in \mathbb{R}^{n×d}$是batch中一个序列中，n个词元的d维嵌入表示。位置编码使用<strong>相同形状</strong>的位置嵌入矩阵$\bf{P} \in \mathbb{R}^{n×d}$，输出$\bf{X + P}$。位置嵌入矩阵第<strong>i</strong>行的第$\bf{2j}$列和第$\bf{2j + 1}$列的元素由如下公式计算：</p><script type="math/tex;mode=display">p_{i, 2j} = sin(\frac{i}{10000^{2j/d}}),</script><script type="math/tex;mode=display">p_{i, 2j+1} = cos(\frac{i}{10000^{2j/d}}).</script><p>​ 为了让基于正弦函数和余弦函数地固定位置编码方法更加直观，下面的图片展示了相应的效果，每一行$row$是词元所在的位置，四条不同颜色的曲线代表不同维度下的正弦函数。可以看到，选定一个$row$，不同$col$对应的三角函数值是不一样的，也就是说<strong>同一个词元，不同维度特征得到的位置编码不同</strong>，<strong>不同词元之间的位置编码也不尽相同</strong>。</p><p><img data-src="../../assets/屏幕截图-2023-11-28-151246.png" alt="image"></p><h2 id="相对位置信息"><a href="#相对位置信息" class="headerlink" title="相对位置信息"></a>相对位置信息</h2><p>​ 上述所作的编码，是绝对位置编码。如果已知任何确定的位置偏移$\delta$，位置$i + \delta$处的位置编码可以<strong>线性投影</strong>到位置$i$处的位置编码。</p><p>令$\omega = 1 / 10000^{2j/d}$，那么任何一对$(p_{i, 2j}, p_{i, 2j+1})$都可以投影到$(p_{i+\delta, 2j}, p_{i+\delta, 2j+1})$，具体公式可见：</p><script type="math/tex;mode=display">\left[
\begin{matrix}
cos(\delta w_j) & sin(\delta w_j)    \\
-sin(\delta w_j) & cos(\delta w_j)
\end{matrix}
\right]
\left[
\begin{matrix}
p_{i, 2j} \\
p_{i, 2j+1} 
\end{matrix}
\right]    \\
= \left[
\begin{matrix}
cos(\delta w_j)sin(i w_j) + sin(\delta w_j)cos(i w_j) \\
-sin(\delta w_j)sin(i w_j) + cos(\delta w_j)cos(i w_j)
\end{matrix}
\right]    \\
= \left[
\begin{matrix}
sin((i + \delta) w_j) \\
cos((i + \delta) w_j) 
\end{matrix}
\right]    \\
= \left[
\begin{matrix}
p_{i + \delta, 2j} \\
p_{i + \delta, 2j+1}
\end{matrix}
\right],</script><p>很明显，这是通过上面的2×2投影矩阵做到的，这个矩阵不依赖于任何位置的索引i。</p><h1 id="多头注意力"><a href="#多头注意力" class="headerlink" title="多头注意力"></a>多头注意力</h1><p>​ 自注意力机制的<strong>缺陷</strong>就是：模型在对当前位置的信息进行编码时，会过度的将注意力集中于自身的位置，因此$Transfomer$作者提出了通过<strong>多头注意力机制</strong>来解决这一问题。</p><blockquote><p>在实践中，当给定<strong>相同的查询、键和值的集合</strong>时， 我们希望模型可以基于<strong>相同的注意力机制</strong>学习到<strong>不同的行为</strong>， 然后将不同的行为作为知识组合起来， 捕获序列内各种范围的依赖关系 （例如，短距离依赖和长距离依赖关系）。 因此，允许注意力机制组合使用查询、键和值的不同 <em>子空间表示</em>（$representation$ $subspaces$）可能是有益的。</p><p>为此，与其只使用单独一个注意力汇聚， 我们可以用独立学习得到的ℎ组不同的 <em>线性投影</em>（$linear$ $projections$）来变换查询、键和值。 然后，这ℎ组变换后的查询、键和值将并行地送到注意力汇聚中。 最后，将这ℎ个注意力汇聚的输出拼接在一起， 并且通过另一个可以学习的线性投影进行变换， 以产生最终输出。 这种设计被称为<em>多头注意力</em>（$multihead$ $attention$） 对于ℎ个注意力汇聚输出，每一个注意力汇聚都被称作一个<em>头</em>（$head$）。 展示了使用全连接层来实现可学习的线性变换的多头注意力。</p></blockquote><p><img data-src="../../assets/屏幕截图-2023-11-28-160635.png" alt="iamge"></p><p>给定查询$\large{\bf{q} \in \mathbb{R}^{d_q}}$、键$\large{\bf{k} \in \mathbb{R}^{d_k}}$和值$\large{\bf{v} \in \mathbb{R}^{d_v}}$，每个注意力头$\bf{h_i}$(i = 1, 2, …, h)的计算方法为：</p><script type="math/tex;mode=display">\large{h_i = f(W_i^{\small{(q)}}q, W_i^{\small{(k)}}k,W_i^{\small{(v)}}v) \in \mathbb{R}^{p_v}},</script><p>​ 其中，可学习的参数为$\large{W_i^{\small{(q)}} \in \mathbb{R}^{p_q×d_q}}$、$\large{W_i^{\small{(k)}} \in \mathbb{R}^{p_k×d_k}}$和$\large{W_i^{\small{(v)}} \in \mathbb{R}^{p_v×d_v}}$以及注意力汇聚函数f。f可以是缩放点积注意力，也可以是加性注意力。</p><p>多头注意力的输出是多个头经过连结，再经过一个线性转换（全连接层）的结果，输出为：</p><script type="math/tex;mode=display">W_o \left[\begin{matrix} h_1 \\ \vdots \\ h_h \end{matrix} \right] \in \mathbb{R}^{p_o}.</script><p>其中，$\large{W_o \in \mathbb{R}^{p_o×h×p_v}}$是可学习参数。</p><p>​ 在实现的过程中，通常用缩放点积注意力作为每一个注意力头。令$p_q = p_k = p_v = p_o / h$，这时候就可以实现$h$个头的并行计算。我们来看代码如何实现，以促进理解。在实现中，$p_o$就是参数$num_hiddens$。</p><pre><code class="lang-python">class MultiHeadAttention(nn.Module):
    &quot;&quot;&quot;多头注意力&quot;&quot;&quot;

    def __init__(self, key_size, query_size, value_size, num_hiddens,
                 num_heads, dropout, bias=False, **kwargs):
        #   num_hiddens是词元特征数
        super(MultiHeadAttention, self).__init__(**kwargs)
        self.num_heads = num_heads
        self.attention = d2l.DotProductAttention(dropout)
        self.W_q = nn.Linear(query_size, num_hiddens, bias=bias)
        #   把查询的特征数转换为词元特征数
        self.W_k = nn.Linear(key_size, num_hiddens, bias=bias)
        #   把键的特征数转换为词元特征数
        self.W_v = nn.Linear(value_size, num_hiddens, bias=bias)
        #   把值的特征数转换为词元特征数
        self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias)
        #   把多个头的特征数转换为输出特征数

    def forward(self, queries, keys, values, valid_lens):
        # queries，keys，values的形状:
        # (batch_size，查询或者“键－值”对的个数，num_hiddens)
        # valid_lens　的形状:
        # (batch_size，)或(batch_size，查询的个数)
        # 经过变换后，输出的queries，keys，values　的形状:
        # (batch_size*num_heads，查询或者“键－值”对的个数，
        # num_hiddens/num_heads)
        queries = transpose_qkv(self.W_q(queries), self.num_heads)
        keys = transpose_qkv(self.W_k(keys), self.num_heads)
        values = transpose_qkv(self.W_v(values), self.num_heads)

        if valid_lens is not None:
            # 在轴0，将第一项（标量或者矢量）复制num_heads次，
            # 然后如此复制第二项，然后诸如此类。
            valid_lens = torch.repeat_interleave(
                valid_lens, repeats=self.num_heads, dim=0)

        # output的形状:(batch_size*num_heads，查询的个数，
        # num_hiddens/num_heads)
        output = self.attention(queries, keys, values, valid_lens)

        # output_concat的形状:(batch_size，查询的个数，num_hiddens)
        output_concat = transpose_output(output, self.num_heads)
        return self.W_o(output_concat)
</code></pre><p>​ 多个头的并行计算是通过下面这两个转置函数实现的。<code>transpose_output</code>函数反转了<code>transpose_qkv</code>函数的操作。</p><pre><code class="lang-python">def transpose_qkv(X, num_heads):
    &quot;&quot;&quot;为了多注意力头的并行计算而变换形状&quot;&quot;&quot;
    # 输入X的形状:(batch_size，查询或者“键－值”对的个数，num_hiddens)
    # 输出X的形状:(batch_size，查询或者“键－值”对的个数，num_heads，
    # num_hiddens/num_heads)
    X = X.reshape(X.shape[0], X.shape[1], num_heads, -1)

    # 输出X的形状:(batch_size，num_heads，查询或者“键－值”对的个数,
    # num_hiddens/num_heads)
    X = X.permute(0, 2, 1, 3)

    # 最终输出的形状:(batch_size*num_heads,查询或者“键－值”对的个数,
    # num_hiddens/num_heads)
    return X.reshape(-1, X.shape[2], X.shape[3])


def transpose_output(X, num_heads):
    &quot;&quot;&quot;逆转transpose_qkv函数的操作&quot;&quot;&quot;
    X = X.reshape(-1, num_heads, X.shape[1], X.shape[2])
    X = X.permute(0, 2, 1, 3)
    return X.reshape(X.shape[0], X.shape[1], -1)
</code></pre><p>​ 很容易看到，<strong>经过多头注意力后的输出形状并未改变</strong>，与查询、键和值的形状相同，均为(batch_size，查询或者“键－值”对的个数，$num_hiddens$)。</p><h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><p>​ <strong>Transformer完全基于注意力机制</strong>，没有任何卷积层或循环神经网络层。</p><p>​ 尽管Transformer最初是应用于在文本数据上的序列到序列学习，但现在已经推广到各种现代的深度学习中，例如语言、视觉、语音和强化学习领域。</p><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>整体架构如下图：<br><img data-src="../../assets/屏幕截图-2023-11-28-175302.png" alt="image"></p><p>​ <strong>Transformer是由基于自注意力模块叠加而成的编码器和解码器组成的</strong>。源序列和目标序列的嵌入将加上位置编码，再<strong>分别</strong>输入打编码器和解码器中。</p><p>​ 从宏观角度来看，Transformer的<strong>编码器</strong>由多个相同的块叠加而成，共有n层。每个块都有两个子层（后续用$sublayer$表示子层）。第一个子层是多头自注意力汇聚。第二个子层是基于位置的前馈网络。</p><p>​ 更具体来说，编码器的自注意力的查询、键和值都是来自<strong>前一个编码器块</strong>的输出。每个子层都采用了残差连接和层规范化。</p><p>​ 输入序列的每一个词元$\Large{\bf{x} \in \mathbb{R}^d}$经过编码器的一层后的输出也是一个$\large{\bf{d}}$维的向量。</p><p>​ Transformer的<strong>解码器</strong>也是由多个相同的块叠加而成的，并且块中使用了残差连接和层规范化。然而，除了编码器中描述的两个子层之外，解码器还在这两个子层之间插入第三个子层，称为<strong>编码器-解码器注意力层</strong>。在这个层中，查询来自前一个解码器层的输出，而键和值来自<strong>整个</strong>编码器的输出。</p><p>​ 解码器解码的每个词元都只能考虑该词元之前的所有词元，而不能考虑此词元往后的词元，这称为<strong>掩蔽注意力</strong>，这可以确保预测仅依赖于已生成的输出词元。</p><h2 id="基于位置的前馈网络"><a href="#基于位置的前馈网络" class="headerlink" title="基于位置的前馈网络"></a>基于位置的前馈网络</h2><p>​ 本质上，基于位置的前馈网络是在对<strong>一个序列</strong>的<strong>所有词元</strong>使用<strong>同一个多层感知机</strong>进行变换。</p><p>​ 在下面的实现中，输入<code>X</code>的形状（批量大小，时间步数或序列长度，<code>ffn_num_input</code>）将被一个两层的感知机转换成形状为（批量大小，时间步数或序列长度，<code>ffn_num_outputs</code>）的输出张量。</p><pre><code class="lang-python">class PositionWiseFFN(nn.Module):
    #   &quot;&quot;&quot;基于位置的前馈网络&quot;&quot;&quot;
    def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs,
                 **kwargs):
        super(PositionWiseFFN, self).__init__(**kwargs)
        self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)
        self.relu = nn.ReLU()
        self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs)

    def forward(self, X):
        return self.dense2(self.relu(self.dense1(X)))
</code></pre><p>​ 这里插述一下关于<code>nn.Linear</code>的操作。</p><p>​ 众所周知，<code>nn.Linear</code>就是一个<strong>单层感知机</strong>，参数是(<code>input, output</code>)，通常这个参数就是<code>Linear</code>的<strong>权重矩阵</strong>。</p><p>​ 对于一个二维的输入，<code>input</code>是输入特征的维度，也就是矩阵的最后一维。对一张图片来说（形状为h*w*c），特征维是通道c。<code>output</code>是该层感知机的隐藏单元数。<strong>单层感知机的操作就是把二维输入最后一维的维数转换为隐藏单元数，而其他的维度都视为样本数</strong>。比如输入矩阵形状为(x，y)，单层感知机的隐藏单元数为z，也就是说<code>Linear</code>的参数设置为(<code>y</code>，z)。于是，经过<code>Linear</code>的处理，结果为(x，z)。</p><p>​ 对于一个三维或者三维以上的输入，同理，它<strong>只将最后一维视为特征维进行变换，而其他维统一视作样本数</strong>。比如，一个形状为(<code>batch_size</code>，<code>valid_lens</code>，<code>size</code>)的输入矩阵，<code>Linear</code>首先会把它看成(<code>batch_size*valid_lens</code>，<code>size</code>)的二维矩阵，然后再进行变换，变换为(<code>batch_size*valid_lens</code>，<code>num_hiddens</code>)。</p><p>​ 可以观察到，<strong><code>Linear</code>不会变化样本维，只会变化特征维</strong>。俗称，<strong>改变张量的最里层维度的尺寸</strong>。</p><h2 id="残差连接和层规范化"><a href="#残差连接和层规范化" class="headerlink" title="残差连接和层规范化"></a>残差连接和层规范化</h2><p>​ 残差连接被认为是<strong>深度网络</strong>的必备技术，提出者是何凯明，其论文出处：</p><p>​ <span class="exturl" data-url="aHR0cHM6Ly9vcGVuYWNjZXNzLnRoZWN2Zi5jb20vY29udGVudF9jdnByXzIwMTYvcGFwZXJzL0hlX0RlZXBfUmVzaWR1YWxfTGVhcm5pbmdfQ1ZQUl8yMDE2X3BhcGVyLnBkZg==">Deep Residual Learning for Image Recognition (thecvf.com)</span></p><p>​ 层规范化同批量规范化一样，都是<strong>正则化</strong>的重要手段，但是在自然语言处理中，层规范化的效果要优于批量规范化，层规范化和批量规范化的区别在这篇博客中解释地非常到位，在此不再赘述：</p><p>​ <span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0xpdHRsZV9XaGl0ZV85L2FydGljbGUvZGV0YWlscy8xMjMzNDUwNjI=">BatchNorm和LayerNorm——通俗易懂的理解-CSDN博客</span></p><p>代码实现如下：</p><pre><code class="lang-python">class AddNorm(nn.Module):
    &quot;&quot;&quot;残差连接后进行层规范化&quot;&quot;&quot;

    def __init__(self, normalized_shape, dropout, **kwargs):
        super(AddNorm, self).__init__(**kwargs)
        self.dropout = nn.Dropout(dropout)
        #   随机丢弃数据的一些特征，但总体形状不变
        self.ln = nn.LayerNorm(normalized_shape)
        #   LayerNorm的输入参数是[句子长度，每个单词的特征维度数]，很明显LayerNorm不改变输入样本的特征

    def forward(self, X, Y):
        return self.ln(self.dropout(Y) + X)
</code></pre><h2 id="编码器"><a href="#编码器" class="headerlink" title="编码器"></a>编码器</h2><p>有了前面的铺垫，下面可以来<strong>实现编码器中的一个块</strong>：</p><pre><code class="lang-python">class EncoderBlock(nn.Module):
    #   &quot;&quot;&quot;Transformer编码器块&quot;&quot;&quot;
    #   这只是编码器的一个块
    # Transformer编码器中的任何块都不会改变其输入的形状

    def __init__(self, key_size, query_size, value_size, num_hiddens,
                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,
                 dropout, use_bias=False, **kwargs):
        super(EncoderBlock, self).__init__(**kwargs)
        self.attention = d2l.MultiHeadAttention(
            key_size, query_size, value_size, num_hiddens, num_heads, dropout,
            use_bias)

        self.addnorm1 = AddNorm(norm_shape, dropout)
        self.ffn = PositionWiseFFN(
            ffn_num_input, ffn_num_hiddens, num_hiddens)
        self.addnorm2 = AddNorm(norm_shape, dropout)

    def forward(self, X, valid_lens):
        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))
        return self.addnorm2(Y, self.ffn(Y))
</code></pre><p>测试一下：</p><pre><code class="lang-python">X = torch.ones((2, 100, 24))
valid_lens = torch.tensor([3, 2])
encoder_blk = EncoderBlock(24, 24, 24, 24, [100, 24], 24, 48, 8, 0.5)
encoder_blk.eval()
print(encoder_blk(X, valid_lens).shape)
</code></pre><p>结果：</p><pre><code class="lang-python">torch.Size([2, 100, 24])
</code></pre><p>然后，可以根据这个编码器的基本块，<strong>堆叠n个</strong>，实现最后的<strong>编码器</strong>：</p><pre><code class="lang-python">class TransformerEncoder(d2l.Encoder):
    # &quot;&quot;&quot;Transformer编码器&quot;&quot;&quot;

    def __init__(self, vocab_size, key_size, query_size, value_size,
                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,
                 num_heads, num_layers, dropout, use_bias=False, **kwargs):
        super(TransformerEncoder, self).__init__(**kwargs)
        self.num_hiddens = num_hiddens
        self.embedding = nn.Embedding(vocab_size, num_hiddens)
        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)
        self.blks = nn.Sequential()
        for i in range(num_layers):
            self.blks.add_module(&quot;block&quot; + str(i),
                                 EncoderBlock(key_size, query_size, value_size, num_hiddens,
                                              norm_shape, ffn_num_input, ffn_num_hiddens,
                                              num_heads, dropout, use_bias))

    def forward(self, X, valid_lens, *args):
        # 因为位置编码值在-1和1之间，
        # 因此嵌入值乘以嵌入维度的平方根进行缩放，
        # 然后再与位置编码相加。
        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))
        for i, blk in enumerate(self.blks):
            X = blk(X, valid_lens)
            self.attention_weights[i] = blk.attention.attention.attention_weights
        return X
</code></pre><p>​ 测试一下：</p><pre><code class="lang-python">encoder = TransformerEncoder(
    200, 24, 24, 24, 24, [100, 24], 24, 48, 8, 2, 0.5)
encoder.eval()
valid_lens = torch.tensor([3, 2])
print(encoder(torch.ones((2, 100), dtype=torch.long), valid_lens).shape)
</code></pre><p>​ 结果：</p><pre><code class="lang-python">torch.Size([2, 100, 24])
</code></pre><h2 id="解码器"><a href="#解码器" class="headerlink" title="解码器"></a>解码器</h2><p>​ <code>Transformer</code>解码器也是由多个相同的块组成。每个块包含<strong>三个子层</strong>：解码器自注意力、“编码器-解码器”注意力和基于位置的前馈网络。这些子层也都被残差连接和层规范化围绕。</p><p>​ 前面说到了<strong>掩蔽多头解码器的自注意力层</strong>（第一个子层）中，查询、键和值都来自上一个解码器的输出。我们知道，在序列到序列模型中，训练阶段的输出序列所有位置（时间步）的词元都是已知的；然而，在预测阶段，其输出序列的词元是逐个生成的。因此，为了在解码器中保留这个自回归属性，其<strong>掩蔽多头解码器的自注意力层</strong>设置了参数<code>dec_valid_lens</code>，以便任何查询，都只会与解码器中所有已经生成词元的位置（即直到该查询位置为止）进行注意力计算。</p><p>​ 下面搭建一个解码器块：</p><pre><code class="lang-python">class DecoderBlock(nn.Module):
    # &quot;&quot;&quot;解码器中第i个块&quot;&quot;&quot;

    def __init__(self, key_size, query_size, value_size, num_hiddens,
                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,
                 dropout, i, **kwargs):
        super(DecoderBlock, self).__init__(**kwargs)
        self.i = i
        self.attention1 = d2l.MultiHeadAttention(
            key_size, query_size, value_size, num_hiddens, num_heads, dropout)
        self.addnorm1 = AddNorm(norm_shape, dropout)
        self.attention2 = d2l.MultiHeadAttention(
            key_size, query_size, value_size, num_hiddens, num_heads, dropout)
        self.addnorm2 = AddNorm(norm_shape, dropout)
        self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens,
                                   num_hiddens)
        self.addnorm3 = AddNorm(norm_shape, dropout)

    def forward(self, X, state):
        enc_outputs, enc_valid_lens = state[0], state[1]
        # 训练阶段，输出序列的所有词元都在同一时间处理，
        # 因此state[2][self.i]初始化为None。
        # 预测阶段，输出序列是通过词元一个接着一个解码的，
        # 因此state[2][self.i]包含着直到当前时间步第i个块解码的输出表示，也就是上一个解码器块的输出
        # 第i个解码器块位于第i个时间步
        if state[2][self.i] is None:
            key_values = X
        else:
            key_values = torch.cat((state[2][self.i], X), axis=1)
        state[2][self.i] = key_values
        if self.training:
            batch_size, num_steps, _ = X.shape
            # dec_valid_lens的开头:(batch_size,num_steps),
            # 其中每一行是[1,2,...,num_steps]
            # 从这里可以知道，valid_lens是所有词元数
            dec_valid_lens = torch.arange(
                1, num_steps + 1, device=X.device).repeat(batch_size, 1)
            # 产生一个(batch_size, num_steps)形状的dec_valid_lens
        else:
            dec_valid_lens = None

        # 自注意力
        X2 = self.attention1(X, key_values, key_values, dec_valid_lens)
        Y = self.addnorm1(X, X2)
        # 编码器－解码器注意力。
        # enc_outputs的开头:(batch_size,num_steps,num_hiddens)
        Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens)
        Z = self.addnorm2(Y, Y2)
        return self.addnorm3(Z, self.ffn(Z)), state
</code></pre><p><strong>测试</strong>一下：</p><pre><code class="lang-python">decoder_blk = DecoderBlock(24, 24, 24, 24, [100, 24], 24, 48, 8, 0.5, 0)
decoder_blk.eval()
X = torch.ones((2, 100, 24))
state = [encoder_blk(X, valid_lens), valid_lens, [None]]
print(decoder_blk(X, state)[0].shape)
</code></pre><p><strong>结果</strong>：</p><pre><code class="lang-python">torch.Size([2, 100, 24])
</code></pre><p>现在，我们可以构建由<strong><code>num_layers</code>个块</strong>组成的完整的解码器。最后通过一个全连接层计算所有<code>vocab_size</code>个可能的输出词元的预测值。</p><pre><code class="lang-python">class TransformerDecoder(d2l.AttentionDecoder):
    def __init__(self, vocab_size, key_size, query_size, value_size,
                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,
                 num_heads, num_layers, dropout, **kwargs):
        super(TransformerDecoder, self).__init__(**kwargs)
        self.num_hiddens = num_hiddens
        self.num_layers = num_layers
        self.embedding = nn.Embedding(vocab_size, num_hiddens)
        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)
        self.blks = nn.Sequential()
        for i in range(num_layers):
            self.blks.add_module(&quot;block&quot; + str(i),
                                 DecoderBlock(key_size, query_size, value_size, num_hiddens,
                                              norm_shape, ffn_num_input, ffn_num_hiddens,
                                              num_heads, dropout, i))
        self.dense = nn.Linear(num_hiddens, vocab_size)
        # 用Linear转化为词元特征维度，输出。

    def init_state(self, enc_outputs, enc_valid_lens, *args):
        return [enc_outputs, enc_valid_lens, [None] * self.num_layers]

    def forward(self, X, state):
        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))
        self._attention_weights = [[None] * len(self.blks) for _ in range(2)]
        for i, blk in enumerate(self.blks):
            X, state = blk(X, state)
        return self.dense(X), state
</code></pre><h2 id="训练与预测"><a href="#训练与预测" class="headerlink" title="训练与预测"></a>训练与预测</h2><blockquote><p>提示：需要安装下载<code>d2l</code>包。导入<code>torch</code>和<code>torch.nn</code>是必要的。</p></blockquote><p>下面训练<code>Transformer</code>。</p><blockquote><p>注：<strong><code>BLEU</code>分数</strong>是机器翻译的<strong>评价标准</strong>，在0~1区间内，<strong>越接近1，越准确</strong>。</p></blockquote><pre><code class="lang-python">num_hiddens, num_layers, dropout, batch_size, num_steps = 32, 2, 0.1, 64, 10
lr, num_epochs, device = 0.005, 200, d2l.try_gpu()
ffn_num_input, ffn_num_hiddens, num_heads = 32, 64, 4
key_size, query_size, value_size = 32, 32, 32
norm_shape = [32]

train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)

encoder = TransformerEncoder(
    len(src_vocab), key_size, query_size, value_size, num_hiddens,
    norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,
    num_layers, dropout)
decoder = TransformerDecoder(
    len(tgt_vocab), key_size, query_size, value_size, num_hiddens,
    norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,
    num_layers, dropout)
net = d2l.EncoderDecoder(encoder, decoder)
d2l.train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)
</code></pre><p><strong>训练</strong>结果：</p><pre><code class="lang-python">loss 0.032, 5679.3 tokens/sec on cuda:0
&lt;Figure size 350x250 with 1 Axes&gt;
</code></pre><p>训练结束后，将一些英语句子<strong>翻译</strong>成法语，并且计算它们的<code>BLEU</code>分数。</p><pre><code class="lang-python">engs = [&#39;go .&#39;, &quot;i lost .&quot;, &#39;he\&#39;s calm .&#39;, &#39;i\&#39;m home .&#39;]
fras = [&#39;va !&#39;, &#39;j\&#39;ai perdu .&#39;, &#39;il est calme .&#39;, &#39;je suis chez moi .&#39;]
for eng, fra in zip(engs, fras):
    translation, dec_attention_weight_seq = d2l.predict_seq2seq(
        net, eng, src_vocab, tgt_vocab, num_steps, device, True)
    print(f&#39;&#123;eng&#125; =&gt; &#123;translation&#125;, &#39;,
          f&#39;bleu &#123;d2l.bleu(translation, fra, k=2):.3f&#125;&#39;)
</code></pre><p><strong>结果</strong>：</p><pre><code class="lang-python">go . =&gt; va !,  bleu 1.000
i lost . =&gt; j&#39;ai perdu .,  bleu 1.000
he&#39;s calm . =&gt; il est calme .,  bleu 1.000
i&#39;m home . =&gt; je suis chez moi .,  bleu 1.000
</code></pre><blockquote><p>本文毕。</p></blockquote></div><footer><div class="meta"><span class="item"><span class="icon"><i class="ic i-calendar-check"></i> </span><span class="text">更新于</span> <time title="修改时间：2025-05-16 22:08:10" itemprop="dateModified" datetime="2025-05-16T22:08:10+08:00">2025-05-16</time> </span><span id="ai/注意力机制/" class="item leancloud_visitors" data-flag-title="注意力机制" title="阅读次数"><span class="icon"><i class="ic i-eye"></i> </span><span class="text">阅读次数</span> <span class="leancloud-visitors-count"></span> <span class="text">次</span></span></div><div class="reward"><button><i class="ic i-heartbeat"></i> 赞赏</button><p>请我喝[茶]~(￣▽￣)~*</p><div id="qr"><div><img data-src="/images/wechatpay.jpg" alt="Runhua Deng 微信支付"><p>微信支付</p></div><div><img data-src="/images/alipay.png" alt="Runhua Deng alipay"><p>alipay</p></div><div><img data-src="/images/paypal.png" alt="Runhua Deng paypal"><p>paypal</p></div></div></div><div id="copyright"><ul><li class="author"><strong>本文作者： </strong>Runhua Deng <i class="ic i-at"><em>@</em></i>枯萎的花将在另一彼岸悄然绽放</li><li class="link"><strong>本文链接：</strong> <a href="https://yunhdan.github.io/ai/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/" title="注意力机制">https://yunhdan.github.io/ai/注意力机制/</a></li><li class="license"><strong>版权声明： </strong>本站所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</span> 许可协议。转载请注明出处！</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/research/PairLIE%E8%AE%BA%E6%96%87%E8%AF%A6%E8%A7%A3/" itemprop="url" rel="prev" data-background-image="https:&#x2F;&#x2F;s2.loli.net&#x2F;2024&#x2F;12&#x2F;07&#x2F;6Bxol9QWmLbTAJr.jpg" title="PairLIE论文详解"><span class="type">上一篇</span> <span class="category"><i class="ic i-flag"></i> 学术</span><h3>PairLIE论文详解</h3></a></div><div class="item right"><a href="/research/Retinexformer%E8%AE%BA%E6%96%87%E8%AF%A6%E8%A7%A3/" itemprop="url" rel="next" data-background-image="https:&#x2F;&#x2F;s2.loli.net&#x2F;2024&#x2F;12&#x2F;04&#x2F;wnuNMRzQhYjdib9.png" title="Retinexformer论文详解"><span class="type">下一篇</span> <span class="category"><i class="ic i-flag"></i> 学术</span><h3>Retinexformer论文详解</h3></a></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="文章目录"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%8F%90%E7%A4%BA%E5%8F%8C%E7%BB%84%E4%BB%B6%E4%B8%8E%E6%9F%A5%E8%AF%A2%E3%80%81%E9%94%AE%E3%80%81%E5%80%BC"><span class="toc-number">1.</span> <span class="toc-text">注意力提示双组件与查询、键、值</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%9D%9E%E5%8F%82%E6%95%B0%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%B1%87%E8%81%9A%EF%BC%9A-Nadaraya-Watson-%E6%A0%B8%E5%9B%9E%E5%BD%92"><span class="toc-number">2.</span> <span class="toc-text">非参数注意力汇聚：$Nadaraya-Watson$ 核回归</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%B8%A6%E5%8F%82%E6%95%B0%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%B1%87%E8%81%9A"><span class="toc-number">3.</span> <span class="toc-text">带参数的注意力汇聚</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%AF%84%E5%88%86%E5%87%BD%E6%95%B0"><span class="toc-number">4.</span> <span class="toc-text">注意力评分函数</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8A%A0%E6%80%A7%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-number">5.</span> <span class="toc-text">加性注意力</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Masked-softmax-%EF%BC%9A%E6%8E%A9%E8%94%BD-softmax-%E6%93%8D%E4%BD%9C"><span class="toc-number">6.</span> <span class="toc-text">Masked-$softmax$：掩蔽$softmax$操作</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BC%A9%E6%94%BE%E7%82%B9%E7%A7%AF%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-number">7.</span> <span class="toc-text">缩放点积注意力</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-number">8.</span> <span class="toc-text">自注意力</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="toc-number">9.</span> <span class="toc-text">位置编码</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%9D%E5%AF%B9%E4%BD%8D%E7%BD%AE%E4%BF%A1%E6%81%AF"><span class="toc-number">9.1.</span> <span class="toc-text">绝对位置信息</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%B8%E5%AF%B9%E4%BD%8D%E7%BD%AE%E4%BF%A1%E6%81%AF"><span class="toc-number">9.2.</span> <span class="toc-text">相对位置信息</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-number">10.</span> <span class="toc-text">多头注意力</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Transformer"><span class="toc-number">11.</span> <span class="toc-text">Transformer</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B"><span class="toc-number">11.1.</span> <span class="toc-text">模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E4%BD%8D%E7%BD%AE%E7%9A%84%E5%89%8D%E9%A6%88%E7%BD%91%E7%BB%9C"><span class="toc-number">11.2.</span> <span class="toc-text">基于位置的前馈网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5%E5%92%8C%E5%B1%82%E8%A7%84%E8%8C%83%E5%8C%96"><span class="toc-number">11.3.</span> <span class="toc-text">残差连接和层规范化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8"><span class="toc-number">11.4.</span> <span class="toc-text">编码器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A7%A3%E7%A0%81%E5%99%A8"><span class="toc-number">11.5.</span> <span class="toc-text">解码器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E4%B8%8E%E9%A2%84%E6%B5%8B"><span class="toc-number">11.6.</span> <span class="toc-text">训练与预测</span></a></li></ol></li></ol></div><div class="related panel pjax" data-title="系列文章"><ul><li class="active"><a href="/ai/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/" rel="bookmark" title="注意力机制">注意力机制</a></li><li><a href="/ai/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="bookmark" title="强化学习">强化学习</a></li><li><a href="/ai/Deep-Learning-Experiment-Tricks/" rel="bookmark" title="Deep Learning Experiment Tricks">Deep Learning Experiment Tricks</a></li><li><a href="/ai/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/" rel="bookmark" title="深度学习理论">深度学习理论</a></li><li><a href="/ai/%E7%8E%B0%E4%BB%A3%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="bookmark" title="现代深度学习">现代深度学习</a></li></ul></div><div class="overview panel" data-title="站点概览"><div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="image" itemprop="image" alt="Runhua Deng" data-src="/images/me.jpg"><p class="name" itemprop="name">Runhua Deng</p><div class="description" itemprop="description">计算机视觉 & 图像恢复</div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">39</span> <span class="name">文章</span></a></div><div class="item categories"><a href="/categories/"><span class="count">7</span> <span class="name">分类</span></a></div></nav><div class="social"><span class="exturl item github" data-url="aHR0cHM6Ly9naXRodWIuY29tL1l1bkhEYW4=" title="https:&#x2F;&#x2F;github.com&#x2F;YunHDan"><i class="ic i-github"></i></span> <span class="exturl item music" data-url="aHR0cHM6Ly9tdXNpYy4xNjMuY29tLyMvdXNlci9ob21lP2lkPTIxMjYzMjE4OTI=" title="https:&#x2F;&#x2F;music.163.com&#x2F;#&#x2F;user&#x2F;home?id&#x3D;2126321892"><i class="ic i-cloud-music"></i></span> <a href="/alex2312666252@gmail.com" title="alex2312666252@gmail.com" class="item email"><i class="ic i-envelope"></i></a> <span class="exturl item csdn" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L20wXzczNTk5NzM4P3NwbT0xMDAwLjIxMTUuMzAwMS41MzQz" title="https:&#x2F;&#x2F;blog.csdn.net&#x2F;m0_73599738?spm&#x3D;1000.2115.3001.5343"><i class="ic i-link"></i></span></div><ul class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>首页</a></li><li class="item dropdown"><a href="/about/" rel="section"><i class="ic i-user"></i>关于</a><ul class="submenu"><li class="item"><a href="/about/me/" rel="section"><i class="ic i-file"></i>简历</a></li><li class="item"><a href="/about/learn_me/" rel="section"><i class="ic i-smile"></i>了解我</a></li></ul></li><li class="item dropdown"><a href="javascript:void(0);"><i class="ic i-feather"></i>文章</a><ul class="submenu"><li class="item"><a href="/archives/" rel="section"><i class="ic i-list-alt"></i>归档</a></li><li class="item"><a href="/categories/" rel="section"><i class="ic i-th"></i>分类</a></li><li class="item"><a href="/tags/" rel="section"><i class="ic i-tags"></i>标签</a></li></ul></li><li class="item"><a href="/friends/" rel="section"><i class="ic i-heart"></i>友達</a></li></ul></div></div></div><ul id="quick"><li class="prev pjax"><a href="/research/PairLIE%E8%AE%BA%E6%96%87%E8%AF%A6%E8%A7%A3/" rel="prev" title="上一篇"><i class="ic i-chevron-left"></i></a></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/research/Retinexformer%E8%AE%BA%E6%96%87%E8%AF%A6%E8%A7%A3/" rel="next" title="下一篇"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>随机文章</h2><ul><li class="item"><div class="breadcrumb"><a href="/categories/other/" title="分类于 琐碎">琐碎</a></div><span><a href="/other/%E7%BE%BD%E6%AF%9B%E7%90%83%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89/" title="羽毛球学习（一）">羽毛球学习（一）</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/research/" title="分类于 学术">学术</a></div><span><a href="/research/Retinexformer%E8%AE%BA%E6%96%87%E8%AF%A6%E8%A7%A3/" title="Retinexformer论文详解">Retinexformer论文详解</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/other/" title="分类于 琐碎">琐碎</a></div><span><a href="/other/My%20%E4%BA%8C%E6%AC%A1%E5%85%83/" title="My 二次元">My 二次元</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 人工智能">人工智能</a></div><span><a href="/ai/Deep-Learning-Experiment-Tricks/" title="Deep Learning Experiment Tricks">Deep Learning Experiment Tricks</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/research/" title="分类于 学术">学术</a></div><span><a href="/research/%E5%85%B7%E8%BA%AB%E6%99%BA%E8%83%BD%E5%9F%BA%E7%A1%80%E6%8A%80%E6%9C%AF/" title="具身智能基础技术">具身智能基础技术</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/research/" title="分类于 学术">学术</a></div><span><a href="/research/EnlightenGAN%E8%AE%BA%E6%96%87%E8%AF%A6%E8%A7%A3/" title="EnlightenGAN论文详解">EnlightenGAN论文详解</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 人工智能">人工智能</a></div><span><a href="/ai/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/" title="深度学习理论">深度学习理论</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/cs/" title="分类于 计算机">计算机</a></div><span><a href="/cs/Pip%E3%80%81Conda%E3%80%81github%E9%95%9C%E5%83%8F/" title="Pip、Conda、Github镜像">Pip、Conda、Github镜像</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/cs/" title="分类于 计算机">计算机</a></div><span><a href="/cs/Javascript%E7%9F%A5%E8%AF%86%E6%A0%91/" title="Javascript知识树">Javascript知识树</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/research/" title="分类于 学术">学术</a></div><span><a href="/research/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%AE%BA%E6%96%87%E7%B2%BE%E7%82%BC/" title="多模态论文精炼">多模态论文精炼</a></span></li></ul></div><div><h2>最新评论</h2><ul class="leancloud-recent-comment"></ul></div></div><div class="status"><div class="copyright">&copy; 2023 – <span itemprop="copyrightYear">2025</span> <span class="with-love"><i class="ic i-sakura rotate"></i> </span><span class="author" itemprop="copyrightHolder">Runhua Deng @ Runfar's Zone</span></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-chart-area"></i> </span><span title="站点总字数">194k 字</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="ic i-coffee"></i> </span><span title="站点阅读时长">2:57</span></div><div class="powered-by">基于 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL={path:"ai/注意力机制/",favicon:{show:"（●´3｀●）やれやれだぜ",hide:"(´Д｀)大変だ！"},search:{placeholder:"文章搜索",empty:"关于 「 ${query} 」，什么也没搜到",stats:"${time} ms 内找到 ${hits} 条结果"},valine:!0,copy_tex:!0,katex:!0,fancybox:!0,copyright:'复制成功，转载请遵守 <i class="ic i-creative-commons"></i>BY-NC-SA 协议。',ignores:[function(e){return e.includes("#")},function(e){return new RegExp(LOCAL.path+"$").test(e)}]}</script><script src="https://cdn.polyfill.io/v2/polyfill.js"></script><script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script><script src="/js/app.js?v=0.2.5"></script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({log:!1,pluginJsPath:"lib/",pluginModelPath:"assets/",pluginRootPath:"live2dw/",tagMode:!1})</script></body></html><!-- rebuild by hrmmi -->